[
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 3b",
    "section": "",
    "text": "This exercise covers how to create animated data visualisation by using gganimate and plotly r packages, and how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\ntheory: a summary of R for Visual Analytics - Chap 4"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 3b",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nIn this exercise, plotly, idyverse, gganimate, gifski, gapminder packages are used. A summary of the new packages introduced can be found at the section below.\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-data",
    "title": "Hands-on Exercise 3b",
    "section": "Importing data",
    "text": "Importing data\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\nUsing mutate_at()Using across()\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#basic-concepts-of-animation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#basic-concepts-of-animation",
    "title": "Hands-on Exercise 3b",
    "section": "Basic concepts of animation",
    "text": "Basic concepts of animation\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#terminology",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#terminology",
    "title": "Hands-on Exercise 3b",
    "section": "Terminology",
    "text": "Terminology\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\nThings to consider:\n\nDoes it makes sense to go through the effort?\n\nexploratory data analysis: a animated graphic may not be worth the time investment.\npresentation: a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#packages",
    "title": "Hands-on Exercise 3b",
    "section": "Packages",
    "text": "Packages\nIn this exercise, beside tidyverse, 3 new R packages will be used. They are:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquantâ€™s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#gganimate",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#gganimate",
    "title": "Hands-on Exercise 3b",
    "section": "gganimate",
    "text": "gganimate\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\nBasicBuilding the animated bubble plot\n\n\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e.Â Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3b",
    "section": "Animated Data Visualisation: plotly",
    "text": "Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\nBuilding an animated bubble plot: ggplotly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using ggplotly() method.\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component for controlling the animation\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk below.\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\nBuilding an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#reference",
    "title": "Hands-on Exercise 3b",
    "section": "Reference",
    "text": "Reference\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html",
    "title": "In-class Exercise 1a: Now You ðŸ‘€ It!",
    "section": "",
    "text": "In this hands-on exercise, 2 R packages will be used. They are:\n\ntidyverse\nhaven\n\nThe code chunk used as follows:\n\npacman::p_load(tidyverse,haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html#loading-r-packages",
    "title": "In-class Exercise 1a: Now You ðŸ‘€ It!",
    "section": "",
    "text": "In this hands-on exercise, 2 R packages will be used. They are:\n\ntidyverse\nhaven\n\nThe code chunk used as follows:\n\npacman::p_load(tidyverse,haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html#filtering-pisa-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html#filtering-pisa-data",
    "title": "In-class Exercise 1a: Now You ðŸ‘€ It!",
    "section": "Filtering PISA data",
    "text": "Filtering PISA data\n\n\n\n\n\n\nNote\n\n\n\nthe code under this section filtered PISA data to include only SG data\nthe dataset is saved as a rds file called stu_qqq_SG.rds and loaded back as a dataframe.\nonly stu_qqq_SG.rds will be imported for efficiency purpose for the rest of exercise.\n\n\nThe code chunk below uses read_sas() of haven to import PISA data into R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe code chunk below use filter() of dplyr to filter observation in SG only.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\n  filter(CNT == \"SGP\")\n\nThe code chunk below use write_rds() to save the data into rds format.\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html#importing-filtered-pisa-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01a.html#importing-filtered-pisa-data",
    "title": "In-class Exercise 1a: Now You ðŸ‘€ It!",
    "section": "Importing filtered PISA data",
    "text": "Importing filtered PISA data\nThe code chunk below uses read_rds() to import PISA SG data into R environment.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "This exercise aims to use data analysis methods to improve some of the visualizations produced for exercise 1. The original analysis could be found through this link\nThe complete project brief could be found here.\n\n\nThe 2022 Programme for International Student Assessment (PISA) data, released on December 5, 2022, assesses global education systems by testing 15-year-olds in mathematics, reading, and science. The PISA 2022 database includes responses from students, schools, and parents across five data files, primarily in SAS and SPSS formats.\nThe dataset used for this exercise is a pre-engineered dataset followed the same instructions of data preperation given in the source.\n\n\n\nThis take-home exercise covers\n\ncritic of the original submission in terms of clarity and aesthetics\nsketch for the alternative design\nremake of the original design using ggplot2, ggplot2 extensions and tidyverse packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The 2022 Programme for International Student Assessment (PISA) data, released on December 5, 2022, assesses global education systems by testing 15-year-olds in mathematics, reading, and science. The PISA 2022 database includes responses from students, schools, and parents across five data files, primarily in SAS and SPSS formats.\nThe dataset used for this exercise is a pre-engineered dataset followed the same instructions of data preperation given in the source."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#project-task",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#project-task",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "This take-home exercise covers\n\ncritic of the original submission in terms of clarity and aesthetics\nsketch for the alternative design\nremake of the original design using ggplot2, ggplot2 extensions and tidyverse packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#loading-r-packages",
    "title": "Take-home Exercise 2",
    "section": "Loading R packages",
    "text": "Loading R packages\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               ggdist, ggridges,\n               colorspace,ggstatsplot, \n               tidyverse)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-filtered-pisa-data-for-sg-students",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-filtered-pisa-data-for-sg-students",
    "title": "Take-home Exercise 2",
    "section": "Importing filtered PISA data for SG students",
    "text": "Importing filtered PISA data for SG students\n\nThe code chunk below uses read_rds() to import PISA SG data into R environment.\n\nfiltered_data &lt;- read_rds(\"data/filtered_data.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#overall-critique",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#overall-critique",
    "title": "Take-home Exercise 2",
    "section": "Overall Critique",
    "text": "Overall Critique\nThe submission presents a thorough analysis, with commendable clarity and aesthetic consistency.\n\nStrengths\n\nClarity\n\nExtremely comprehensive introduction supported by external references, readings, and PISA report.\nThe selection of variables and the data preparation process are both logical and well-justified, which contributes to the persuasive and clear narrative of the report.\nFlowcharts designed to be both visually appealing and informative, providing a clear understanding of the data structure employed.\nThe analytical approach is engaging, particularly the comparative examination of the top and bottom five schools.\n\n\n\nAesthetic\n\nThere is a commendable uniformity across the submission, with a consistent color theme, style, layout, and aspect ratio maintained throughout.\nTitles and subtitles are well-crafted, and the labeling on the patched plot is especially effective in preventing misunderstandings.\n\n\n\n\nAreas for Improvement\n\nClarity\n\nA cross-subject comparison could offer additional insights into performance variations among different subjects. Although the analysis is conducted in parallel across subjects, presenting multiple charts under 1 subject simultaneously may hinder the audienceâ€™s ability to make intuitive comparisons and contrasts.\nThere are missed opportunities to highlight and draw attention to the key insights derived from the graphs.\n\n\n\nAesthetic\n\nThe chart aspect ratios are not fully optimized for computer screens, which could affect readability.\nSome chart elements, such as color choices and transparency levels, result in an overlap of information that may be difficult to discern."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plot-1-evaluation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plot-1-evaluation",
    "title": "Take-home Exercise 2",
    "section": "Plot 1 Evaluation",
    "text": "Plot 1 Evaluation\n\nOriginal Design Intent\n\nThis plot serves as an introduction to the variables analyzed in the study, providing the reader with an overview of the sample size and distribution characteristics before delving into deeper analysis.\n\n\nCritique\nNote: Positive Aspects Highlighted in Red, Areas for Improvement Highlighted in Blue. Each point in the description could be refereed to the diagram below\n\n\nStrengths\nClarity\n\np1: The title succinctly encapsulates the main takeaway of the plot, effectively pointing out the right-skewed distribution and offering a summary that orients the reader.\np2: The arrangement of three plausible value (PV) distributions across subjects in a vertical format allows for straightforward comparison of the histogramsâ€™ peaks and shapes along a consistent x-axis.\np3: Sub-title is clear and informative\n\nAesthetic\n\np4: grid line in lighter color, good hieghachy of information\np5: extremely consistent use of color, style, font size etc.\n\n\n\nAreas for Improvement\nClarity\n\np1: The title could be expanded to encapsulate all the information presented in the chart.\np2: While histograms are useful for showing general distribution, boxplots might reveal additional details like outliers. This applies to ESCS histogram as well.\np3: The inconsistent y-axis scale across different subjects could lead to misinterpretation when comparing population sizes across grades.\np4: The distribution of PVs lacks guideline indicators such as mean or median values, which could enhance interpretability.\np6: The similar height of the gender distribution bars could be more informative if annotated with exact numbers.\n\nAesthetic\n\np1: A central title with a larger font could improve the visual hierarchy.\np5: The base of the x-axis should feature a slightly darker color to ground the distribution.\np7: The chartâ€™s aspect ratio is not well-suited for computer screens, leading to potential display issues and readability challenges.\n\n\n\n\nSketch\n\nimprovements based on the above 9 points mentioned earlier:\n\np1: Main title should be centered to better balance the visual layout. Additionally, the title has been broadened to encompass all the information presented in the diagram, ensuring a more holistic representation.\np2: Boxplots should be added to each distribution to reveal more in-depth information. This is particularly effective in identifying and presenting outliers, providing a clearer understanding of data variances.\np3: The scales for all 3 y-axes corresponding to different subjects should be adjusted for alignment. This ensures a consistent and fair comparison across subjects.\np4: Add guidelines highlight the mean scores across students, offering a quick reference to these central data points, aiding in immediate comprehension of average performances.\np5: The inclusion of minor gridlines at 100-unit intervals to provide a more detailed and precise scale for measurement, enhancing the plotâ€™s usability.\np6: Labels indicating the number of samples in each gender category to offer immediate quantitative understanding of gender distribution within the sample.\np7: Switch layout to a landscape orientation, catering to computer screen dimensions to improve readability and overall user experience when interacting with the data on digital platforms.\n\n\n\nMake Over\n\n\nShow the code\nmath &lt;- ggplot(data=filtered_data, \n             aes(x = MATH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\",\n                 linewidth=0.3) +\n  geom_boxplot(width = 50,\n               fill = \"white\", \n               color = \"black\",\n               outlier.colour = \"#c73824\",\n               outlier.fill = \"#c73824\",\n               outlier.size = 1,\n               outlier.alpha = 0.2) +\n  coord_cartesian(xlim=c(0,1000), ylim=c(0,1000)) +\n  ggtitle(\"Distribution of Math Scores\") + \n  geom_vline(data = filtered_data %&gt;%\n               summarize(mean_score = mean(MATH), median_score = median(MATH)),\n             aes(xintercept = mean_score), color = \"#c73824\", linetype = \"dashed\") +  \n  geom_text(data = filtered_data %&gt;%\n               summarize(mean_score = mean(MATH), median_score = median(MATH)),\n             aes(x = mean_score - 100, y = Inf, label = paste(\"Mean:\", round(mean_score, 2))), color = \"#c73824\", vjust = 1, size = 3) +\n  scale_x_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100))  \n\nread &lt;- ggplot(data=filtered_data, \n             aes(x = READ)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\",\n                 linewidth=0.3) +\n  geom_boxplot(width = 50,\n               fill = \"white\", \n               color = \"black\",\n               outlier.colour = \"#c73824\",\n               outlier.fill = \"#c73824\",\n               outlier.size = 1,\n               outlier.alpha = 0.2) +\n  coord_cartesian(xlim=c(0,1000), ylim=c(0,1000)) +\n  ggtitle(\"Distribution of Reading Scores\") + \n  geom_vline(data = filtered_data %&gt;%\n               summarize(mean_score = mean(READ), median_score = median(READ)),\n             aes(xintercept = mean_score), color = \"#c73824\", linetype = \"dashed\") +  \n  geom_text(data = filtered_data %&gt;%\n               summarize(mean_score = mean(READ), median_score = median(READ)),\n             aes(x = mean_score - 100, y = Inf, label = paste(\"Mean:\", round(mean_score, 2))), color = \"#c73824\", vjust = 1, size = 3) +\n  scale_x_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100)) + \n  theme(plot.title = element_text(size = 12)) \n\nsci &lt;- ggplot(data=filtered_data, \n             aes(x = SCIENCE)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\",\n                 linewidth=0.3) +\n  geom_boxplot(width = 50,\n               fill = \"white\", \n               color = \"black\",\n               outlier.colour = \"#c73824\",\n               outlier.fill = \"#c73824\",\n               outlier.size = 1,\n               outlier.alpha = 0.2) +\n  coord_cartesian(xlim=c(0,1000), ylim=c(0,1000)) +\n  ggtitle(\"Distribution of Science Scores\") + \n  geom_vline(data = filtered_data %&gt;%\n               summarize(mean_score = mean(SCIENCE), median_score = median(SCIENCE)),\n             aes(xintercept = mean_score), color = \"#c73824\", linetype = \"dashed\") +  \n  geom_text(data = filtered_data %&gt;%\n               summarize(mean_score = mean(SCIENCE), median_score = median(SCIENCE)),\n             aes(x = mean_score - 100, y = Inf, label = paste(\"Mean:\", round(mean_score, 2))), color = \"#c73824\", vjust = 1, size = 3) +\n  scale_x_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100))  \n\nses &lt;- ggplot(data=filtered_data, \n             aes(x = SES)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\",\n                 linewidth = 0.3) +\n  geom_boxplot(width = 50,\n               fill = \"white\", \n               color = \"black\",\n               outlier.colour = \"#c73824\",\n               outlier.fill = \"#c73824\",\n               outlier.size = 1,\n               outlier.alpha = 0.2) +\n  coord_cartesian(xlim=c(-5,5), ylim = c(0,1500)) +\n  ggtitle(\"Distribution of SES\") + \n  geom_vline(data = filtered_data %&gt;%\n                 summarize(mean_score = mean(SES), median_score = median(SES)),\n             aes(xintercept = mean_score), color = \"#c73824\", linetype = \"dashed\") +  \n  geom_text(data = filtered_data %&gt;%\n               summarize(mean_score = mean(SES), median_score = median(SES)),\n             aes(x = mean_score - 1, y = Inf, label = paste(\"Mean:\", round(mean_score, 2))), color = \"#c73824\", vjust = 1, size = 3) \n\ngender &lt;- ggplot(data=filtered_data, \n             aes(x = GENDER)) +\n   geom_bar(color=\"grey25\", fill=\"grey90\", linewidth =0.2) +\n   geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.3) + \n   coord_cartesian(xlim=c(0,3), ylim =c(0,4000)) +\n  ggtitle(\"Distribution of Gender\")\n\npatchwork &lt;- ( math / read/ sci) | (ses / gender)\npatchwork &lt;- patchwork + \n  plot_annotation(\n    title = \"Distributions Across Subjects Preformance, Socialeconomic Status and Gender\",\n    theme = theme(plot.title = element_text(size = 50))  \n  )\npatchwork & theme_fivethirtyeight(base_size = 7) +\n  theme(\n    panel.grid.minor.x = element_line(linewidth = 0.2, linetype = 'solid', colour = \"grey85\"),\n    plot.title = element_text(face = \"bold\", hjust = 0.5)\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plot-2-evaluation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#plot-2-evaluation",
    "title": "Take-home Exercise 2",
    "section": "Plot 2 Evaluation",
    "text": "Plot 2 Evaluation\n\nOriginal Design Intent\n\nThis plot synthesizes a comprehensive analysis of Singaporean studentsâ€™ performance in mathematics, taking into account various factors including school, gender, and socioeconomic status. Given that the same layout is applied to the other two subjects, this evaluation will concentrate on mathematics performance as a representative example.\n\n\nCritique - Overall\nNote: Positive Aspects Highlighted in Red, Areas for Improvement Highlighted in Blue. Each point in the description could be refereed to the diagram below\n\n\nStrengths\nClarity\n\np1: The title succinctly encapsulates the main takeaway of the plot\np2: Sub-title is clear and informative\np3: Interesting to include OECD mean value as a benchmark introduces a comparative dimension to the analysis, enriching the narrative around Singaporeâ€™s performance.\n\nAesthetic\n\np4: grid line in lighter color, good hierarchy of information, extremely consistent use of color, style, font size etc.\n\n\n\nAreas for Improvement\nClarity\n\np1: The repeated presentation of histograms may lead to redundant information if they do not offer new insights beyond previous charts.\np2: While the plot compares performance across gender, socioeconomic status, and schools for each subject, it misses the opportunity to analyze these factors in parallel across different subjects, which could provide a more holistic understanding.\n\nAesthetic\n\np1: The current proportions of the histogram are too narrow, making it challenging to view all data on one screen and hindering readability.\n\nTo enhance the analytical narrative and provide a clearer comparative perspective, it would be advantageous to reorganize the data presentation structure for a more focused analysis on each thematic factor, facilitating a deeper understanding of how each influences performance across different academic disciplines. Such an approach could reveal more nuanced insights and trends that might be obscured in the current subject-by-subject layout\n\n\n\nCritique - School\nNote: Positive Aspects Highlighted in Red, Areas for Improvement Highlighted in Blue. Each point in the description could be refereed to the diagram below\n\n\nStrengths\nClarity\n\np1: The use of density plots is an effective choice for illustrating the relationship between schools, with a smart focus on comparing only the top and bottom 5 schools for clarity.\np2: The inclusion of mean value lines provides a clear reference point, offering an informative range indication.\n\nAesthetic\n\np3: The application of a color gradient effectively differentiates scoring ranges, aiding in visual interpretation.\np4: grid line in lighter color, good hierarchy of information, extremely consistent use of color, style, font size etc.\n\n\n\nAreas for Improvement\nClarity\n\np1: The title is too general and fails to convey critical insights or specific focus of the analysis.\np2: The chart is limited to presenting data for a single subject, which restricts broader comparative analysis.\np3: Guideline annotations lack labels\np4: The use of quantiles for each school may not be necessary in this context. Given the varying performance and sample sizes across schools, a general trend might suffice to convey the intended message.\np5: The absence of y-axis labels detracts from the interpretability of the data.\np6: Inconsistent x-axis scales across different subjects can introduce bias in parallel comparisons.\n\nAesthetic\n\np7: The color scheme might not be distinct enough to differentiate clearly between the lower and upper 5 schools, potentially hindering quick visual differentiation.\n\n\n\n\nSketch\n\nimprovements based on the above points mentioned earlier:\n\np1: Main title should be centered to better balance the visual layout. Additionally, the title should use to highlight key information in the chart\np2: Combining grades from different subjects for the top and bottom 5 schools into a single chart could be more effective. Since the focus is on the distribution patterns rather than specific school identities, this approach will facilitate a clearer comparative analysis.\np3: Enhance clarity by adding informative annotations along the mean line.\np4: The quantile lines, deemed unnecessary for this context, have been removed to simplify the visual presentation and focus on the most relevant data trends.\np5: The y-axis has been clearly labeled, ensuring that viewers can easily interpret the data and understand the variables being analyzed.\np6: The x-axis has been standardized across different subjects to maintain consistency and avoid any potential bias in comparative analysis.\np7: Different shades or colors will be used to represent different subjects, enabling a more intuitive distinction between them and enhancing the overall readability of the chart.\n\n\n\nMake Over\n\n\nShow the code\n# create dataframe to store school ranking information\ntop_bottom_schools_df &lt;- data.frame(\n  SchoolID = character(),\n  Subject_Rank = integer(),\n  MeanScore = numeric(),\n  stringsAsFactors = FALSE\n)\n\nsubjects &lt;- c(\"MATH\", \"READ\", \"SCIENCE\")\n\nfor (subject in subjects) {\n  school_means &lt;- filtered_data %&gt;%\n    group_by(SCHOOLID) %&gt;%\n    summarize(MeanScore = mean(get(subject), na.rm = TRUE), .groups = 'drop')\n  \n  school_means &lt;- school_means %&gt;%\n    arrange(desc(MeanScore)) %&gt;%\n    mutate(Subject_Rank = rank(desc(MeanScore)))\n  \n  top_bottom &lt;- school_means %&gt;%\n    filter(Subject_Rank &lt;= 5 | Subject_Rank &gt;= (n() - 4))\n  \n  top_bottom$Subject &lt;- subject\n  \n  top_bottom_schools_df &lt;- bind_rows(top_bottom_schools_df, top_bottom)\n}\n\ntop_bottom_schools_wide &lt;- top_bottom_schools_df %&gt;%\n  select(-MeanScore) %&gt;%\n  pivot_wider(names_from = Subject, values_from = Subject_Rank)\n\ncolnames(top_bottom_schools_wide) &lt;- gsub(\"(.*)_Rank\", \"\\\\1\", colnames(top_bottom_schools_wide))\n\n\ntop_bottom_schools_wide &lt;- top_bottom_schools_wide %&gt;%\n  rename(\n    MATH_rank = MATH,\n    READ_rank = READ,\n    SCIENCE_rank = SCIENCE\n  )\n\n# join the dataframe\ndf &lt;- inner_join(filtered_data, top_bottom_schools_wide, by = \"SCHOOLID\")\ndf &lt;- df %&gt;%\n  mutate_at(vars(SCIENCE_rank,READ_rank,MATH_rank), as.factor)\n\n# Create the ggplot\nggplot(df %&gt;% filter(!is.na(SCIENCE_rank)), aes(x = SCIENCE, y = SCIENCE_rank)) +\n  stat_density_ridges(\n    aes(fill = \"Science\"),  \n    calc_ecdf = TRUE,\n    color = \"black\",\n    alpha = 0,\n    linetype = \"solid\",\n    scale = 0.8,\n    linewidth = 0.5\n  ) +\n  stat_density_ridges(\n    data = df %&gt;% filter(!is.na(MATH_rank)),  \n    aes(x = MATH, y = MATH_rank, fill = \"Math\"),  \n    calc_ecdf = TRUE,\n    fill = \"grey20\",  \n    linewidth = 0,\n    alpha = 0.3,\n    scale = 0.8  \n  ) +\n  stat_density_ridges(\n    data = df %&gt;% filter(!is.na(READ_rank)),  \n    aes(x = READ, y = READ_rank, fill = \"Reading\"),  \n    calc_ecdf = TRUE,\n    color = \"red\",  \n    alpha = 0,\n    linetype = \"dotted\",\n    scale = 0.8,\n    linewidth = 0.7\n  ) +\n  labs(y = \"School ID\", x = \"Scores\") +\n  scale_x_continuous(breaks = seq(0, 1000, 100)) +\n  ylab(\"School ID\") +\n  ggtitle(\"Science, Math, and Reading Scores between Top and Bottom 5 Schools\") +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = \"none\")  \n\n\n\n\n\nAlternatively, another approach is to analyze the school in a broader context by examining the distribution of school scores across all subjects among the surveyed schools, rather than focusing solely on the performance of students from a specific school.\n\nimprovements based on the above points mentioned earlier:\n\np1: Rather than comparing between schools, the analysis will focus on comparing performance across subjects based on school performance.\np2: For each subject, visual representations such as density plots, box plots, and dotted plots will be used to illustrate the distribution of school performance.\np3: The analysis will highlight the top and bottom 3 scorers in each subject to provide additional context\n\n\n\nShow the code\nschool_means &lt;- filtered_data %&gt;%\n  group_by(SCHOOLID) %&gt;%\n  summarize(MeanMathScore = mean(MATH, na.rm = TRUE), \n            MeanReadScore = mean(READ, na.rm = TRUE), \n            MeanSciScore = mean(SCIENCE, na.rm = TRUE), \n            .groups = 'drop'\n            )\n\ntop_bottom_schools &lt;- school_means %&gt;%\n  mutate(TopMath = rank(-MeanMathScore) &lt;= 3,\n         BottomMath = rank(MeanMathScore) &lt;= 3,\n         TopRead = rank(-MeanReadScore) &lt;= 3,\n         BottomRead = rank(MeanReadScore) &lt;= 3,\n         TopSci = rank(-MeanSciScore) &lt;= 3,\n         BottomSci = rank(MeanSciScore) &lt;= 3)\n\nlong_school_means &lt;- top_bottom_schools %&gt;%\n  gather(key = \"Subject\", value = \"Score\", MeanMathScore, MeanReadScore, MeanSciScore)\n\nhighlighted_schools &lt;- long_school_means %&gt;%\n  mutate(Highlight = ifelse((Subject == \"MeanMathScore\" & (TopMath | BottomMath)) |\n                            (Subject == \"MeanReadScore\" & (TopRead | BottomRead)) |\n                            (Subject == \"MeanSciScore\" & (TopSci | BottomSci)),\n                            \"Highlighted\", \"Normal\")) %&gt;%\n  filter(Highlight == \"Highlighted\")\n\nmean_math &lt;- mean(filtered_data$MATH, na.rm = TRUE)\nmean_read &lt;- mean(filtered_data$READ, na.rm = TRUE)\nmean_sci &lt;- mean(filtered_data$SCIENCE, na.rm = TRUE)\n\nggplot(long_school_means, aes(x = Score, y = Subject)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               height = 0.4) +\n  geom_boxplot(width = .15, outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.1, \n            binwidth = 3,\n            dotsize = 1.1,\n            alpha = 0.4,\n            fill = \"#0477bf\", \n            color = \"#0477bf\") +\n  geom_point(data = highlighted_schools, aes(x = Score, y = Subject),\n             size = 1.3, alpha = 0.8, color = \"red\",\n             position = position_nudge(y = -0.11)) +\n  geom_segment(aes(x = mean_math, xend = mean_math, y = 0.8, yend = 1.2), color = \"#c73824\", linetype = \"dashed\") +\n  geom_text(aes(x = mean_math, y = 1.5, label = paste(\"National Math Mean:\", round(mean_math, 2))),\n            color = \"#c73824\", vjust = 4, size = 3) +\n  geom_segment(aes(x = mean_read, xend = mean_read, y = 1.8, yend = 2.2), color = \"#c73824\", linetype = \"dashed\") +\n  geom_text(aes(x = mean_read, y = 1.5, label = paste(\"National Reading Mean:\", round(mean_read, 2))),\n            color = \"#c73824\", vjust = - 13, size = 3) +\n  geom_segment(aes(x = mean_sci, xend = mean_sci, y = 2.8, yend = 3.2), color = \"#c73824\", linetype = \"dashed\") +\n  geom_text(aes(x = mean_sci, y = 1.5, label = paste(\"National Science Mean:\", round(mean_sci, 2))),\n            color = \"#c73824\", vjust = -30, size = 3) +\n  theme_fivethirtyeight() +\n  theme(legend.position = \"none\", \n        panel.grid.major.x = element_line(color = \"grey80\", size = 0.3),\n        panel.grid.minor.x = element_line(color = \"grey80\", size = 0.2),\n        axis.text.x = element_text(hjust = 0.5),\n        plot.title = element_text(size = 15, hjust = 0.5)) +\n  labs(title = \"Performance Disparity Across Subjects in Singapore's Schools\") +\n  scale_x_continuous(breaks = seq(0, 1000, by = 100), \n                     minor_breaks = seq(0, 1000, by = 50)) +\n  scale_y_discrete(labels = c('Maths','Reading', 'Science')) \n\n\n\n\n\n\n\nCritique - Social Economic Status\nNote: Positive Aspects Highlighted in Red, Areas for Improvement Highlighted in Blue. Each point in the description could be refereed to the diagram below\n\n\nStrengths\nClarity\n\np1: Utilizing a scatterplot effectively uncovers the relationship between two continuous variables: scores and socioeconomic status, offering clear insights.\np2: The inclusion of a linear regression (lm) line serves as a helpful reference, revealing the nature of the relationship between these variables.\np3: The addition of guidelines, such as mean lines, provides valuable context for interpretation and facilitates comparisons across different subjects.\n\nAesthetic\n\np4: grid line in lighter color, good hierarchy of information, extremely consistent use of color, style, font size etc.\n\n\n\nAreas for Improvement\nClarity\n\np1: The title is overly broad and fails to convey the specific focus or key findings of the analysis.\np2: Guidelines are lacking labels\n\nAesthetic\n\np3: The scatterplotâ€™s current opacity setting and black color make it difficult to discern the data density. Adjusting the opacity could greatly improve clarity.\np4: The reference line appears too solid, making it somewhat incongruent with the rest of the chartâ€™s design. A subtler approach might be more fitting.\np5: The absence of labels on the y-axis diminishes the chartâ€™s interpretability.\np6: The x-axis appears overly broad, with intervals set at 250. A narrower interval might enhance readability and detail.\n\n\n\nSketch\n\nimprovements based on the above points mentioned earlier:\n\np1: Main title should be centered to better balance the visual layout. Additionally, the title use to highlight key information in the chart\np2: Clear and informative annotations will be added along the mean line, providing contextual information to aid in data interpretation.\np4: Adjustments will be made to the scatterplotâ€™s opacity and dot size, enhancing the visibility of data density patterns.\np5: The y-axis will be properly labeled, ensuring consistency and alignment across the three subjects within a single chart. This will aid in comparative analysis and improve readability.\np6: The x-axis will be standardized across the different subjects to maintain uniformity and prevent inconsistencies, facilitating more accurate parallel comparisons.\n\n\n\nMake Over\n\n\nShow the code\nmath_ses &lt;- ggplot(filtered_data, aes(x = MATH, y = SES)) +\n  geom_point(alpha = 0.03) +\n  geom_smooth(method = loess, size = 0.5, color = \"blue\") +\n  geom_vline(aes(xintercept = mean(MATH, na.rm = TRUE)), color = \"#c73824\", linetype = \"dashed\", size = 1) +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(-5, 5)) +\n  theme_fivethirtyeight() +\n  labs(x = NULL, y = \"Social Economic Status Index\", title = \"Mathematics\") +\n  theme(\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        ) +\n  annotate(\"text\", x = 400, y = 4, label = paste(\"Mean:\", round(mean(filtered_data$MATH, na.rm = TRUE), 2)), size = 3, color = \"#c73824\")\n\nread_ses &lt;- ggplot(filtered_data, aes(x = READ, y = SES)) +\n  geom_point(alpha = 0.03) +\n  geom_smooth(method = loess, size = 0.5, color = \"blue\") +\n  geom_vline(aes(xintercept = mean(READ, na.rm = TRUE)), color = \"#c73824\", linetype = \"dashed\", size = 1) +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(-5, 5)) +\n  theme_fivethirtyeight() +\n  labs(title = \"Reading\") + \n  scale_y_continuous(breaks = seq(-5, 5, by = 2.5)) + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        legend.position = \"none\") +\n  annotate(\"text\", x = 400, y = 4, label = paste(\"Mean:\", round(mean(filtered_data$SCIENCE, na.rm = TRUE), 2)), size = 3, color = \"#c73824\")\n\nsci_ses &lt;- ggplot(filtered_data, aes(x = SCIENCE, y = SES)) +\n  geom_point(alpha = 0.03) +\n  geom_smooth(method = loess, size = 0.5, color = \"blue\") +\n  geom_vline(aes(xintercept = mean(SCIENCE, na.rm = TRUE)), color = \"#c73824\", linetype = \"dashed\", size = 1) +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(-5, 5)) +\n  theme_fivethirtyeight() +\n  labs(title = \"Science\") + \n  scale_y_continuous(breaks = seq(-5, 5, by = 2.5)) + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        legend.position = \"none\") +\n  annotate(\"text\", x = 400, y = 4, label = paste(\"Mean:\", round(mean(filtered_data$READ, na.rm = TRUE), 2)), size = 3, color = \"#c73824\")\n\np1 &lt;- (math_ses + read_ses + sci_ses) + \n  plot_annotation(title= \"Moderate Linear Relationship between Social Economic Status and Performance \",\n                  theme = theme_fivethirtyeight() +\n                    theme(plot.title = element_text(size = 15, hjust = 0.5))\n                  )\n\np1\n\n\n\n\n\n\n\n\nCritique - Gender\nNote: Positive Aspects Highlighted in Red, Areas for Improvement Highlighted in Blue. Each point in the description could be refereed to the diagram below\n\n\nStrengths\nClarity\n\np1: Good choice of density plot for understanding the distribution of data.\np2: Additional boxplot adds depth to reveal the relationships between variables.\np3: Inclusion of guidelines (e.g., mean) provides additional information for interpreting and comparing across subjects.\n\nAesthetic\n\np4: Effective use of lighter-colored gridlines, maintaining a clear hierarchy of information, and consistent application of color, style, font size, etc.\n\n\n\nAreas for Improvement\nClarity\n\np1: The title is overly broad and fails to convey the specific focus or key findings of the analysis.\np2: Explore opportunities to better understand the relationship between gender and different subjects.\np3: Provide labels for the guidelines to enhance clarity.\np4: Absence of outliers for completeness.\n\nAesthetic\n\np5: Distinguish between male and female data points by using different colors to improve visual differentiation.\n\n\n\nSketch\n\nimprovements based on the above points mentioned earlier:\n\np1: Main title should be centered to better balance the visual layout. Additionally, the title use to highlight key information in the chart\np2: Combine different subjects into one chart, sharing the same y-axis to reveal the distribution among different subjects simultaneously.\np3: Add additional points or labels to highlight the mean value for better clarity.\np4: Address the issue of outliers in the chart.\np5: Use different colors to differentiate between male and female data points for better visual distinction.\n\n\n\nMake Over\n\n\nShow the code\nmath_gender &lt;- ggplot(data= filtered_data,\n       aes(x= GENDER, y= MATH, color = GENDER)) +\n  geom_violin(aes(fill = GENDER), size = 0.6, alpha = 0.3, linewidth = 0) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey20\", outlier.size = 1, \n               outlier.alpha = 0.3) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"black\",        \n               size=2) +  \n  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#c73824\", \"#0477bf\")) +\n  theme_fivethirtyeight() +\n  labs(title=\"Mathematics\") +\n  scale_y_continuous(breaks = seq(0, 1000, by = 200)) +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        legend.position = \"none\") \n\nread_gender &lt;- ggplot(data= filtered_data,\n       aes(x= GENDER, y= READ, color = GENDER)) +\n  geom_violin(aes(fill = GENDER), size = 0.6, alpha = 0.3, linewidth = 0) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey20\", outlier.size = 1, \n               outlier.alpha = 0.3) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"black\",        \n               size=2) +  \n  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#c73824\", \"#0477bf\")) +\n  theme_fivethirtyeight() +\n  labs(title=\"Reading\") + \n  scale_y_continuous(breaks = seq(0, 1000, by = 200)) + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        legend.position = \"none\") \n\nsci_gender &lt;- ggplot(data= filtered_data,\n       aes(x= GENDER, y= SCIENCE, color = GENDER)) +\n  geom_violin(aes(fill = GENDER), size = 0.6, alpha = 0.3, linewidth = 0) +\n  geom_boxplot(width= 0.4, outlier.colour = \"grey20\", outlier.size = 1, \n               outlier.alpha = 0.3) +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"black\",        \n               size=2) +  \n  coord_cartesian(ylim = c(0,1000)) +\n  scale_color_manual(values=c(\"#c73824\", \"#0477bf\")) +\n  theme_fivethirtyeight() +\n  labs(title=\"Science\") +\n  scale_y_continuous(breaks = seq(0, 1000, by = 200)) + \n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        plot.title=element_text(size= 12, hjust= 0.5),\n        axis.text = element_text(size= 10),\n        legend.position = \"none\") \n\np2 &lt;- (math_gender + read_gender + sci_gender) +\n    plot_annotation(title= \"Performance by Gender Across Subjects\",\n                    theme = theme_fivethirtyeight() +\n                      theme(plot.title = element_text(size = 15, hjust = 0.5)\n                      )\n    )\n\np2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "version updated: 22 Jan 3PM, fixed the error of EDA4 Description was not pushed to Git properly.\nversion updated: 22 Jan 7PM, included EDA5 and Appendix."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data",
    "title": "Take-home Exercise 1",
    "section": "Data",
    "text": "Data\nThe 2022 Programme for International Student Assessment (PISA) data, released on December 5, 2022, assesses global education systems by testing 15-year-olds in mathematics, reading, and science. The PISA 2022 database includes responses from students, schools, and parents across five data files, primarily in SAS and SPSS formats. This assignment, will only focus on the Student Questionnaire Data File. Additional resources like codebooks, and the PISA 2022 Technical Report are also used as supplementary materials to understand the dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#project-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#project-task",
    "title": "Take-home Exercise 1",
    "section": "Project Task",
    "text": "Project Task\nIn this take-home exercise, 5 EDA visualisations will be used to understand:\n\nthe distribution of Singapore studentsâ€™ performance in mathematics, reading, and science,\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#loading-r-packages",
    "title": "Take-home Exercise 1",
    "section": "Loading R packages",
    "text": "Loading R packages\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               ggdist, ggridges,\n               colorspace,ggstatsplot, \n               tidyverse)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-filtered-pisa-data-for-sg-students",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-filtered-pisa-data-for-sg-students",
    "title": "Take-home Exercise 1",
    "section": "Importing filtered PISA data for SG students",
    "text": "Importing filtered PISA data for SG students\nThe code chunk below uses read_rds() to import PISA SG data into R environment.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#summary-statistics-of-stu_qqq_sg",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#summary-statistics-of-stu_qqq_sg",
    "title": "Take-home Exercise 1",
    "section": "Summary Statistics of stu_qqq_SG",
    "text": "Summary Statistics of stu_qqq_SG\n\nDataframe DimensionColumn AttributesData Health\n\n\nthe code below use dim() to check the dataframe dimension\n\ndim(stu_qqq_SG)\n\n[1] 6606 1279\n\n\nThe PISA dataset comprises 6606 observations, with 1279 variables. Displaying a sample using standard functions like head() is not feasible due to the sheer number of columns. Similarly, including all columns in the analysis would be impractical.\nA more effective approach involves a thorough understanding of the datasetâ€™s fields, enabling the selection of relevant variables that are crucial to the analysis.\n\n\nTo understand the structure and content available of the columns in the stu_qqq_SG dataframe, the code below retrieve the attributes of the first three columns\n\nlapply(stu_qqq_SG, attributes)[0:3]\n\n$CNT\n$CNT$label\n[1] \"Country code 3-character\"\n\n\n$CNTRYID\n$CNTRYID$label\n[1] \"Country Identifier\"\n\n\n$CNTSCHID\n$CNTSCHID$label\n[1] \"Intl. School ID\"\n\n\nThe dataframe includes label which can be served as column descriptions, with this, making it easier to identify columns which are more relevant to the analysis.\n\n\nthe code below use anyDuplicated() to check if any duplicated entries in the dataset.\n\nanyDuplicated(stu_qqq_SG)\n\n[1] 0\n\n\nthe code use is.na() to check for total number of missing entries in the dataset.\n\nsum(is.na(stu_qqq_SG))\n\n[1] 4168500\n\n\nAll observations are unique. But a huge number of missing values are found in the given data frame. This suggests that another potential approach to handle this dataset is to eliminate columns with a high percentage of missing values.\n\n\n\n\n\n\nNote\n\n\n\naccording to PISA, it was suggested that cases with fewer than three valid responses received score â€œ99â€, indicating a missing scale score due to insufficient responses. This value should be considered as missing values and filtered out if existed in the data set used later."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#understanding-the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#understanding-the-data",
    "title": "Take-home Exercise 1",
    "section": "Understanding the Data",
    "text": "Understanding the Data\nGiven the challenge in understanding the labels under each column attributes, which act as column descriptors, and the cumbersome process of examining them one by one, creating a data schema table is a more efficient approach for facilitating in-depth analysis\n\nData Schema OverviewSchema Distribution Insights\n\n\nThe code chunk below uses tibble() to create a data frame. This data frame summarizes essential attributes of each column in the 'stu_qqq_SG', such as column descriptions, the number of missing values, and the count of unique values. The first five rows are displayed for a preliminary understanding of the data structure.\n\n\nShow the code\ndata_schema &lt;- tibble(\n  column_name = names(stu_qqq_SG),\n  description = sapply(stu_qqq_SG, function(col) {\n    lbl &lt;- attr(col, \"label\")\n    if(is.null(lbl)) \"\" else lbl # If label is null, return an empty string\n  }),\n  data_type = sapply(stu_qqq_SG, class),\n  num_missing_values = sapply(stu_qqq_SG, function(col) {\n    sum(is.na(col)) # Calculates the number of NA values in the column\n  }),\n  num_unique_values = sapply(stu_qqq_SG, function(col) {\n    length(unique(col)) # Calculates the number of unique values in the column\n  })\n)\n\nhead(data_schema, 5)\n\n\n# A tibble: 5 Ã— 5\n  column_name description         data_type num_missing_values num_unique_values\n  &lt;chr&gt;       &lt;chr&gt;               &lt;chr&gt;                  &lt;int&gt;             &lt;int&gt;\n1 CNT         Country code 3-chaâ€¦ character                  0                 1\n2 CNTRYID     Country Identifier  numeric                    0                 1\n3 CNTSCHID    Intl. School ID     numeric                    0               164\n4 CNTSTUID    Intl. Student ID    numeric                    0              6606\n5 CYC         PISA Assessment Cyâ€¦ character                  0                 1\n\n\n\n\nTo delve into the data schemaâ€™s specifics, summary() is used to examine the overall distribution of unique and missing values across the dataset. This step is crucial for understanding the original dataâ€™s structure and determining the dataâ€™s health and integrity.\n\n\nShow the code\nsummary(data_schema)\n\n\n column_name        description         data_type         num_missing_values\n Length:1279        Length:1279        Length:1279        Min.   :   0.0    \n Class :character   Class :character   Class :character   1st Qu.:  69.5    \n Mode  :character   Mode  :character   Mode  :character   Median :3294.0    \n                                                          Mean   :3259.2    \n                                                          3rd Qu.:6606.0    \n                                                          Max.   :6606.0    \n num_unique_values\n Min.   :   1.0   \n 1st Qu.:   1.0   \n Median :   5.0   \n Mean   : 711.3   \n 3rd Qu.:   7.0   \n Max.   :6606.0   \n\n\nThe summary indicates that over 25% of the columns are filled with missing values, and over 25% contain only one unique value. As previously noted, such columns with uniform data may not contribute meaningful insights and could be considered for exclusion to streamline the analysis process.\n\n\n\n\nColumn Description\nA detailed examination of the column descriptions, along with the questionnaire details provided on the technical report reveals the data schema of the 'stu_qqq_SG' dataframe. The following diagram illustrates the data structure:\n\n\n\n\nflowchart TD\nstyle A fill:#000000,color:#ffffff\nstyle B fill:#c73824,color:#ffffff\nstyle H fill:#c73824,color:#ffffff\nstyle I fill:#c73824,color:#ffffff\nA(stu_qqq_SG) --&gt; B(STxxx)\nA --&gt; C(ICxxx)\nA --&gt; D(FLxxx)\nA --&gt; E(WBxxx)\nA --&gt; F(PAxxx)\nA --&gt; G(Wxxx)\nA --&gt; H(PVxxx)\nA --&gt; I(Others)\n\n\n\n\n\n\nThe dataset provides the information of following:\n\nST: Student Questionnaire, including student demographics and Economic, Social, and Cultural Status (ESCS) information. Further details can be found in Annex 5.A of the technical report.\nIC: Information Communication Technology Questionnaire (ICTQ).\nFL: Financial Literacy Questionnaire (FLQ).\nWB: Well-being Questionnaire (WBQ).\nPA: Parent Questionnaire (PaQ).\nW: Final trimmed nonresponse adjusted student weight.\nPV: Plausible Values representing student performance.\nOthers: This includes basic student information and derived variables like the ESCS index. Details of all field could be found under the technical reports\n\nFor this exercise, which focuses on the performance distribution of Singapore students in mathematics, reading, and science, and their correlation with school type, gender, and socioeconomic status, the ST, PV, and Others categories are particularly pertinent. A deeper dive into these fields is available in the subsequent chapter."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#dive-into-relevant-information",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#dive-into-relevant-information",
    "title": "Take-home Exercise 1",
    "section": "Dive into Relevant Information",
    "text": "Dive into Relevant Information\n\nPerformance in Mathematics, Reading, and Science\nThe dataset organizes Plausible Values (PVs) across subjects: Mathematics, Reading, and Science. Mathematics, for instance, is subdivided into specific topics representing various mathematical competencies.\n\n\n\n\nflowchart LR\nstyle A fill:#000000,color:#ffffff\nstyle B fill:#c73824,color:#ffffff\nstyle C fill:#c73824,color:#ffffff\nstyle D fill:#c73824,color:#ffffff\nA(PV) --&gt; B(Maths)\nA --&gt; C(Reading)\nA --- D(Science)\nB --- E(Change and Relationships)\nB --- F(Quantity)\nB --- G(Space and Shape)\nB --- H(Uncertainty and Data)\nB --- I(Employing Mathematical Concepts, Facts, and Procedures)\nB --- J(Formulating Situations Mathematically)\nB--- K(Interpreting, Applying, and Evaluating Mathematical Outcomes)\nB --- L(Reasoning)\n\n\n\n\n\nFor a balanced analysis of student performance across subjects, itâ€™s crucial to maintain a consistent level of detail. Each PV score comprises 10 distinct values, reflecting various potential levels of student performance. Therefore, determining the appropriate level of aggregation is essential for an unbiased and comprehensive analysis.\n\nAggregation of PV ScoresUsing PISA methodData Manipulation\n\n\nSelecting a single PV simplifies analysis but may underestimate standard errors and ignoring the variability. PISA analysts note that in large samples, the choice between one and multiple PVs may not significantly alter results.\nAnother approach is to calculate the average PV score for each student in a given subject to represent their overall performance. However, as noted in this literature, this method can severely underestimate standard errors, especially if only a single PV is used.\n\n\nShow the code\n# Selecting the relevant data\nselected_data &lt;- stu_qqq_SG %&gt;% \n  select(CNTSTUID, PV1MATH, PV2MATH, PV3MATH, PV4MATH, PV5MATH, PV6MATH, PV7MATH, PV8MATH, PV9MATH, PV10MATH)\n\n# Calculate the average PV score for each student\nselected_data &lt;- selected_data %&gt;%\n  mutate(AVG_MATH = rowMeans(select(., starts_with(\"PV\")), na.rm = TRUE))\n\n# Reshape the data to long format including the average\nmath_PV &lt;- selected_data %&gt;% \n  pivot_longer(cols = -CNTSTUID, names_to = \"PV_math\", values_to = \"score\")\n\n# Create a custom order for the plot\nmath_PV$PV_math &lt;- factor(math_PV$PV_math, levels = c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \n                                                      \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \"AVG_MATH\"))\n\n# Plot the boxplot with custom order and refined theme settings\nggplot(math_PV, aes(x = PV_math, y = score)) + \n  geom_boxplot() + \n  theme_minimal() +\n  labs(title = \"Boxplot of Math Scores for Each Plausible Value and Average\", \n       x = \"Math Performance\", \n       ) +\n  scale_x_discrete(labels = c(\"PV1\", \"PV2\", \"PV3\", \"PV4\", \"PV5\", \"PV6\", \"PV7\", \"PV8\", \"PV9\", \"PV10\", \"Average\")) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        axis.title.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks.x = element_line(),      \n        axis.line.x = element_line())\n\n\n\n\n\nThe boxplot demonstrates that studentsâ€™ performances vary across the different Plausible Values (PVs) in mathematics, suggesting that a single PV may not fully capture an individualâ€™s skillset. Averaging the scores provides a generalized view to identify broad patterns and trends. Additionally, calculating the variance of PVs for each student offers insights into their performance stability across different mathematical competencies.\n\n\nalternatively, under How to prepare and analyse the PISA database, a step by step instruction is illustrated as below:\n\nthe code chunk below compare the difference of using PISA and aggregation method in a historgam to illustrate the distribution\n\n\nShow the code\nall_PVs &lt;- pivot_longer(selected_data, cols = starts_with(\"PV\"), names_to = \"PV\", values_to = \"score\")\n\nggplot() +\n  geom_density(data = all_PVs, aes(x = score, fill = \"PISA Method\"), color = \"grey90\", fill = \"grey90\") + \n  geom_density(data = selected_data, aes(x = AVG_MATH, linetype = \"Agg Method\"), color = \"#c73824\", size = 0.8, linetype = \"dotted\") +\n  labs(title = \"Density Plot of PV Math Scores for SG Students\", x = \"Score\", y = \"Density\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    axis.ticks.x = element_line(),\n    axis.line.x = element_line(),\n    legend.position = \"right\",\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 8)\n  ) +\n  guides(fill = guide_legend(title = \"Legend\"), linetype = guide_legend(title = \"Legend\"))\n\n\n\n\n\nThe density plot provides a visual comparison between the detailed PISA method(in grey shde) and the simplified aggregate(in red dotted line) approach. Although the PISA method offers a comprehensive view, capturing the full spectrum of data, its expansive range may not significantly enhance the understanding of general subject performance.\nThis plot reveals that the PISA method, while meticulous, may add unnecessary complexity to the overall evaluation of student performance. Unless the analysis aims to investigate specific patterns within the PVs, the broader range of the histogram does not contribute additional insights into the overarching trends.\n\n\nIn summary, averaging PVs provides a snapshot of general student performance, revealing dominant trends and serving as a foundation for further detailed analysis. The inclusion of variance calculations adds depth by assessing the balance and consistency of individual performances.\nThe code chunk below created new aggregated columns for PV values.\n\nstu_qqq_SG &lt;- stu_qqq_SG %&gt;%\n  mutate(\n    PV_avg_math = rowMeans(select(., starts_with(\"PV\") & ends_with(\"MATH\")), na.rm = TRUE),\n    PV_var_math = apply(select(., starts_with(\"PV\") & ends_with(\"MATH\")), 1, var, na.rm = TRUE),\n    PV_avg_read = rowMeans(select(., starts_with(\"PV\") & ends_with(\"READ\")), na.rm = TRUE),\n    PV_var_read = apply(select(., starts_with(\"PV\") & ends_with(\"READ\")), 1, var, na.rm = TRUE),\n    PV_avg_scie = rowMeans(select(., starts_with(\"PV\") & ends_with(\"SCIE\")), na.rm = TRUE),\n    PV_var_scie = apply(select(., starts_with(\"PV\") & ends_with(\"SCIE\")), 1, var, na.rm = TRUE)\n  )\n\nAs the variance value contains extreme values, these data are also binned into â€˜lowâ€™, â€˜lower-midâ€™, â€˜upper-midâ€™, â€˜highâ€™ to indicate studentsâ€™ performance consistency\n\nstu_qqq_SG &lt;- stu_qqq_SG %&gt;%\n  mutate(\n    consistency_math = cut(\n      PV_var_math,\n      breaks = quantile(PV_var_math, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE),\n      labels = c('high', 'upper-mid', 'lower-mid', 'low'),\n      include.lowest = TRUE\n    ),\n    consistency_read = cut(\n      PV_var_read,\n      breaks = quantile(PV_var_read, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE),\n      labels = c('high', 'upper-mid', 'lower-mid', 'low'),\n      include.lowest = TRUE\n    ),\n    consistency_sci = cut(\n      PV_var_scie,\n      breaks = quantile(PV_var_scie, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE),\n      labels = c('high', 'upper-mid', 'lower-mid', 'low'),\n      include.lowest = TRUE\n    )\n  )\n\n\n\n\n\n\nSchools\nBased on the documentation and column distribution, 3 columns listed below brought my attention:\n\nCNTSCHID: Intl. School ID\nPROGN: Unique national study programme code\nPQSCHOOL: School quality (WLE)\n\n\nData SummaryData QualityData Distribution - PROGNData Distribution - CNTSCHID\n\n\nThe code below use filter() to select relevant columns from the data schema\n\ndata_schema %&gt;%\n  filter(column_name %in% c('CNTSCHID', 'PROGN', 'PQSCHOOL'))\n\n# A tibble: 3 Ã— 5\n  column_name description         data_type num_missing_values num_unique_values\n  &lt;chr&gt;       &lt;chr&gt;               &lt;chr&gt;                  &lt;int&gt;             &lt;int&gt;\n1 CNTSCHID    Intl. School ID     numeric                    0               164\n2 PROGN       Unique national stâ€¦ character                  0                 2\n3 PQSCHOOL    School quality (WLâ€¦ numeric                 6606                 1\n\n\nPQSCHOOL intended to represent the quality of schools as evaluated by parents, as indicated in the technical report chapter 19. However, itâ€™s worth noting that for the Singapore dataset, all values are missing. This suggests that this question was not included in the context of Singapore.\nPROGN reveals the presence of two unique programs in the dataset. A closer investigation, based on the codebook provided, indicates that these two values represent different levels of education. This distinction could introduce bias when comparing lower and upper secondary education. Therefore, further investigation is required to understand the distribution of these values and decide whether to analyze the two groups separately.\n\n07020001: Singapore : Lower Secondary education\n07020001: Singapore : Upper Secondary education\n\nAlternatively, CNTSCHID provides a unique index for different school IDs. None of the value is missing, and there are 164 unique entries for this column.\n\n\nThe code chunk below converts this column into a factor which better represents the categories rather than quantitative values\n\nstu_qqq_SG$CNTSCHID &lt;- factor(stu_qqq_SG$CNTSCHID)\n\n\n\nThe code below calculates the distribution of the two different programs surveyed in the dataset. Notably, we observed that 07020001, which represents Lower Secondary Education, comprises only 72 entries, approximately 1% of the total dataset. Given this small proportion, dropping this data may be a prudent choice to avoid biases stemming from the differing levels of education surveyed.\n\nstu_qqq_SG %&gt;%\n  count(PROGN) %&gt;%\n  mutate(Percentage = n / sum(n) * 100)\n\n# A tibble: 2 Ã— 3\n  PROGN        n Percentage\n  &lt;chr&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 07020001    72       1.09\n2 07020002  6534      98.9 \n\n\nthe code chunk below is to retain only the records related to Upper Secondary Education (where PROGN == \"07020002\")\n\nstu_qqq_SG_filtered &lt;- stu_qqq_SG %&gt;%\n  filter(PROGN == \"07020002\")\n\n\n\nThe CNTSCHID column contains 168 unique values, indicating the participation of 168 different schools in the exercise. However, without further information on school types or educational quality, itâ€™s challenging to draw conclusions solely from this data. To gain insight into the source of the students and the distribution of the number of students from each school, we group the dataset by school ID (CNTSCHID) and calculate the student count per school. This is then visualized with a box plot, showing the number of students per school.\n\n\nShow the code\nstudent_count_per_school &lt;- stu_qqq_SG_filtered %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise(NumberOfStudents = n())\n\nggplot(student_count_per_school, aes(x = NumberOfStudents)) +\n  stat_dots(side = \"top\",\n            justification = 1.5,\n            binwidth = 0.8,\n            dotsize = 0.8) + \n  geom_boxplot(width = 0.20, position = position_nudge(y = 0.1)) +  \n  scale_x_continuous(labels = scales::label_comma(),  # Format labels with commas\n                     breaks = seq(0, max(student_count_per_school$NumberOfStudents)+5, by = 5), \n                     limits = c(0, max(student_count_per_school$NumberOfStudents)+5), expand = c(0, 0)) +  \n  labs(title = \"Distribution of Student Number from Each Surveyed School\", x = \"Number of Students/School\") + \n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major = element_blank(),  \n        panel.grid.minor = element_blank(),\n        axis.ticks.x = element_line(),      \n        axis.line.x = element_line()) \n\n\n\n\n\n\n\n\n\n\nGender\nBased on the documentation and column distribution, ST004D01T represents Student (Standardized) Gender.\n\nData SummaryData QualityData Distribution\n\n\nThe code chunk below checks the summary of the column in the dataset. This data is currently in numeric, and need to be adjusted as categorical, as 1 and 2 indicates the index of the gender.\n\ndata_schema %&gt;%\n  filter(column_name %in% c('ST004D01T'))\n\n# A tibble: 1 Ã— 5\n  column_name description         data_type num_missing_values num_unique_values\n  &lt;chr&gt;       &lt;chr&gt;               &lt;chr&gt;                  &lt;int&gt;             &lt;int&gt;\n1 ST004D01T   Student (Standardiâ€¦ numeric                    0                 2\n\n\nThere are no missing entries in this column, and it contains two unique values representing gender categories.\n\n\nThe code chunk below converts this column into a factor which better represents the categories (like gender) rather than quantitative values\n\nstu_qqq_SG_filtered$ST004D01T &lt;- factor(stu_qqq_SG_filtered$ST004D01T)\n\n\n\nThe code chunk below give an initial distribution to understand the number of students in each gender participating in the survey\n\n\nShow the code\nggplot(stu_qqq_SG_filtered, aes(x = ST004D01T)) +\n  geom_bar(fill = \"grey80\", alpha = 0.75, width = 0.5) +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5) + \n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.major = element_blank(),  # Remove major grid lines\n    panel.grid.minor = element_blank(),  # Remove minor grid lines\n    axis.title.y = element_blank(),  # Remove y-axis title\n    axis.text.y = element_blank(),  # Remove y-axis text (labels)\n    axis.ticks.y = element_blank(),  # Remove y-axis ticks\n    axis.ticks.x = element_line(),      \n    axis.line.x = element_line() \n  ) +\n  labs(title = \"Number of Students by Gender\", x = \"Gender Index\", y = \"\")\n\n\n\n\n\nThere are 3248 students in gender category 1 and 3358 students in gender category 2. The sample size is substantial, allowing for generalization. Additionally, the sample population exhibits a balanced distribution between both genders, enabling a fair and comprehensive analysis with equal representation.\n\n\n\n\n\nSocioeconomic Status\nIn the Student Questionnaire, there are several questions labeled as STxxx, which pertain to the Economic, Social, and Cultural Status (ESCS) section, as outlined in Annex 5.A of the technical report. Additionally, there is a distinct column labeled ESCS, described as an index representing economic, social, and cultural status.\nThis composite ESCS index, as illustrated in chapter 19 of the technical report, is derived from the responses in the Student Questionnaire. It serves as a comprehensive composite value, encompassing various influencing factors. Consequently, this ESCS index should be utilized for analytical purposes.\n\n\nData SummaryData QualityData DistributionData Manipulation\n\n\nThe code below use filter() to select relevant columns from the data schema\n\ndata_schema %&gt;%\n  filter(column_name %in% c('ESCS'))\n\n# A tibble: 1 Ã— 5\n  column_name description         data_type num_missing_values num_unique_values\n  &lt;chr&gt;       &lt;chr&gt;               &lt;chr&gt;                  &lt;int&gt;             &lt;int&gt;\n1 ESCS        Index of economic,â€¦ numeric                   47              5815\n\n\nThe code chunk below use head() to visulize the first few values of ESCS\n\nhead(stu_qqq_SG_filtered$ESCS)\n\n[1]  0.1836  0.8261 -1.0357 -0.9606  0.0856  0.1268\n\n\nThe ESCS contains an index which is a numeric value indicating the economic, social and cultural status using both positive and negative floats.\n\n\nThere are only 47 missing entries, which account for less than 0.8% of the dataset. Such a small proportion is considered insignificant for analysis, and these data points can be safely dropped in a dataset of this size.\nCode below to calculate the number of missing values and the percentage of missing values\n\nstu_qqq_SG_filtered %&gt;%\n  summarise(MissingCount = sum(is.na(ESCS)),\n            PercentageMissing = (MissingCount / n()) * 100)\n\n# A tibble: 1 Ã— 2\n  MissingCount PercentageMissing\n         &lt;int&gt;             &lt;dbl&gt;\n1           47             0.719\n\n\nThe code chunk below is to select non-missing values for ESCS in the dataset\n\nfiltered_data &lt;- stu_qqq_SG_filtered %&gt;%\n  filter(!is.na(ESCS))\n\n\n\nThe code chunk below display the distribution of ESCS index among surveryed students\n\n\nShow the code\nggplot(filtered_data, aes(x = ESCS)) +\n  geom_density(fill = \"grey90\", alpha = 0.5) +  # Density plot\n  geom_boxplot(width = 0.1) +  # Boxplot\n  theme_minimal() +\n  labs(title = \"Density Plot and Boxplot for ESCS Index Distribution\", x = \"ESCS Index\") +\n  # adjust x-axis label interval\n  scale_x_continuous(breaks = seq(-4, 4, by = 0.5)) +\n  # style\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(), \n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(),\n    axis.line.x = element_line()\n  )\n\n\n\n\n\nThe density plot, shaded in grey, suggests a roughly normal distribution of ESCS scores, with the majority of data clustering around the center. This is indicated by the bell-shaped curve with its peak at the mean ESCS Index value. Overall, the plot suggests a symmetrical distribution of ESCS Index values, with no significant skewness or outliers.\n\n\nThe column is further binned into lower, lower_mid, upper_mid, and upper as ESCS_bin:\n\nbin_boundaries &lt;- quantile(filtered_data$ESCS, probs = c(0, 0.25, 0.5, 0.75, 1))\nbin_labels &lt;- c(\"lower\", \"lower-mid\", \"upper-min\", \"upper\")\nfiltered_data$ESCS_bin &lt;- cut(filtered_data$ESCS, breaks = bin_boundaries, labels = bin_labels, include.lowest = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#filtering-dataset",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#filtering-dataset",
    "title": "Take-home Exercise 1",
    "section": "Filtering Dataset",
    "text": "Filtering Dataset\nThe code chunk below used to only include the relevant data fields for analysis. We have intentionally retained all â€œPVsâ€ (Potential Variables) to ensure their availability should we require a more comprehensive examination in the future.\n\n\nShow the code\npisa_new &lt;- filtered_data %&gt;%\n  select(CNTSTUID, CNTSCHID, ST004D01T, starts_with(\"PV\"), ESCS, ESCS_bin, starts_with('consistency'))\n\n# View the resulting dataframe\nhead(pisa_new,5)\n\n\n# A tibble: 5 Ã— 124\n  CNTSTUID CNTSCHID ST004D01T PV1MATH PV2MATH PV3MATH PV4MATH PV5MATH PV6MATH\n     &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 70200001 70200052 1            639.    601.    621.    632.    579.    592.\n2 70200002 70200134 2            697.    754.    672.    657.    621.    656.\n3 70200003 70200112 2            694.    654.    697.    646.    678.    644.\n4 70200004 70200004 2            427.    410.    424.    389.    331.    380.\n5 70200005 70200152 1            436.    453.    392.    440.    443.    453.\n# â„¹ 115 more variables: PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;,\n#   PV10MATH &lt;dbl&gt;, PV1READ &lt;dbl&gt;, PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;,\n#   PV5READ &lt;dbl&gt;, PV6READ &lt;dbl&gt;, PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;,\n#   PV10READ &lt;dbl&gt;, PV1SCIE &lt;dbl&gt;, PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;,\n#   PV5SCIE &lt;dbl&gt;, PV6SCIE &lt;dbl&gt;, PV7SCIE &lt;dbl&gt;, PV8SCIE &lt;dbl&gt;, PV9SCIE &lt;dbl&gt;,\n#   PV10SCIE &lt;dbl&gt;, PV1MCCR &lt;dbl&gt;, PV2MCCR &lt;dbl&gt;, PV3MCCR &lt;dbl&gt;, PV4MCCR &lt;dbl&gt;,\n#   PV5MCCR &lt;dbl&gt;, PV6MCCR &lt;dbl&gt;, PV7MCCR &lt;dbl&gt;, PV8MCCR &lt;dbl&gt;, â€¦\n\n\nThe code below changes the datatype for the unique student ID\n\n\nShow the code\npisa_new$CNTSTUID &lt;- as.factor(pisa_new$CNTSTUID)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-1-distribution-of-singapore-students-performance-in-mathematics-reading-and-science",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-1-distribution-of-singapore-students-performance-in-mathematics-reading-and-science",
    "title": "Take-home Exercise 1",
    "section": "EDA 1: Distribution of Singapore Studentsâ€™ Performance in mathematics, Reading, and Science",
    "text": "EDA 1: Distribution of Singapore Studentsâ€™ Performance in mathematics, Reading, and Science\nThe code chunk below creates a histogram to illustrate the distribution of Student Performance across subjects.\n\n\nShow the code\n# Select average columns\npv_avg &lt;- pisa_new %&gt;%\n  select(PV_avg_math, PV_avg_read, PV_avg_scie)\n\n# Reshape the data for plotting\npv_avg &lt;- gather(pv_avg, key = \"Subject\", value = \"Average_Score\")\n\n# Create a labeller function to rename facet titles\ncustom_labeller &lt;- function(variable, value) {\n  if (variable == \"Subject\") {\n    value &lt;- case_when(\n      value == \"PV_avg_math\" ~ \"Math\",\n      value == \"PV_avg_read\" ~ \"Read\",\n      value == \"PV_avg_scie\" ~ \"Science\",\n      TRUE ~ as.character(value)\n    )\n  }\n  return(value)\n}\n\navg_dist &lt;- ggplot(pv_avg, \n       aes(x = Average_Score, fill = Subject)) +\n  geom_histogram(bins = 25, boundary = 5, color = \"white\", fill = \"grey80\") +\n  geom_boxplot(width = 50, outlier.size = 0.2, outlier.color = \"#c73824\", position = position_dodge(width = 0.75), fill = \"white\", color = \"black\") +\n  labs(title = \"Distribution of Performance among Subjects\", x = \"Average Score\", y = \"Frequency\") +\n  theme_minimal() +\n  facet_wrap(~ Subject, scales = \"free\", nrow = 3, label = custom_labeller) + \n  theme(\n    strip.text = element_text(size = 12, face = \"bold\")\n  ) +\n  coord_cartesian(xlim = c(200, 800), ylim = c(0, 800)) +  # Set x and y limits\n  geom_vline(data = pv_avg %&gt;%\n               group_by(Subject) %&gt;%\n               summarize(mean_score = mean(Average_Score), median_score = median(Average_Score)),\n             aes(xintercept = mean_score), color = \"#c73824\", linetype = \"dashed\") +  # Add mean vertical lines\n  geom_text(data = pv_avg %&gt;%\n               group_by(Subject) %&gt;%\n               summarize(mean_score = mean(Average_Score), median_score = median(Average_Score)),\n             aes(x = mean_score - 50, y = Inf, label = paste(\"Mean:\", round(mean_score, 2))), color = \"#c73824\", vjust = 1, size = 3) +  \n  geom_vline(data = pv_avg %&gt;%\n               group_by(Subject) %&gt;%\n               summarize(mean_score = mean(Average_Score), median_score = median(Average_Score)),\n             aes(xintercept = median_score), color = \"#0477bf\", linetype = \"dashed\") +  # Add median vertical lines\n  geom_text(data = pv_avg %&gt;%\n               group_by(Subject) %&gt;%\n               summarize(mean_score = mean(Average_Score), median_score = median(Average_Score)),\n             aes(x = median_score + 50, y = Inf, label = paste(\"Median:\", round(median_score, 2))), color = \"#0477bf\", vjust = 1, size = 3) +  \n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(), \n    axis.title.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey\"),  \n    axis.line.x = element_line(color = \"grey\")\n    )    \n\n# avg_dist\n\n\nThe code chunk below creates a density plot to illustrate the distribution of Performance Consistency among Subject.\n\n\nShow the code\n# Select var columns\npv_var &lt;- pisa_new %&gt;%\n  select(Math = PV_var_math, Science = PV_var_scie, Read = PV_var_read)\npv_var &lt;- gather(pv_var, key = \"Subject\", value = \"Score_Variance\")\nvar_dist &lt;- ggplot(pv_var, aes(x = Score_Variance, fill = Subject)) +\n  geom_density(alpha = 0.2, color = \"white\", size = 0.3) +  \n  labs(title = \"Distribution of Performance Consistency among Subjects\", x = \"Variance\") +  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(), \n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey\"),  \n    axis.line.x = element_line(color = \"grey\")\n  ) +\n  scale_x_continuous(limits = c(0, 6000))  # Adjust x-axis limits\n# var_dist\n\n\nThe code chunk below creates a heatmap to illustrate the relationship between students preformance in different subjects.\n\n\nShow the code\n# Select variables of interest and rename them\nselected_vars &lt;- pisa_new %&gt;%\n  select(PV_avg_math, PV_avg_read, PV_avg_scie, PV_var_math, PV_var_read, PV_var_scie) %&gt;%\n  rename(\n    \"Math Avg\" = PV_avg_math,\n    \"Read Avg\" = PV_avg_read,\n    \"Science Avg\" = PV_avg_scie,\n    \"Math Variance\" = PV_var_math,\n    \"Read Variance\" = PV_var_read,\n    \"Science Variance\" = PV_var_scie\n  )\n\n# Create the correlogram\ncorrelogram &lt;- ggcorrmat(\n  data = selected_vars,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title = \"Correlogram for Student Performance\",\n  subtitle = \"Subject Performance is relatively related in a linear manner\"\n)\n\n# Display the correlogram\n# correlogram\n\n\nThe code chunk below patch all the plots together in 1 plot.\n\n\nShow the code\nplot1 &lt;- avg_dist\nplot3 &lt;- var_dist\nplot2 &lt;- correlogram\n\npatchwork &lt;- (plot1 | (plot3 / plot2))\npatchwork &lt;- patchwork +\n  plot_annotation(title = \"Distribution of Singapore Students Performance in Mathematics, Reading, and Science\", \n                  theme = theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5)))\n\npatchwork\n\n\n\n\n\nThe visualizations present an overview of the performance distribution of Singapore students in mathematics, reading, and science.\n\nSG students preforms better in Maths and Science compared to readings in general, with a higher mean, median, and along with a more prominent upper quartile boxplots.\nA slightly left-skewed distribution observed for math, but nearly symmetric for reading and science. This suggests that while students generally perform well, a significant number excel in math compared to the other subjects.\nReading, however, shows more outliers on the lower end, implying a struggle among a notable group of students\nThe variance in reading scores is the most pronounced, indicating inconsistent performance within the subject, hinting at possible disparities in studentsâ€™ abilities to tackle different reading materials or question types. This variability could result in a wider spread of scores, influencing the overall lower mean in reading.\nIn contrast, the lower variance in mathematics and science suggests more uniform achievement, pointing towards consistent individual performance in these subjects.\nThe correlogram reinforces the interconnection between subjects, with math and science exhibiting a particularly strong positive correlation. However, the relationship between individual consistency (variance) and overall grades is not immediately apparent, suggesting that high variance in subject performance does not necessarily correlate with lower overall grades or the opposite."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-2-relationship-between-performances-with-gender",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-2-relationship-between-performances-with-gender",
    "title": "Take-home Exercise 1",
    "section": "EDA 2: Relationship between Performances with Gender",
    "text": "EDA 2: Relationship between Performances with Gender\nThe code chunk below use a Half Eye graph and Boxplot to understand the SG students maths performance by gender.\n\n\nShow the code\npv_gender &lt;- pisa_new %&gt;%\n  select(Math = PV_avg_math, Science = PV_avg_scie, Reading = PV_avg_read, Gender = ST004D01T, consistency_math, consistency_read, consistency_sci)\n\npv_math_gender &lt;- ggplot(pv_gender, \n       aes(y = Gender, \n           x = Math)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               height = 0.75) +\n  geom_boxplot(width = .2,\n               outlier.shape = NA) + \n  theme_minimal() +\n  labs(title = \"Distribution of Students' Maths Performance by Gender\", x = \"average PV\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(), \n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey\"),  \n    axis.line.x = element_line(color = \"grey\"))\n\n# Calculate mean and median\nmean_math &lt;- mean(pv_gender$Math, na.rm = TRUE)\nmedian_math &lt;- median(pv_gender$Math, na.rm = TRUE)\n\n# Add vertical lines for mean and median and label them with shifted text\npv_math_gender &lt;- pv_math_gender +\n  geom_vline(xintercept = mean_math, color = \"#c73824\", linetype = \"dashed\") +\n  geom_vline(xintercept = median_math, color = \"#0477bf\", linetype = \"dashed\") +\n  annotate(\"text\", x = mean_math - 70, y = Inf, vjust = 1, label = paste(\"Mean =\", round(mean_math, 2)), color = \"#c73824\") +\n  annotate(\"text\", x = median_math + 70, y = Inf, vjust = 1, label = paste(\"Median =\", round(median_math, 2)), color = \"#0477bf\")\n\n# pv_math_gender\n\n\nThe code chunk below use a Half Eye graph and Boxplot to understand the SG students science performance by gender.\n\n\nShow the code\npv_sci_gender &lt;- ggplot(pv_gender, \n       aes(y = Gender, \n           x = Science)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               height = 0.75) +\n  geom_boxplot(width = .2,\n               outlier.shape = NA) + \n  theme_minimal() +\n  labs(title = \"Distribution of Students' Science Performance by Gender\", x = \"average PV\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(), \n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey\"),  \n    axis.line.x = element_line(color = \"grey\"))\n\n# Calculate mean and median\nmean_sci &lt;- mean(pv_gender$Science, na.rm = TRUE)\nmedian_sci &lt;- median(pv_gender$Science, na.rm = TRUE)\n\n# Add vertical lines for mean and median and label them with shifted text\npv_sci_gender &lt;- pv_sci_gender +\n  geom_vline(xintercept = mean_sci, color = \"#c73824\", linetype = \"dashed\") +\n  geom_vline(xintercept = median_sci, color = \"#0477bf\", linetype = \"dashed\") +\n  annotate(\"text\", x = mean_sci - 70, y = Inf, vjust = 1, label = paste(\"Mean =\", round(mean_sci, 2)), color = \"#c73824\") +\n  annotate(\"text\", x = median_sci + 70, y = Inf, vjust = 1, label = paste(\"Median =\", round(median_sci, 2)), color = \"#0477bf\")\n\n# pv_sci_gender\n\n\nThe code chunk below use a Half Eye graph and Boxplot to understand the SG students reading performance by gender.\n\n\nShow the code\npv_read_gender &lt;- ggplot(pv_gender, \n       aes(y = Gender, \n           x = Reading)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               height = 0.75) +\n  geom_boxplot(width = .2,\n               outlier.shape = NA) + \n  theme_minimal() +\n  labs(title = \"Distribution of Students' Reading Performance by Gender\", x = \"average PV\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(), \n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey\"),  \n    axis.line.x = element_line(color = \"grey\"))\n\n# Calculate mean and median\nmean_read &lt;- mean(pv_gender$Reading, na.rm = TRUE)\nmedian_read &lt;- median(pv_gender$Reading, na.rm = TRUE)\n\n# Add vertical lines for mean and median and label them with shifted text\npv_read_gender &lt;- pv_read_gender +\n  geom_vline(xintercept = mean_read, color = \"#c73824\", linetype = \"dashed\") +\n  geom_vline(xintercept = median_read, color = \"#0477bf\", linetype = \"dashed\") +\n  annotate(\"text\", x = mean_read - 70, y = Inf, vjust = 1, label = paste(\"Mean =\", round(mean_read, 2)), color = \"#c73824\") +\n  annotate(\"text\", x = median_read + 70, y = Inf, vjust = 1, label = paste(\"Median =\", round(median_read, 2)), color = \"#0477bf\")\n\n# pv_read_gender\n\n\nThe code chunk below shows mapped out the % of students coming from each performance consistency level for all subjects and gender.\n\n\nShow the code\n# Define a custom blue color palette\nblue_palette &lt;- c(\"#ccece6\", \"#99d8c9\", \"#66c2a4\", \"#238b45\")\n\n# Reshape the data into long format for consistency categories and calculate percentages\npv_gender_long &lt;- pv_gender %&gt;%\n  pivot_longer(\n    cols = c(consistency_math, consistency_read, consistency_sci),\n    names_to = \"Subject_Consistency\",\n    values_to = \"Consistency\"\n  ) %&gt;%\n  mutate(Subject = case_when(\n    grepl(\"math\", Subject_Consistency) ~ \"Math\",\n    grepl(\"read\", Subject_Consistency) ~ \"Reading\",\n    grepl(\"sci\", Subject_Consistency) ~ \"Science\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  select(-Subject_Consistency) %&gt;%\n  group_by(Subject, Gender, Consistency) %&gt;%\n  summarise(Count = n(), .groups = 'drop') %&gt;%\n  group_by(Subject, Gender) %&gt;%\n  mutate(Total = sum(Count), Percentage = Count / Total) %&gt;%\n  ungroup()\n\n# Creating the stacked bar chart with custom blue color palette\npv_gender_stacked_bar &lt;- ggplot(pv_gender_long, aes(x = Subject, y = Percentage, fill = Consistency)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  geom_text(aes(label = scales::percent(Percentage, accuracy = 0.1)),\n            position = position_fill(vjust = 0.5),\n            color = \"black\", size = 3) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_fill_manual(values = blue_palette) +  # Use custom blue palette\n  facet_wrap(~Gender) +  # Separate by gender\n  labs(title = \"% of Students' Performance Consistency by Subject and Gender\",\n       x = \"Subject\",\n       y = \"Percentage\") +\n  theme_minimal() +\n  theme(legend.title = element_blank(),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(), \n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey\"),  \n    axis.line.x = element_line(color = \"grey\"))\n\n# pv_gender_stacked_bar\n\n\nThe code chunk below zoom into the individual Maths PV values, to understand which is infuncing factor which resulted in the observation in the previous plot between the 2 gender.\n\n\nShow the code\npv_gender_math_long &lt;- pisa_new %&gt;%\n  pivot_longer(cols = matches(\"^PV[0-9]+MATH\"), \n               names_to = \"PVxMath\", values_to = \"Value\") %&gt;%\n  select(Gender = ST004D01T, PVxMath, Value)\n\n# Rename the x-axis labels\nx_axis_labels &lt;- paste0(\"PV\", 1:10)\n\n# Create the plot with custom colors and x-axis labels\nmath_by_gender_pv &lt;- pv_gender_math_long %&gt;%\n  ggplot(aes(x = factor(PVxMath, levels = unique(PVxMath)), y = Value, fill = as.factor(Gender))) +\n  geom_boxplot(alpha = 0.7, outlier.size = 0.5) +\n  labs(title = \" Distribution of Individual Math Performance by Gender\",\n       x = \"\", y = \"Value\", fill = \"Gender\") +\n  scale_x_discrete(labels = x_axis_labels) +  # Rename the x-axis labels\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(), \n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# math_by_gender_pv\n\n\nThe code chunk below patch all visulizations together.\n\n\nShow the code\npatchwork2 &lt;- pv_math_gender / pv_sci_gender / pv_read_gender | (pv_gender_stacked_bar /math_by_gender_pv)\npatchwork2 &lt;- patchwork2 +\n  plot_annotation(title = \"Singapore Students Performance in Mathematics, Reading, and Science by Gender\", \n                  theme = theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5)))\npatchwork2\n\n\n\n\n\nThe visualizations present educational performance among Singaporean students, by subject and gender.\n\nGender 2 performs slightly higher for Maths but slight worse in Reading compared to Gender 1. Both genders show a similar performance for science.\nGender 2 general has a larger spread compared to Gender 1, suggesting a higher variance among the students in terms of performance across the subjects.\nThe stacked bar charts reveal subtle differences in performance consistency across subjects and gender. Gender 1 exhibits a slightly higher consistency in Math, whereas Gender 2 demonstrates greater consistency in Reading.\nThe boxplots then zoom into the individual Math PV scores, segmented by gender. Gender 2 shows a general better performance compared to Gender 1 across all PVs.\nGenerally, Gender 2 has a high upper 25 percentile among all PVs, while the lower 25 percentile does not vary that much between the 2. This might be the cause of the observation discovered in the stacked barchart."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-3-relationship-between-performances-with-school",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-3-relationship-between-performances-with-school",
    "title": "Take-home Exercise 1",
    "section": "EDA 3: Relationship between Performances with School",
    "text": "EDA 3: Relationship between Performances with School\nthe code chunk below create a new dataframe\n\n\nShow the code\npv_school &lt;- pisa_new %&gt;% select(school = CNTSCHID, starts_with(\"PV_avg\"), starts_with(\"consistency\"), Gender = ST004D01T)\npv_school_summary &lt;- pv_school %&gt;%\n  group_by(school) %&gt;%\n  summarise(\n    avg_math = mean(PV_avg_math, na.rm = TRUE),\n    var_math = var(PV_avg_math, na.rm = TRUE),\n    avg_read = mean(PV_avg_read, na.rm = TRUE),\n    var_read = var(PV_avg_read, na.rm = TRUE),\n    avg_sci = mean(PV_avg_scie, na.rm = TRUE),\n    var_sci = var(PV_avg_scie, na.rm = TRUE),\n    no_students = n(),\n    no_1 = sum(Gender == '1', na.rm = TRUE),\n    no_2 = sum(Gender == '2', na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Create bins for each variance column\n  mutate(across(starts_with(\"var_\"), ~cut(., \n                                          breaks = quantile(., probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE),\n                                          labels = c(\"low\", \"lower-avg\", \"upper-avg\", \"high\"),\n                                          include.lowest = TRUE),\n                .names = \"binned_{.col}\"))\n\n\nThe code chunk below displayed the distribution of schoolâ€™s average performance by subject.\n\n\nShow the code\n# Reshape the data for ggplot\npv_school_long &lt;- pv_school_summary %&gt;%\n  pivot_longer(\n    cols = starts_with(\"avg_\"),\n    names_to = \"subject\",\n    values_to = \"average_score\"\n  ) %&gt;%\n  mutate(subject = sub(\"avg_\", \"\", subject)) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"binned_var_\"),\n    names_to = \"var_metric\",\n    values_to = \"variance_bin\"\n  ) %&gt;%\n  mutate(subject_var = sub(\"binned_var_\", \"\", var_metric)) %&gt;%\n  filter(subject == subject_var) # Ensure that the subject matches with its respective variance\n\n# Create the plot\nschool_plot&lt;- ggplot(pv_school_long, aes(y = subject, x = average_score)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               height = 0.5) +\n  geom_boxplot(width = .15,\n               outlier.shape = NA) +\n  stat_dots(aes(fill = variance_bin, color = variance_bin, alpha = 0.5),  \n            side = \"left\", \n            justification = 1.2, \n            binwidth = 2,\n            dotsize = 1.5) +\n  labs(title = 'Students Performance Acorss School', \n         x = \"Average PV by School\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_text(margin = margin(t = -5, b = 10))\n  ) \n\n#school_plot\n\n\nThe code chunk below checks for if the school preforms consistently acorss various subject using a correlation heatmap\n\n\nShow the code\n# Select variables of interest and rename them\nselected_avg &lt;- pv_school_summary %&gt;%\n  select(avg_math, avg_read, avg_sci) %&gt;%\n  rename(\n    \"Math Avg\" = avg_math,\n    \"Read Avg\" = avg_read,\n    \"Science Avg\" = avg_sci,\n  )\n\n# Create the correlogram\ncorrelogram_avg &lt;- ggcorrmat(\n  data = selected_avg,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title = \"Average School Performance\",\n  subtitle = \"School Performance is relatively related in a linear manner\"\n)\n\n# Display the correlogram\n#correlogram_avg\n\n# Select variables of interest and rename them\nselected_vars &lt;- pv_school_summary %&gt;%\n  select(var_math, var_read, var_sci) %&gt;%\n  rename(\n    \"Math Variance\" = var_math,\n    \"Read Variance\" = var_read,\n    \"Science Variance\" = var_sci\n  )\n\n# Create the correlogram\ncorrelogram_var &lt;- ggcorrmat(\n  data = selected_vars,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title = \"School Performance Consistency Acorss Students\",\n  subtitle = \"Consistency in Performance is relatively related in a linear manner\"\n)\n\n# Display the correlogram\n#correlogram_var\n\n\nThe code chunk below patch the plots in one plot.\n\n\nShow the code\npatchwork_sch &lt;- (school_plot/(correlogram_avg + correlogram_var)) + \n  plot_layout(heights = c(4, 1)) # This makes the left plot three times wider than the right ones\n\npatchwork_sch &lt;- patchwork_sch + \n  plot_annotation(title = \"Singapore Students Performance in Mathematics, Reading, and Science by School\") & \n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5))\n\npatchwork_sch\n\n\n\n\n\nThe visualization presents a comparative distribution of average student performance scores in Mathematics, Reading, and Science across various schools.\n\nThe density plots for all 3 subjects shows two distinct peaks, this indicates a gap in terms of studentâ€™s performance between the 2 sectors of schools, which is likely to be private, and public.\nAdditionally, most number of schools with low variance among students performance which shaded in red is spotted along this upper part of the density curve. even stronger indication of general good performance across all students in the school.\nThis chart reveals potential relationship between school types and students pref romance in Maths, Science and Readings.\nThe 2 correlograms also highlights the correlationship in terms of subjetive performance for each school, suggesting that if the school does well in 1 subject, they are likely to do well in others. If the performance is consistent among students in 1 subject, the consistency is likely to be observed in other subjects as well."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-4-relationship-between-performances-with-socioeconomic-status",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-4-relationship-between-performances-with-socioeconomic-status",
    "title": "Take-home Exercise 1",
    "section": "EDA 4: Relationship between Performances with Socioeconomic Status",
    "text": "EDA 4: Relationship between Performances with Socioeconomic Status\nthe code chunk below create a new dataframe\n\n\nShow the code\npv_ESCS &lt;- pisa_new %&gt;% select(starts_with(\"PV_avg\"), starts_with(\"consistency\"), Gender = ST004D01T, ESCS)\n\n\nThe code chunk below creates a scatterstats plot for Maths and ESCS Relationship.\n\n\nShow the code\nescs_math &lt;- pv_ESCS %&gt;%\n  ggscatterstats(\n    x = ESCS, \n    y = PV_avg_math, \n    conf.level = 0.95,\n    bf.message = FALSE,\n    alpha = 0.5, \n    point.args = list(size = 2, alpha = 0.1, stroke = 0, color = 'grey20'),\n    xsidehistogram.args = list(fill = \"#c73824\", color = \"white\", na.rm = TRUE, alpha = 0.7),\n    ysidehistogram.args = list(fill = \"#0477bf\", color = \"white\", na.rm = TRUE, alpha = 0.7),\n    smooth.line.args = list(linewidth = 0.4, color = \"blue\", method = \"lm\", formula = y ~\n    x)\n  ) +\n  labs(\n    title = \"Math Performance\",\n    x = \"ESCS INDEX\",\n    y = \"Average Math Score\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n# escs_math\n\n\nThe code chunk below creates a scatterstats plot for Science and ESCS Relationship.\n\n\nShow the code\nescs_sci &lt;- pv_ESCS %&gt;%\n  ggscatterstats(\n    x = ESCS, \n    y = PV_avg_scie, \n    conf.level = 0.95,\n    bf.message = FALSE,\n    alpha = 0.5, \n    point.args = list(size = 2, alpha = 0.1, stroke = 0, color = 'grey20'),\n    xsidehistogram.args = list(fill = \"#c73824\", color = \"white\", na.rm = TRUE, alpha = 0.7),\n    ysidehistogram.args = list(fill = \"#0477bf\", color = \"white\", na.rm = TRUE, alpha = 0.7),\n    smooth.line.args = list(linewidth = 0.4, color = \"blue\", method = \"lm\", formula = y ~\n    x)\n  ) +\n  labs(\n    title = \"Science Performance\",\n    x = \"ESCS INDEX\",\n    y = \"Average Science Score\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n# escs_sci\n\n\nThe code chunk below creates a scatterstats plot for Reading and ESCS Relationship.\n\n\nShow the code\nescs_read &lt;- pv_ESCS %&gt;%\n  ggscatterstats(\n    x = ESCS, \n    y = PV_avg_read, \n    conf.level = 0.95,\n    bf.message = FALSE,\n    alpha = 0.5, \n    point.args = list(size = 2, alpha = 0.1, stroke = 0, color = 'grey20'),\n    xsidehistogram.args = list(fill = \"#c73824\", color = \"white\", na.rm = TRUE, alpha = 0.7),\n    ysidehistogram.args = list(fill = \"#0477bf\", color = \"white\", na.rm = TRUE, alpha = 0.7),\n    smooth.line.args = list(linewidth = 0.4, color = \"blue\", method = \"lm\", formula = y ~\n    x)\n  ) +\n  labs(\n    title = \"Reading Performance\",\n    x = \"ESCS INDEX\",\n    y = \"Average Reading Score\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.grid.minor = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n# escs_read\n\n\nThe code chunk below plots a correlation heatmap between aggregated PV values with ESCS Index\n\n\nShow the code\n# Select variables of interest and rename them\nselected_vars &lt;- pisa_new %&gt;%\n  select(starts_with('PV_'),\n         ESCS)\n\n# Create the correlogram\ncorrelogram2 &lt;- ggcorrmat(\n  data = selected_vars,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title = \"Correlogram for Student Performance\",\n  subtitle = \"Subject Performance is relatively related in a linear manner\"\n)\n# orrelogram2\n\n\nThe code chunk below patch all visulizations together.\n\n\nShow the code\npatchwork3 &lt;- (correlogram2 | escs_math) / (escs_sci | escs_read)\npatchwork3 &lt;- patchwork3 +\n  plot_annotation(title = \"Singapore Students Performance in Mathematics, Reading, and Science by ESCS\", \n                  theme = theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5)))\npatchwork3\n\n\n\n\n\nThe chart illustrates a moderate, positive correlation between the Economic, Social, and Cultural Status (ESCS) Index and student performance scores in Mathematics, Reading, and Science for a sample of Singaporean students.\n\nThe p-value reported are all below 0.05, this indicates that the null hypothesis can be rejected with a high degree of confidence. There is a statistically significant correlation between Performance and the ESCS Index for all subjects.\nPearson correlation coefficients is around 0.44 to 0.45 across all subjects, the data suggests that higher socio-economic status is associated with better academic performance. However, as the values is small, this relationship is only a moderate relationship.\nAdditionally, there is also no significant difference between the influence by ESCS to different subjects."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-5-how-about-socioeconomic-status-and-school",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#eda-5-how-about-socioeconomic-status-and-school",
    "title": "Take-home Exercise 1",
    "section": "EDA 5: How about Socioeconomic Status and School ?",
    "text": "EDA 5: How about Socioeconomic Status and School ?\nEDA 3 reveals that a small groups of schools are preforming relatively better than other schools, EDA 4 proves a moderate linear relationship between socioeconomic status and studentâ€™s performance. Might this be a reason that certain school preforms extremely well among all students to the compared to other schools? Which subjects will be influenced the most?\nthe code chunk below assign a group number for different schools in terms of their performance. 5 being the best preformaning school, while 1 is the least.\n\n\nShow the code\npv_school_group &lt;- pv_school_summary %&gt;%\n  mutate(across(starts_with(\"avg_\"), ~cut(., \n                                          breaks = quantile(., probs = c(0, 0.2, 0.4, 0.6, 0.7, 1), na.rm = TRUE),\n                                          labels = c(1, 2, 3, 4, 5),\n                                          include.lowest = TRUE),\n                .names = \"binned_{.col}\"))\n# it is noticed that most school has a consistency in terms of performance, one single field will be assigned based on the most appeared bin for easier comparison. \n\npv_school_group &lt;- pv_school_group %&gt;%\n  rowwise() %&gt;%\n  mutate(sch_group = factor(round(mean(as.numeric(as.character(c_across(starts_with(\"binned_avg\")))), na.rm = TRUE)))) %&gt;%\n  ungroup()\n\n\nthe code chunk below creates a new dataframe and join the school group with the individual studentâ€™s ESCS index.\n\n\nShow the code\nschool_escs_new &lt;- pisa_new %&gt;% select(school = CNTSCHID, ESCS)\n\nschool_escs_new  &lt;- school_escs_new  %&gt;%\n  left_join(pv_school_group %&gt;% select(school, sch_group), by = \"school\") %&gt;%\n  mutate(sch_group = factor(sch_group, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\"))\n         )\n\n\nThe code chunk below conducted one-way ANOVA test between ESCS index Across Different School Group.\n\n\nShow the code\nanova &lt;- ggbetweenstats(\n  data = school_escs_new,\n  x = sch_group, \n  y = ESCS,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n) +\n  labs(title = \"ESCS index Across Different School Group\", x = \"School Group\") +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5)\n  )\n\n# anova\n\n\nBut will a students who are from lower ESCS tier preform bettwe in a better school?\nthe code chunk below creates a new dataframe and join the school group with the individual student performance.\n\n\nShow the code\nschool_escs &lt;- pisa_new %&gt;% select(school = CNTSCHID, starts_with(\"PV_avg\"), ESCS_bin)\n\nschool_escs &lt;- school_escs %&gt;%\n  left_join(pv_school_group %&gt;% select(school, sch_group), by = \"school\")\n\nschool_escs_long &lt;- school_escs %&gt;%\n  pivot_longer(\n    cols = c(PV_avg_math, PV_avg_scie, PV_avg_read), \n    names_to = \"subject\",      \n    values_to = \"value\"       \n  )\n\nschool_escs_long &lt;- school_escs_long %&gt;%\n  mutate(sch_group = factor(sch_group, levels = c(\"1\", \"2\", \"3\", \"4\", \"5\")),\n         subject = factor(subject, \n                          levels = c(\"PV_avg_math\", \"PV_avg_read\", \"PV_avg_scie\"),\n                          labels = c(\"Math\", \"Reading\", \"Science\")))\n\n\nThe code chunk below draws histogram to see the distribution of PVs across different school type and ESCS tiers.\n\n\nShow the code\nbase_color &lt;- \"#c73824\"\nred_shades &lt;- c(\n  lighten(base_color, amount = 0.9),  # Lightest\n  lighten(base_color, amount = 0.7),\n  lighten(base_color, amount = 0.5),\n  lighten(base_color, amount = 0.3),\n  lighten(base_color, amount = 0.1)\n)\n\nsch_escs_plot &lt;- ggplot(school_escs_long, aes(x = ESCS_bin, y = value, fill = ESCS_bin)) + \n  geom_boxplot() + \n  facet_grid(sch_group ~ subject) +\n  scale_fill_manual(values = red_shades) +\n  labs(title = \"Distribution of Students Performance by School Group and ESCS tier\") +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.background = element_rect(fill = \"white\", colour = \"white\"),\n    panel.grid.major = element_line(color = \"grey90\"),\n    panel.grid.minor = element_line(color = \"grey90\"),\n    strip.background = element_rect(fill = \"black\"),\n    strip.text = element_text(color = \"white\"), \n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=0.5)  # Add border around each facet\n  )\n# sch_escs_plot\n\n\n\n\nShow the code\npatchwork_5 &lt;- (anova|sch_escs_plot) \n\npatchwork_5 &lt;- patchwork_5 + \n  plot_annotation(title = \"Students Performance by School Group and ESCS\") & \n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5))\n\npatchwork_5\n\n\n\n\n\nThe ANOVA test on the left shows a distribution of the Economic, Social, and Cultural Status (ESCS) index across different school groups.\n\nSchool Groups with higher average preform show a higher ESCS index.\nF-statistic is approximately 294.76 with a p-value of approximately 5.89e-210. This suggests that there is a statistically significant difference in the ESCS index among the different school groups.\nAbove the violin plots, there are lines connecting the different groups with associated p-values indicating the results of each pairwise comparison. This suggests that all groups shown are having small p values, suggesting that the ESCS index means of each pair of groups are significantly different from each other.\nThe violin plot overlaid with box plots for school groups 5 shows a much wider sections of the violin indicating at the higher ESCS, suggesting a higher density of students are from this portion\n\nThe box plots arranged in a grid, showing student performance in three different subjects: Math, Reading, and Science, among 4 different ESCS tiers. This plot aims to understand within the same group of school, would the difference in ESCS make a difference within the same school group in student performance.\n\nComparing Students performance within each school group, the observation of higher ESCS tier reflects a higher performance seems to hold truth as well.\nA notably higher number of outliers are observed for School Group 5 across all ESCS tiers, particularly in Math and Science. This pattern may indicate a wider range of performance levels or varying factors affecting achievement within this group.\nIn contrast, the subject of Reading displays outliers across all school groups and tiers, hinting at a broader variance in performance among Singaporean students in Reading compared to Math and Science."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#appendix-how-about-socioeconomic-status-and-gender",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#appendix-how-about-socioeconomic-status-and-gender",
    "title": "Take-home Exercise 1",
    "section": "Appendix: How about Socioeconomic Status and Gender ?",
    "text": "Appendix: How about Socioeconomic Status and Gender ?\nThe code chunk below creates a new datafrmae for gender and escs information.\n\n\nShow the code\n# the code chunk below creates a new dataframe and join the school group with the individual student.\ngender_escs &lt;- pisa_new %&gt;% select(starts_with(\"PV_avg\"), ESCS_bin, Gender = ST004D01T)\n\ngender_escs_long &lt;- gender_escs %&gt;%\n  pivot_longer(\n    cols = c(PV_avg_math, PV_avg_scie, PV_avg_read), \n    names_to = \"subject\",      \n    values_to = \"value\"       \n  )\n\ngender_escs_long &lt;- gender_escs_long %&gt;%\n  mutate(subject = factor(subject, \n                          levels = c(\"PV_avg_math\", \"PV_avg_read\", \"PV_avg_scie\"),\n                          labels = c(\"Math\", \"Reading\", \"Science\")))\n\n\nThe code chunk below draws a histogram to see the distribution of PVs across different ESCS tier and Gender.\n\n\nShow the code\nbase_color &lt;- \"#c73824\"\nred_shades &lt;- c(\n  lighten(base_color, amount = 0.9),  # Lightest\n  lighten(base_color, amount = 0.7),\n  lighten(base_color, amount = 0.5),\n  lighten(base_color, amount = 0.3),\n  lighten(base_color, amount = 0.1)\n)\n\nggplot(gender_escs_long, aes(x = ESCS_bin, y = value, fill = ESCS_bin)) + \n  geom_boxplot() + \n  facet_grid(Gender ~ subject) +\n  scale_fill_manual(values = red_shades) +\n  labs(title = \"Distribution of Students Performance by Gender and ESCS tier\", x = \"ESCS Tier\") +\n  theme_minimal() +\n  theme(\n    legend.title = element_blank(),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    panel.background = element_rect(fill = \"white\", colour = \"white\"),\n    panel.grid.major = element_line(color = \"grey90\"),\n    panel.grid.minor = element_line(color = \"grey90\"),\n    strip.background = element_rect(fill = \"black\"),\n    strip.text = element_text(color = \"white\"), \n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill=NA, size=0.5)  # Add border around each facet\n  )\n\n\n\n\n\nâ€“ end of page â€“"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "theory: a summary of R for Visual Analytics - Chap 1\n\nstarting with R\nbasics about ggplot2\ndive into each layered grammars of ggplot2\n\npractice: some exploration about the dataset\n\nQucik access to Some Plotting Exercise below"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on Exercise 1",
    "section": "Install and launching R packages",
    "text": "Install and launching R packages\nThe code below use p_load() of pacman packages to check if tidyverse packages are installed in the computer. If there are, then they will launch into R\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on Exercise 1",
    "section": "Importing the data",
    "text": "Importing the data\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nGenerating a summary of the imported data\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\n\n\n\n\n\nAbout summary(data_frame)\n\n\n\nIt will display length, class, and mode for categorical variables, and display Min, 1st Qu, Median, Mean, 3rd Qu, Max for numeric variables"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#comparing-r-graphics-vs-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#comparing-r-graphics-vs-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "Comparing R Graphics VS ggplot",
    "text": "Comparing R Graphics VS ggplot\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThere are two principles in Grammar of Graphics:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mapping"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "A Layered Grammar of Graphics",
    "text": "A Layered Grammar of Graphics\nggplot2 is an implementation of Leland Wilkinsonâ€™s Grammar of Graphics. Figure below shows the seven grammars of ggplot2.\n\n\n\nReference: Hadley Wickham (2010) â€œA layered grammar of graphics.â€ Journal of Computational and Graphical Statistics, vol.Â 19, no. 1, pp.Â 3â€“28.\n\n\nA short description of each building block are as follows:\n\nData: The dataset being plotted.\nAesthetics take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics, statistical transformations that summarise data (e.g.Â mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-axis title, or legend background."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#ggplot2-summary",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#ggplot2-summary",
    "title": "Hands-on Exercise 1",
    "section": "An Overview of Layered Grammar",
    "text": "An Overview of Layered Grammar\nbelow shows how each layer built on top of each other based on the 7 grammars of ggplot2\n\ndataaesgeostatfacetscoordinatethemes\n\n\n\nggplot(data=exam_data)\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS))\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) \n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\") +\n  facet_wrap(~ CLASS)                                                          \n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\") +\n  facet_wrap(~ CLASS) +\n  coord_flip()\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\") +\n  facet_wrap(~ CLASS) +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-1",
    "title": "Hands-on Exercise 1",
    "section": "Data",
    "text": "Data\n\nggplot() initializes a ggplot object.\nThe data argument defines the dataset to be used for plotting.\n\n\nggplot(data=exam_data)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#aesthetic-mappings",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#aesthetic-mappings",
    "title": "Hands-on Exercise 1",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\nAll aesthetics of a plot are specified in the aes() function call\neach geom layer can have its own aes specification\n\nadding the aesthetic element to include the x-axis and the axisâ€™s label\n\nggplot(data=exam_data, \n       aes(x= MATHS))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#ggplot2-geometric-objects",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#ggplot2-geometric-objects",
    "title": "Hands-on Exercise 1",
    "section": "ggplot2 Geometric Objects",
    "text": "ggplot2 Geometric Objects\nGeometric objects are the actual marks we put on a plot. Examples include:\n\ngeom_point for drawing individual points (e.g., a scatter plot)\ngeom_line for drawing lines (e.g., for a line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., for bar charts)\ngeom_histogram for drawing binned values (e.g.Â a histogram)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function).\n\nA plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator.\n\n\n\nFor complete list, please refer to here\n\n\n\nGeometric Objects: geom_bar\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\nGeometric Objects: geom_dotplot\n\nwidth of a dot: bin width (or maximum width, depending on the binning algorithm),\ndots are stacked, each dot representing one observation\n\n\nDo thisNot this\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL) \n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe y scale is not very useful, in fact it is very misleading.\nInstead should perform the following two steps:\n\nscale_y_continuous() is used to turn off the y-axis, and\nbinwidth argument is used to change the binwidth to 2.5.\n\n\n\n\n\nGeometric Objects: geom_histogram()\nplotting a histogram using geom_histogram() using MATHS field of exam_data\n\nBasic HistogramModifying geom()Modifying aes()\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nbins: change the number of bins to 20 (default = 30)\nfill: shade the histogram with light blue color\ncolor: change the outline colour to black\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")       \n\n\n\n\n\n\n\ndifferent gender are shaded in different colors\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")     \n\n\n\n\n\n\n\n\n\nGeometric Objects: geom-density()\n\ncomputes and plots kernel density estimate - smoothed version of histogram\nalternative to histogram for continuous data that comes from an underlying smooth distribution\n\n\nBasicBy Gender\n\n\ndistribution of Maths scores in a kernel density estimate plot\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()                 \n\n\n\n\n\n\ntwo kernel density lines by using colour or fill arguments of aes()\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()                 \n\n\n\n\n\n\n\n\n\nGeometric Objects: geom_boxplot\n\ngeom_boxplot() displays continuous value list.\nvisualises 5 summary statistics (median, 2 hinges and 2 whiskers), and all â€œoutlyingâ€ points individually.\n\n\nBasic BoxplotNotched plot\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()                            \n\n\n\n\n\n\n\nNotches are used to help visually assess whether the medians of distributions differ\nIf the notches do not overlap, the medians are different\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\nGeometric Objects: geom_violin\n\ngeom_violin creates violin plot which are comparing multiple data distributions\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()                         \n\n\n\n\n\n\nGeometric Objects: geom_point()\n\ngeom_point() is especially useful for creating scatterplot.\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()                                       \n\n\n\n\n\n\nA Combined Plot\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stat-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stat-1",
    "title": "Hands-on Exercise 1",
    "section": "Stat",
    "text": "Stat\nThe Statistics functions statistically transform data, usually as some form of summary\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\nWorking with Boxplot\nThe default boxplots are incomplete because the positions of the means were not shown.\n\nstat_()geom()\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=4)                                                      \n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nâ„¹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             size=4)                                                               \n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\n\nWorking with scatterplot\nThe scatterplot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve.\n\nmethod: loess (default)method: lm\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)                                                             \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)                                                            \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facets-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facets-1",
    "title": "Hands-on Exercise 1",
    "section": "Facets",
    "text": "Facets\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_grid() and facet_wrap.\n\nfacet_wrap()facet_grid()\n\n\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)                                                          \n\n\n\n\n\n\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#coordinates",
    "title": "Hands-on Exercise 1",
    "section": "Coordinates",
    "text": "Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n- coord_cartesian(): default, where you specify x and y values (e.g.Â allow to zoom in or out). - coord_flip(): a cartesian system with the x and y flipped - coord_fixed(): a cartesian system with a â€œfixedâ€ aspect ratio - coord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\nWorking with Coordinates\n\nverticalhorizontal\n\n\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()                                                           \n\n\n\n\n\n\nfliping the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()                                                          \n\n\n\n\n\n\n\n\n\nChanging the y- and x-axis range\nThe scatterplot on the right is slightly misleading because the y-aixs and x-axis range are not equal.\n\ndefaultrange at 0-100\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)                                                          \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))                                                  \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#theme",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#theme",
    "title": "Hands-on Exercise 1",
    "section": "Theme",
    "text": "Theme\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\n\nA list of theme can be found at this link\n\ntheme_gray()theme_classic()theme_minimal()\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()                                                       \n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()                                                       \n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stacked-bar-chart-of-race-distribution-by-gender",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stacked-bar-chart-of-race-distribution-by-gender",
    "title": "Hands-on Exercise 1",
    "section": "Stacked Bar Chart of Race Distribution by Gender",
    "text": "Stacked Bar Chart of Race Distribution by Gender\n\n\nShow the code\nggplot(data = exam_data, \n       aes(x = reorder(RACE, -table(RACE)[RACE]), fill = GENDER)) +\n  geom_bar(position = \"stack\",\n           alpha = 0.9) +\n  geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    position = position_stack(vjust = 0.5),\n    size = 3,\n    color = \"white\"\n  ) +\n  labs(title = \"Race Distribution by Gender\", x = \"Race\", y = \"Number of Students\") +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#boxplot-of-english-scores-by-class",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#boxplot-of-english-scores-by-class",
    "title": "Hands-on Exercise 1",
    "section": "Boxplot of English Scores by Class",
    "text": "Boxplot of English Scores by Class\n\n\nShow the code\nggplot(data = exam_data, \n       aes(x = CLASS, y = ENGLISH)) +\n  geom_boxplot(fill = \"#D1EEEE\", color = \"#7A8B8B\") +\n  geom_hline(yintercept = mean(exam_data$ENGLISH), linetype = \"dashed\", color = \"#CD2626\") +\n  stat_summary(\n    fun = mean, \n    geom = \"point\", \n    color = \"#CD2626\"\n  ) +\n  annotate(\n    \"text\", \n    x = 1,  y = mean(exam_data$ENGLISH) + 2,\n    label = paste(\"Avg:\", round(mean(exam_data$ENGLISH), 2)),\n    color = \"#CD2626\"\n  ) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(\n    title = \"English Scores by Class\",\n    x = \"Class\",\n    y = \"English Score\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#scatterplot-of-math-and-science-scores",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#scatterplot-of-math-and-science-scores",
    "title": "Hands-on Exercise 1",
    "section": "Scatterplot of Math and Science Scores",
    "text": "Scatterplot of Math and Science Scores\n\n\nShow the code\nggplot(data = exam_data,\n       aes(x = MATHS, y = SCIENCE)) +\n  geom_point(aes(color = GENDER), size = 1.5, alpha = 0.7) +\n  geom_hline(yintercept = 50, linetype = \"dashed\", color = \"gray\") +  \n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"gray\") +  \n  geom_smooth(method = \"lm\", size = 0.5) +      \n  labs(\n    title = \"Correlation between Math and Science Scores\",\n    x = \"Math Score\",\n    y = \"Science Score\"\n  ) +\n  coord_cartesian(xlim = c(0, 100), ylim = c(0, 100)) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#density-plot-of-english-scores-by-class",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#density-plot-of-english-scores-by-class",
    "title": "Hands-on Exercise 1",
    "section": "Density Plot of English Scores by Class",
    "text": "Density Plot of English Scores by Class\n\n\nShow the code\n# Density plot of ENGLISH scores combined for both genders faceted by class\nggplot(data = exam_data, \n       aes(x = ENGLISH, fill = GENDER)) +\n  geom_density(alpha = 0.5, color = \"black\", linewidth = 0.3) + \n  labs(title = \"Distribution of English Scores by Class\", x = \"English Score\") +\n  theme_minimal() +\n  facet_grid(CLASS ~ .) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.text = element_text(size = 8),  \n        legend.title = element_text(size = 8))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "a summary of Correlation Analysis - Qucik access to Correlation\na summary of Treemap Visualisation with R- Qucik access to Treemap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#correlation-analysis",
    "title": "Hands-on Exercise 5a",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\n\n\n\npackage\nR\nggstatsplot\ncorrplot\n\n\nfunc\npairs()\nggcormat()\ngrouped_ggcorrmat()\ncorrplot()\n\n\ncommon arguement\ndata[rows,col]\nna.action\nlower.panel\nupper.panel\ncex.labels\nbg\npanel\npch\ncor.vars\nhc.order\nmatrix.type\ntype\nggcorrplot.args\ngrouping.var\nplotgrid.args\nannotation.args\nmethod\ntype\norder\ncorrplot.mixed()\n\n\nlimitation & advantages\nscatter plots appear very cluttered when the number of observations is relatively large\ncomprehensive and professional statistical report\nvisualization & determine the statistical significance of the correlations"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#tree-map",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#tree-map",
    "title": "Hands-on Exercise 5a",
    "section": "Tree Map",
    "text": "Tree Map\n\n\n\n\n\n\n\n\n\npackage\ntreemap\ntreemapify\nd3treeR\n\n\nfunc\ntreemap()\ngeom_treemap()\nd3tree()\n\n\ncommon arguement\nindex\nvSize\nvColor\npalette\ntype\npalette\nsortID\nscale_fill_gradient\naes(area, fill, subgroup)\ngeom_treemap_subgroup_border\ntreemap()\nd3tree()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 5a",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\ncorrplot, ggstatsplot, plotly and tidyverse are used\n\n\nCode\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 5a",
    "section": "Importing and Preparing The Data Set",
    "text": "Importing and Preparing The Data Set\nWine Quality Data Set of UCI Machine Learning Repository consists of 13 variables and 6497 observations. (red wine and white wine data have been combined into one data file called wine_quality and is in csv file format)\nimport the data into R by using read_csv() of readr package.\n\n\nCode\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\nBeside type, the rest of the variables are numerical and continuous data type"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 5a",
    "section": "Building Correlation Matrix: pairs() method",
    "text": "Building Correlation Matrix: pairs() method\npairs() of R can create a scatterplot matrix. Some arguments can be changed are:\n\ndata[rows,col], empty: select all, [start, end]\nna.action = na.omit (By default, missing values are passed and will often be ignored within a panel)\nhorInd and verIndselect or re-order variables: with different ranges of consecutive values they can be used to plot rectangular windows of a full pairs plot; in the latter case â€˜diagonalâ€™ refers to the diagonal of the full plot.\nlower.panel = NULL\nupper.panel = NULL\ncex.labels : text size of diagonal label\nbg = â€œlight blueâ€ : color of the scatter points\npanel = panel.smooth: plot the x, y func\npch = 24: triangular plot with size 24\n\n\nBasicChange inputLowerUpperCorrelation CoefficientsOther Arguments\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 12 by 12 matrix. The variables names are in the diagonals\n\n\nCode\npairs(wine[,1:12])\n\n\n\n\n\n\n\ninput of pairs() can be a matrix or data frame. Row 1 to 30 and Columns 1 to 11 of wine dataframe is used to build the scatterplot matrix.\n\n\nCode\npairs(wine[1:30,1:11])\n\n\n\n\n\n\n\nIt is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nupper.panel argument will be used as shown to show only the lower half.\n\n\nCode\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\nSimilarly, upper half can be kept only\n\n\nCode\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\n\nCode\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\nlower.panel = NULL to off the lower panel, panel = panel.smooth to draw the xy fun, pch = 24 to change scatterpoint to triangle with size 24, bg = â€œlight blueâ€ to set the triangle to light blue, cex.labels = 0.5 to change the text size of diagonal\n\n\nCode\npairs(wine[,2:12], \n      upper.panel = panel.cor,\n      panel = panel.smooth,\n      pch = 24, \n      bg = \"light blue\",\n      cex.labels = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5a",
    "section": "Visualising Correlation Matrix: ggcormat()",
    "text": "Visualising Correlation Matrix: ggcormat()\nlimitation of the correlation matrix:\n\nscatter plots appear very cluttered when the number of observations is relatively large (i.e.Â more than 500 observations).\n\nTo over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nSome R packages provide function to plot corrgram:\n\ncorrgram\nellipse\ncorrplot\nggstatsplot\n\nThis section will cover ggcorrmat() of ggstatsplot package, some arguments can be changed are:\n\ncor.vars : compute the correlation matrix\nhc.order = TRUE: reorder\nmatrix.type : \"upper\" (default), \"lower\", or \"full\"\ntype: A character specifying the type of statistical approach, â€œparametricâ€, â€œnonparametricâ€, \"robust\" , \"bayes\". Can specify just the initial letter.\n\ngrouped_ggcorrmat() of ggstatsplot can be for faceting, arguments are:\n\ngrouping.var\nplotgrid.args : provides a list of additional arguments passed to patchwork::wrap_plots\nannotation.args: calling plot annotation arguments of patchwork package\n\n\nBasicadjust ggcorrplot.argsmultiple plotsother arguments\n\n\nusing ggcorrmat() to visualise a correlation matrix and professional statistical report\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram\n\n\n\nCode\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\nggcorrplot.args : argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nhc.order: If TRUE, correlation matrix will be hc.ordered using hclust function.\ntl.cex: size of text label (variable names)\nlab_size : size to be used for the correlation coefficient labels\n\n\n\n\nCode\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10,\n                         lab_size = 3),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\nfaceting is only available in grouped_ggcorrmat() of ggstatsplot.\n\nonly argument needed: grouping.var\nplotgrid.args : provides a list of additional arguments passed to patchwork::wrap_plots\nannotation.args: calling plot annotation arguments of patchwork package.\n\n\n\nCode\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 5,\n                         lab_size = 1),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n) \n\n\n\n\n\n\n\n\nmatrix.type : â€œlowerâ€ to keep only the lower plot\ntype: A character specifying the type of statistical approach, â€œparametricâ€, â€œnonparametricâ€, \"robust\" , \"bayes\". Can specify just the initial letter.\n\n\n\nCode\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  matrix.type = 'lower',\n  type = \"bayes\",\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10,\n                         lab_size = 3),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 5a",
    "section": "Visualising Correlation Matrix using corrplot Package",
    "text": "Visualising Correlation Matrix using corrplot Package\ncorrplot provides a visual exploratory tool on correlation matrix. It provides a rich array of plotting options in visualization method, graphic layout, color, legend, text labels, etc. It also provides p-values and confidence intervals to help users determine the statistical significance of the correlations.\nÂ has about 50 parameters, however the mostly common ones are only a few. We can get a correlation matrix plot with only one line of code in most scenes.\nCommon parameters for corrplot() :\n\nmethod: 'circle','square','ellipse', 'number','shade','color','pie'\ntype: layout types including 'full','upper' and 'lower'\norder:\n\n'alphabet': alphabetical order\n'AOE': angular order of the eigenvectors. See Michael Friendly (2002)for details.\n'FPC': for the first principal component order\n'hclust': for hierarchical clustering order\n\ncorrplot.mixed(): wrapped function for mixed visualization style, which can set the visual methods of lower and upper triangular separately\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\n\nCode\nwine.cor &lt;- cor(wine[, 1:11])\n\n\n\nDefaultMethodTypediag & tl.colMixedSig. TestReorder AOEReorderExploration\n\n\ncorrplot() is used to plot the corrgram by using all the default setting:\n\ncircle\nsymmetric matrix\ndiverging blue-red:\n\nBlue: positive correlation coefficients\nred colours: negative correlation coefficients\nIntensity: Darker -&gt; relatively stronger linear relationship, lighter colours -&gt; weaker\n\n\n\n\nCode\ncorrplot(wine.cor)\n\n\n\n\n\n\n\nIn corrplot package, there are 7 visual geometrics can be changed using method: circle, square, ellipse, number, shade, color and pie. The default is circle.\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\ncorrplor() supports three layout types, namely: â€œfullâ€, â€œupperâ€ or â€œlowerâ€. The default is â€œfullâ€ which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\ndiag: turn off the diagonal cells\ntl.col: change the axis text label colour to black colour\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\ndesign corrgram with mixed visual matrix of one half and numerical matrix on the other half using corrplot.mixed()\n\nlower half: ellipse\nupper half: numerical matrix (i.e.Â number) using tl.pos\ndiag: specify the glyph on the principal diagonal of the corrgram\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables. We can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\n\nCode\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hidden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e.Â â€œoriginalâ€). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\nâ€œAOEâ€ is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\nâ€œFPCâ€ for the first principal component order\nâ€œhclustâ€ for hierarchical clustering order, and â€œhclust.methodâ€ for the agglomeration method to be used.\n\nâ€œhclust.methodâ€ should be one of â€œwardâ€, â€œsingleâ€, â€œcompleteâ€, â€œaverageâ€, â€œmcquittyâ€, â€œmedianâ€ or â€œcentroidâ€.\n\nâ€œalphabetâ€ for alphabetical order\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"pie\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               order=\"AOE\",\n               tl.col = \"black\",\n               tl.srt = 45\n               )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#reference",
    "title": "Hands-on Exercise 5a",
    "section": "Reference",
    "text": "Reference\nMichael Friendly (2002). â€œCorrgrams: Exploratory displays for correlation matricesâ€. The American Statistician, 56, 316â€“324.\nD.J. Murdoch, E.D. Chow (1996). â€œA graphical display of large correlation matricesâ€. The American Statistician, 50, 178â€“180."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#overview",
    "title": "Hands-on Exercise 5a",
    "section": "Overview",
    "text": "Overview\nThe hands-on exercise consists of 3 main section:\n\nmanipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package\nplot static treemap by using treemap package\ndesign interactive treemap by using d3treeR package"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-launching-r-packages-1",
    "title": "Hands-on Exercise 5a",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\n\n\nCode\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#data-wrangling",
    "title": "Hands-on Exercise 5a",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\n\nImporting the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\n\nCode\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\n\n\nData Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No.of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object theyâ€™ll be automatically applied â€œby groupâ€.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(â€œwindow-functionsâ€).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\nGrouped summaries without the PipeGrouped summaries with the pipe\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\n\nCode\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\nNote\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if thereâ€™s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we donâ€™t have to care about it.\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%\n\n\nCode\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 5a",
    "section": "Designing Treemap with treemap Package",
    "text": "Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\ntreemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\n\nCode\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\nDefaultvColor & type\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because itâ€™s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\nIn the code chunk below, type argument is define as value.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e.Â 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\n\nColours\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between â€œvalueâ€ and â€œmanualâ€ is the default value for mapping. The â€œvalueâ€ treemap considers palette to be a diverging color palette (say ColorBrewerâ€™s â€œRdYlBuâ€), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The â€œmanualâ€ treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\nThe â€œvalueâ€ type treemapThe â€œmanualâ€ type treemapâ€œmanualâ€ type treemap - single colour palette\n\n\nThe code chunk below shows a value type treemap.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\nThe â€œmanualâ€ type does not interpret the values as the â€œvalueâ€ type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\n\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nTreemap Layout\ntreemap() supports two popular treemap layouts, namely: â€œsquarifiedâ€ and â€œpivotSizeâ€. The default is â€œpivotSizeâ€.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\nalgorithm argumentUsing sortID\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nWhen â€œpivotSizeâ€ algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 5a",
    "section": "Designing Treemap using treemapify Package",
    "text": "Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to â€œtreemapifyâ€ its user guide.\n\nbasic treemap\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nDefining hierarchy\n\nGroup by Planning RegionGroup by Planning AreaAdding boundary line\n\n\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 5a",
    "section": "Designing Interactive Treemap using d3treeR",
    "text": "Designing Interactive Treemap using d3treeR\n\nInstalling d3treeR package\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\n\nCode\n# install.packages(\"devtools\")\n\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\n\nCode\nlibrary(devtools)\n# install_github(\"timelyportfolio/d3treeR\")\n\n\n\nNow you are ready to launch d3treeR package\n\n\n\nCode\nlibrary(d3treeR)\n\n\n\n\nDesigning An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\n\nCode\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\n\nCode\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "theory: a summary of R for Visual Analytics - Chap 2\n\na summary of 4 ggplot2 extensions\na dive into ggplot2 extensions\n\npractice: some exploration about the dataset\n\nQucik access to Some Plotting Exercise below"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 2",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nIn this exercise, beside tidyverse, four R packages will be used. They are: ggrepel, ggthemes, hrbrthemes, patchwork.\nA summary of these 4 packages can be found at the section below.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data",
    "title": "Hands-on Exercise 2",
    "section": "Importing data",
    "text": "Importing data\nThe code chunk below is to import the data\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggrepel",
    "title": "Hands-on Exercise 2",
    "section": "ggrepel",
    "text": "ggrepel\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text\n\nwith ggrepelwith ggplot2 only\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetâ€™s look at the code\n\nwith ggrepelwith ggplot2 only\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere, we just simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes",
    "title": "Hands-on Exercise 2",
    "section": "ggthemes",
    "text": "ggthemes\n\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\nggthemes provides â€˜ggplot2â€™ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, â€˜Stataâ€™, â€˜Excelâ€™, and The Wall Street Journal, among others.\n\nRefer to this link to learn more about ggplot2 Themes\n\nggplot2 themeEconomist themeWall Street Journal theme\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj() +\n  theme(plot.title = element_text(size = 14))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#hrbrthemes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#hrbrthemes",
    "title": "Hands-on Exercise 2",
    "section": "hrbrthemes",
    "text": "hrbrthemes\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\nThe second goal centers around productivity for a production workflow. In fact, this â€œproduction workflowâ€ is the context for where the elements of hrbrthemes should be used. Consult this vignette to learn more.\n\n\ndefault theme_ipsum()modifying theme_ipsum()\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\naxis_title_size: increase the font size of the axis title to 18\nbase_size: increase the default axis label to 15\ngrid: remove the x-axis grid lines.\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#patchwork",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#patchwork",
    "title": "Hands-on Exercise 2",
    "section": "patchwork",
    "text": "patchwork\nThere are several ggplot2 extensionâ€™s functions support the needs to prepare composite figure by combining several graphs such as grid.arrange() of gridExtra package and plot_grid() of cowplot package.\npatchwork is specially designed for combining separate ggplot2 graphs into a single figure. It has a very simple syntax where we can create layouts super easily. Hereâ€™s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\ncreate 3 statistical graphics to be used for composite graphs:\n\np1p2p3\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\np1\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\np2\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\np3\n\n\n\n\n\n\n\nCode chunk below shows a composite of graphs created using patchwork\n\n2 graphs3 graphs3 graphs with tagCreating figure with insert\n\n\nCode below shows a composite of two histograms created using patchwork\n\np1 + p2\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\npatchwork provides auto-tagging to identify subplots\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\ninset_element() place one or several plots freely on top or below another plot\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-multiple-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-multiple-packages",
    "title": "Hands-on Exercise 2",
    "section": "combining multiple packages",
    "text": "combining multiple packages\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist() +\n  theme(plot.title = element_text(size = 12))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#stacked-bar-chart-of-race-distribution-by-gender",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#stacked-bar-chart-of-race-distribution-by-gender",
    "title": "Hands-on Exercise 2",
    "section": "Stacked Bar Chart of Race Distribution by Gender",
    "text": "Stacked Bar Chart of Race Distribution by Gender\n\n\nShow the code\np4 &lt;- ggplot(data = exam_data, \n       aes(x = reorder(RACE, -table(RACE)[RACE]), fill = GENDER)) +\n  geom_bar(position = \"stack\",\n           alpha = 0.9) +\n  geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    position = position_stack(vjust = 0.5),\n    size = 3,\n    color = \"white\"\n  ) +\n  labs(title = \"Race Distribution by Gender\", x = \"Race\", y = \"Number of Students\") +\n  theme_fivethirtyeight()\np4"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#boxplot-of-english-scores-by-class",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#boxplot-of-english-scores-by-class",
    "title": "Hands-on Exercise 2",
    "section": "Boxplot of English Scores by Class",
    "text": "Boxplot of English Scores by Class\n\n\nShow the code\np5 &lt;- ggplot(data = exam_data, \n       aes(x = CLASS, y = ENGLISH)) +\n  geom_boxplot(fill = \"grey90\", color = \"grey25\") +\n  geom_hline(yintercept = mean(exam_data$ENGLISH), linetype = \"dashed\", color = \"#CD2626\") +\n  stat_summary(\n    fun = mean, \n    geom = \"point\", \n    color = \"#CD2626\"\n  ) +\n  annotate(\n    \"text\", \n    x = 1,  y = mean(exam_data$ENGLISH) + 2,\n    label = paste(\"Avg:\", round(mean(exam_data$ENGLISH), 2)),\n    color = \"#CD2626\"\n  ) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(\n    title = \"English Scores by Class\",\n    x = \"Class\",\n    y = \"English Score\"\n  ) +\n  theme_wsj()\np5"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#scatterplot-of-math-and-science-scores",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#scatterplot-of-math-and-science-scores",
    "title": "Hands-on Exercise 2",
    "section": "Scatterplot of Math and Science Scores",
    "text": "Scatterplot of Math and Science Scores\n\n\nShow the code\np6 &lt;- ggplot(data = exam_data,\n       aes(x = MATHS, y = SCIENCE)) +\n  geom_point(aes(color = GENDER), size = 1.5, alpha = 0.7) +\n  geom_hline(yintercept = 50, linetype = \"dashed\", color = \"gray\") +  \n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"gray\") +  \n  geom_smooth(method = \"lm\", size = 0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  labs(\n    title = \"Correlation between Math and Science Scores\",\n    x = \"Math Score\",\n    y = \"Science Score\"\n  ) +\n  coord_cartesian(xlim = c(0, 100), ylim = c(0, 100)) +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\np6"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#density-plot-of-english-scores-by-class",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#density-plot-of-english-scores-by-class",
    "title": "Hands-on Exercise 2",
    "section": "Density Plot of English Scores by Class",
    "text": "Density Plot of English Scores by Class\n\n\nShow the code\np7 &lt;- ggplot(data = exam_data, \n       aes(x = ENGLISH, fill = GENDER)) +\n  geom_density(alpha = 0.5, color = \"black\", linewidth = 0.3) + \n  labs(title = \"Distribution of English Scores by Class\", x = \"English Score\") +\n  theme_tufte() +\n  facet_grid(CLASS ~ .) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.text = element_text(size = 8),  \n        legend.title = element_text(size = 8))\np7"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#composite-chart",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#composite-chart",
    "title": "Hands-on Exercise 2",
    "section": "Composite Chart",
    "text": "Composite Chart\n\npatchwork &lt;- (p4 / p6) | p7\npatchwork & theme_economist() +\n  theme(plot.title = element_text(size = 15))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "a summary of Heatmap for Visualising and Analysing Multivariate Data - Qucik access to Visualising Heatmap\na summary of Visual Multivariate Analysis with Parallel Coordinates Plot - Qucik access to Parallel Coordinates Plot\na summary of Creating Ternary Plot with R - Qucik access to Ternary Plot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#heatmap",
    "title": "Hands-on Exercise 5b",
    "section": "Heatmap",
    "text": "Heatmap\n\n\n\npackage\nR\nheatmaply\n\n\nfunc\nheatmap()\nheatmaply()\n\n\ncommon arguement\nRowv\nColv\nscale\ncexRow\ncexCol\nmargins\nscale\nnormalize(data)\npercentize(data)\ndist_method\nhclust_method\nseriate\ncolors\n\n\nType\nstatic\ninteractive"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#parallel",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#parallel",
    "title": "Hands-on Exercise 5b",
    "section": "Parallel Coordinates Plot",
    "text": "Parallel Coordinates Plot\n\n\n\npackage\nGGally\nparallelPlot\n\n\nfunc\nggparcoord()\nparallelPlot()\n\n\ncommon arguement\ngroupColumn\nscale\nalphaLines\nboxplot\ncontinuousCS\nhistoVisibility\n\n\nType\nstatic\ninteractive"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#ternary-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#ternary-plot",
    "title": "Hands-on Exercise 5b",
    "section": "Ternary Plot",
    "text": "Ternary Plot\n\n\n\npackage\nggtern\nPlotly R\n\n\nfunc\nggtern()\nplot_ly()\n\n\ncommon arguement\naes(x, y, z)\nplot_ly( data,\nx, y, x,\ntype = \"scatterternary\")\n%&gt;%\nlayout(annotations, ternary)\n\n\nType\nstatic\ninteractive"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#data-preparation",
    "title": "Hands-on Exercise 5b",
    "section": "Data Preparation",
    "text": "Data Preparation\ncode chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\n\nCode\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\nWorld Happines 2018 report will be used. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\nImportingRenameConvert to Matrix\n\n\nread_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\n\nCode\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\nrename rows to country name instead of row number\n\n\nCode\nrow.names(wh) &lt;- wh$Country\n\n\n\n\ndata has to be a matrix to make a heatmap. The code chunk below transform wh into a data matrix\n\n\nCode\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#heatmap-of-r-stats",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#heatmap-of-r-stats",
    "title": "Hands-on Exercise 5b",
    "section": "heatmap() of R Stats",
    "text": "heatmap() of R Stats\nheatmap() of R stats package draws a simple heatmap.\n\nDefaultWith dendrogramsNormalize\n\n\nuse heatmap() of Base Stats to plot a heatmap\n\n\nCode\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\n\n\n\n\nCode\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHowever, this heatmap is not really informative as the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix.\n\n\n\n\nCode\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNotice that the values are scaled now. margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#interactive-heatmap-with-heatmaply",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#interactive-heatmap-with-heatmaply",
    "title": "Hands-on Exercise 5b",
    "section": "Interactive Heatmap with heatmaply",
    "text": "Interactive Heatmap with heatmaply\nheatmaply is an R package for building interactive cluster heatmap which allows hovering and zooming into a region of the heatmap.\nDifferent from heatmap() of R, for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap. The text label of each raw, on the other hand, is placed on the right hand side of the heat map.\n\nDefault Plot\nThe code chunk creates an interactive heatmap by using heatmaply package.\n\n\nCode\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\nData Transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variablesâ€™ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThe package offers 3 main data transformation methods, namely: scale, normalise and percentilse to ensure all the variables have comparable values.\n\nScalingNormalizingPercentising\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\n\nCode\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variableâ€™s distribution while making them easily comparable on the same â€œscaleâ€.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e.Â wh_matrix as shown in the code chunk below.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e.Â wh_matrix as shown in the code chunk below.\n\n\nCode\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n\n\nClustering Algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: distance (dissimilarity) between both rows and columns. Defaults to dist. The options â€œpearsonâ€, â€œspearmanâ€ and â€œkendallâ€ can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\ndist_method default is NULL, which results in â€œeuclideanâ€ to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is â€œdistâ€â€ hence this can be one of â€œeuclideanâ€, â€œmaximumâ€, â€œmanhattanâ€, â€œcanberraâ€, â€œbinaryâ€ or â€œminkowskiâ€.\nhclustfun: hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\nhclust_method default is NULL, which results in â€œcompleteâ€ method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of â€œward.Dâ€, â€œward.D2â€, â€œsingleâ€, â€œcompleteâ€, â€œaverageâ€ (= UPGMA), â€œmcquittyâ€ (= WPGMA), â€œmedianâ€ (= WPGMC) or â€œcentroidâ€ (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\nManual approachStatistical approach\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with â€œEuclidean distanceâ€ and â€œward.Dâ€ method.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n\nCode\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that â€œaverageâ€ method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\n\nCode\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\n\n\nSeriation\nOne of the problems with hierarchical clustering is that it doesnâ€™t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C canâ€™t end up between A and B, but it doesnâ€™t tell you which way to flip the A+B cluster. It doesnâ€™t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nOLOGWMeanNone\n\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nThe default options is â€œOLOâ€ (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)).\n\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nâ€œGWâ€ (Gruvaeus and Wainer) aims for the same goal but uses a potentially faster heuristic\n\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nThe option â€œmeanâ€ gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\nThe option â€œnoneâ€ gives us the dendrograms without any rotation that is based on the data matrix.\n\n\n\n\n\nWorking with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nFinishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "title": "Hands-on Exercise 5b",
    "section": "Overview",
    "text": "Overview\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), â€œThis certainly isnâ€™t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isnâ€™t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.â€ For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\n2 methods will be covered:\n\nstatistic: ggparcoord() of GGally package\ninteractive: parallelPlot package\n\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used. The code chunks below are used to install and load the packages in R.\n\n\nCode\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\nThe same World Happinees 2018 will be used and the data frame is named as wh\n\n\nCode\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#static-parallel-coordinates-plot---ggparcoord",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#static-parallel-coordinates-plot---ggparcoord",
    "title": "Hands-on Exercise 5b",
    "section": "Static Parallel Coordinates Plot - ggparcoord()",
    "text": "Static Parallel Coordinates Plot - ggparcoord()\n\nDefaultWith BoxplotFacetRotating Text LabelAdjusting Text Label\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n2 argument namely data and columns is used.\n\nData: is used to map the data object (i.e.Â wh)\n\n\n\ncolumns: select the columns for preparing the parallel coordinates plot\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. Boxplot will be included.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\ngroupColumn : group the observations (i.e.Â parallel lines) by using a single variable (i.e.Â Region) and colour the parallel coordinates lines by region name.\nscale : scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines : reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot : urn on the boxplot by using logical TRUE. The default is FALSE.\ntitle : provide the parallel coordinates plot a title.\n\n\n\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\nSome of the variable names overlap on x-axis, rotate the labels by 30 degrees using theme() function in ggplot2\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\naxis.text.x : argument to theme() function. And specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\nadjusting the text location using hjust argument to themeâ€™s text element with element_text(). axis.text.x is used to change the look of x-axis text.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 5b",
    "section": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using â€˜htmlwidgetsâ€™ package and d3.js. It can build interactive parallel coordinates plot.\n\nDefaultRotate axis labelColour schemeWith Histogram\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\n\nCode\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\n\nCode\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\n\nCode\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\n\nCode\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#data-preparation-1",
    "title": "Hands-on Exercise 5b",
    "section": "Data Preparation",
    "text": "Data Preparation\nSingapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used.\n\nImportFeature Engineering\n\n\nTo import respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n\nCode\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\")\n\n\n\n\nuse the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nCode\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5b",
    "section": "Plotting Ternary Diagram with R",
    "text": "Plotting Ternary Diagram with R\n\nstatic - Defaultstatic - rmb themeinteractive\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n\nCode\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R\n\n\nCode\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "According to an official report by Ministry of Suatainbility and the Environment as shown in the infographic below:\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\nThis project will apply suitable interactive techniques to verify the assertions mentioned in the second portion, examining the pronounced contrast between the wet months (November to January) and the dry months (February, and June to September). The complete project brief is available here."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#project-brief",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#project-brief",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "According to an official report by Ministry of Suatainbility and the Environment as shown in the infographic below:\n\nDaily mean temperature are projected to increase by 1.4 to 4.6, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\nThis project will apply suitable interactive techniques to verify the assertions mentioned in the second portion, examining the pronounced contrast between the wet months (November to January) and the dry months (February, and June to September). The complete project brief is available here."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-selection",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-selection",
    "title": "Take-home Exercise 3",
    "section": "Data Selection",
    "text": "Data Selection\nHistorical daily rainfall records have been meticulously maintained and is accessible here. Changi station was strategically chosen for its status as the longest-serving climate station, boasting the most comprehensive dataset.\nThe climate maps below reveals a significant contrast in rainfall between December (indicated by the darkest blue) and February (represented by the lightest blue). However, February has fewer days. To facilitate a more straightforward analysis, data from July, a month with an equal number of days, will be used for comparison alongside December.\n\nThe data, consisting of historical daily temperature and rainfall records for every July and December from the years 1983, 1993, 2003, 2013, and 2023, has been downloaded from Historical Daily Records, in the format of 'DAILYDATA_S24_YYYYMM.csv'"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#loading-r-packages",
    "title": "Take-home Exercise 3",
    "section": "Loading R packages",
    "text": "Loading R packages\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse,\n               readxl, gifski, gapminder,\n               plotly, gganimate,networkD3, \n               ggtext, grid, ggnewscale,shadowtext)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-data",
    "title": "Take-home Exercise 3",
    "section": "Importing Data",
    "text": "Importing Data\nThe code chunk below use read_csv to import datasets for July and December across various years, and decode the data using â€œISO-8859-1â€ for all past data except 2023 data.\n\n\nCode\njul_83 &lt;- read_csv(\"data/DAILYDATA_S24_198307.csv\", locale = locale(encoding = \"ISO-8859-1\"))\njul_93 &lt;- read_csv(\"data/DAILYDATA_S24_199307.csv\", locale = locale(encoding = \"ISO-8859-1\"))\njul_03 &lt;- read_csv(\"data/DAILYDATA_S24_200307.csv\", locale = locale(encoding = \"ISO-8859-1\"))\njul_13 &lt;- read_csv(\"data/DAILYDATA_S24_201307.csv\", locale = locale(encoding = \"ISO-8859-1\"))\njul_23 &lt;- read_csv(\"data/DAILYDATA_S24_202307.csv\")\ndec_83 &lt;- read_csv(\"data/DAILYDATA_S24_198312.csv\", locale = locale(encoding = \"ISO-8859-1\"))\ndec_93 &lt;- read_csv(\"data/DAILYDATA_S24_199312.csv\", locale = locale(encoding = \"ISO-8859-1\"))\ndec_03 &lt;- read_csv(\"data/DAILYDATA_S24_200312.csv\", locale = locale(encoding = \"ISO-8859-1\"))\ndec_13 &lt;- read_csv(\"data/DAILYDATA_S24_201312.csv\", locale = locale(encoding = \"ISO-8859-1\"))\ndec_23 &lt;- read_csv(\"data/DAILYDATA_S24_202312.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#merging-and-selecting-dataframe",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#merging-and-selecting-dataframe",
    "title": "Take-home Exercise 3",
    "section": "Merging and Selecting Dataframe",
    "text": "Merging and Selecting Dataframe\nUpon examining the data, 2023 dataset had different naming conventions for some columns. The code chunk below rename and combine the dataset use rbind.\n\ndec_23_rename &lt;- dec_23 %&gt;% \n  rename(`Highest 30 Min Rainfall (mm)` = `Highest 30 min Rainfall (mm)`,\n         `Highest 60 Min Rainfall (mm)` = `Highest 60 min Rainfall (mm)`,\n         `Highest 120 Min Rainfall (mm)` = `Highest 120 min Rainfall (mm)`)\n\njul_23_rename &lt;- jul_23 %&gt;% \n  rename(`Highest 30 Min Rainfall (mm)` = `Highest 30 min Rainfall (mm)`,\n         `Highest 60 Min Rainfall (mm)` = `Highest 60 min Rainfall (mm)`,\n         `Highest 120 Min Rainfall (mm)` = `Highest 120 min Rainfall (mm)`)\n\ncomb_data &lt;- rbind(jul_83, dec_83, jul_93, dec_93, jul_03, dec_03, jul_13, dec_13, jul_23_rename, dec_23_rename)\n\n\nFiltering Rainfall Related Data\nThe code chunk below filtered columns relevant to the rainfall analysis Daily Rainfall Total (mm) and renaming them to daily_rainfall for ease of reference.\n\nrainfall &lt;- comb_data %&gt;% \n  select(Year, Month, Day, `Daily Rainfall Total (mm)`) %&gt;% \n  rename(`daily_rainfall` = `Daily Rainfall Total (mm)`)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#summary-statistics",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#summary-statistics",
    "title": "Take-home Exercise 3",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nA quick summary statistic of the data frame is conducted as below\n\nData StructureFirst 5 RowsData HealthSummary data\n\n\nthe code below use dim() to check the dataframe dimension\n\ndim(rainfall)\n\n[1] 310   4\n\n\nThe dataset comprises 310 observations, with 4 variables. 5 years of July and December Data, 31*5*2 = 310 days\n\nstr(rainfall)\n\ntibble [310 Ã— 4] (S3: tbl_df/tbl/data.frame)\n $ Year          : num [1:310] 1983 1983 1983 1983 1983 ...\n $ Month         : num [1:310] 7 7 7 7 7 7 7 7 7 7 ...\n $ Day           : num [1:310] 1 2 3 4 5 6 7 8 9 10 ...\n $ daily_rainfall: num [1:310] 8.7 0 5.3 6.2 39.5 14.9 0 0 10.5 55.5 ...\n\n\nThe dataset presentsâ€™Yearâ€™, â€˜Monthâ€™, and â€˜Dayâ€™ information in separate columns. While this format is advantageous for certain types of grouping and categorization, combining into a datetime format can significantly facilitate time series analysis.\n\n\nCode\nrainfall &lt;- rainfall %&gt;% \n  mutate(date = as.Date(paste(Year, Month, Day, sep=\"-\")))\n\n\n\n\nDisplaying first 5 rows using head()\n\nhead(rainfall)\n\n# A tibble: 6 Ã— 5\n   Year Month   Day daily_rainfall date      \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;date&gt;    \n1  1983     7     1            8.7 1983-07-01\n2  1983     7     2            0   1983-07-02\n3  1983     7     3            5.3 1983-07-03\n4  1983     7     4            6.2 1983-07-04\n5  1983     7     5           39.5 1983-07-05\n6  1983     7     6           14.9 1983-07-06\n\n\n\n\nthe code below use anyDuplicated() to check if any duplicated entries in the dataset.\n\nanyDuplicated(rainfall)\n\n[1] 0\n\n\nthe code use is.na() to check for total number of missing entries in the dataset.\n\nsum(is.na(rainfall))\n\n[1] 0\n\n\nAll observations are unique, and there is no missing data in the dataframe.\n\n\n\nsummary(rainfall)\n\n      Year          Month           Day     daily_rainfall   \n Min.   :1983   Min.   : 7.0   Min.   : 1   Min.   :  0.000  \n 1st Qu.:1993   1st Qu.: 7.0   1st Qu.: 8   1st Qu.:  0.000  \n Median :2003   Median : 9.5   Median :16   Median :  0.600  \n Mean   :2003   Mean   : 9.5   Mean   :16   Mean   :  8.031  \n 3rd Qu.:2013   3rd Qu.:12.0   3rd Qu.:24   3rd Qu.:  8.700  \n Max.   :2023   Max.   :12.0   Max.   :31   Max.   :164.400  \n      date           \n Min.   :1983-07-01  \n 1st Qu.:1993-07-16  \n Median :2003-09-30  \n Mean   :2003-09-30  \n 3rd Qu.:2013-12-15  \n Max.   :2023-12-31"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#feature-engineering",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#feature-engineering",
    "title": "Take-home Exercise 3",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nSeasonalityIf RainIntensity of Rain\n\n\ncreate new col to define dry and wet season\n\nrainfall &lt;- rainfall %&gt;%\n  mutate(\n    season = case_when(\n      Month == 7 ~ \"Dry Season\",\n      Month == 12 ~ \"Wet Season\"\n    )\n  )\n\n\n\nIn order to undersatand the Frequency of rains, a new column is created to check if that day rains\nAccording to Meteorological Service Singapore, A day is considered to have â€œrainedâ€ if the total rainfall for that day is 0.2mm or more\nThe code chunk below creates a new column to check the daily rainfall and decide if the day rains,\n\nrainfall &lt;- rainfall %&gt;%\n  mutate(\n    if_rain = daily_rainfall &gt; 0.2\n  )\n\n\n\nIn this study, I analyze the distribution of daily rainfall volumes, as shown in histogram below. A notable observation from this graph is its rightward skew, indicating a concentration of days with lower rainfall volumes and fewer instances of high rainfall.\n\n\nCode\nfiltered_rainfall &lt;- rainfall %&gt;% filter(if_rain == TRUE)\n\nggplot(filtered_rainfall, aes(x = daily_rainfall)) + \n  geom_histogram(bins = 20) +  \n  theme_minimal() +\n  labs(y = \"Number of Days\", x = \"Daily Rainfall (mm)\")  \n\n\n\n\n\nUnderstanding the severity of precipitation is crucial. Classification of daily rainfall intensity is crucial for our analysis. In this context, I adopt the classification standards set by the Chinese Meteorology Department used under this paper, which are as follows:\n\nLess than 0.2 mm: Not rain\n0.2-10 mm: Light rainfall\n10-25 mm: Moderate rainfall\n25-50 mm: Heavy rainfall\n50-100 mm: Storm\n100-250 mm: Severe storm\n250 mm or more: Extreme storm\n\nThe code below use dplyr package to create a new column intensity.\n\n\nCode\nrainfall &lt;- rainfall %&gt;%\n  mutate(\n    Intensity = case_when(\n      daily_rainfall &lt; 0.2 ~ \"Not rain\",\n      daily_rainfall &gt;= 0.2 & daily_rainfall &lt; 10 ~ \"Light rainfall\",\n      daily_rainfall &gt;= 10 & daily_rainfall &lt; 25 ~ \"Moderate rainfall\",\n      daily_rainfall &gt;= 25 & daily_rainfall &lt; 50 ~ \"Heavy rainfall\",\n      daily_rainfall &gt;= 50 & daily_rainfall &lt; 100 ~ \"Storm\",\n      daily_rainfall &gt;= 100 & daily_rainfall &lt; 250 ~ \"Severe storm\",\n      daily_rainfall &gt;= 250 ~ \"Extreme storm\"\n    )\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#volume-of-rainfall",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#volume-of-rainfall",
    "title": "Take-home Exercise 3",
    "section": "Volume of Rainfall",
    "text": "Volume of Rainfall\nOn the bigger picture, it is necessary to understand the total rainfall within the selected month between the dry and wet season across the years,\nthis drawing is inspired by the economist\n\n\nCode\n# define colors scheme\nBLUE &lt;- \"#33CCCC\"\nRED &lt;- \"#FF6666\"\n\n# define labels for the lines\nline_labels&lt;- data.frame(\n  labels = c(\"Wet Season\", \"Dry Season\"),\n  x = c(1983, 1983),\n  y = c(390, 160),\n  color = c(RED, BLUE)\n)\n\n# group by to get monthly rainfall\ntotal_rainfall &lt;- rainfall %&gt;%\n  group_by(Year, season) %&gt;%\n  summarise(accumulated_rainfall = sum(daily_rainfall))\n\n# line plot with point \nplt1 &lt;- ggplot(total_rainfall, aes(x = Year, y = accumulated_rainfall)) +\n  geom_line(aes(color = season), size = 1) +\n  geom_point(\n    aes(fill = season), \n    size = 5, \n    pch = 21, \n    color = \"white\", \n    stroke = 1 \n  ) +\n  scale_color_manual(values = c(RED, BLUE)) +\n  scale_fill_manual(values = c(RED, BLUE)) +\n  theme(legend.position = \"none\") \n\n# setting drawing style\nplt1 &lt;- plt1 +\n  scale_x_continuous(\n    limits = c(1980, 2025),\n    expand = c(0, 0), \n    breaks = c(1983, 1993, 2003, 2013, 2023),  \n    labels = c(\"1983\", \"1993\", \"2003\", \"2013\", '2023') \n  ) +\n  scale_y_continuous(\n    limits = c(0, 410),\n    breaks = seq(0, 400, by = 50), \n    expand = c(0, 0)\n  ) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(color = \"#A8BAC4\", size = 0.3),\n    axis.ticks.length.y = unit(0, \"mm\"), \n    axis.ticks.length.x = unit(2, \"mm\"),\n    axis.title = element_blank(),\n    axis.line.x.bottom = element_line(color = \"black\"),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 10)\n  )\n\n# adding line labels \nplt4 &lt;- plt1 + \n  new_scale_color() + \n  geom_shadowtext(\n    aes(x, y, label = labels, color = color),\n    data = line_labels,\n    hjust = 0, \n    bg.colour = \"white\", \n    bg.r = 0.4, \n    size = 3\n  ) \n\n# Add labels for y axis\nplt4 &lt;- plt4 + \n  geom_text(\n    data = data.frame(x = 2025, y = seq(0, 400, by = 50)),\n    aes(x, y, label = y),\n    hjust = 1, \n    vjust = 0, \n    nudge_y = 32 * 0.01, \n    size = 3\n  )\n\n# Add title\nplt4 &lt;- plt4 +\n  labs(\n    title = \"**Selected Total Monthly Rainfall,** in mm\",\n  ) + \n  theme(\n    plot.title = element_markdown(\n      size = 15\n    )\n  )\n\nplt4"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#frequency-of-rainfall",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#frequency-of-rainfall",
    "title": "Take-home Exercise 3",
    "section": "Frequency of Rainfall",
    "text": "Frequency of Rainfall\nFurther, itâ€™s important to gauge the frequency of rainfall and its corresponding intensity. This aspect looks into how often it rains and the typical severity of these rainfall events.\n\n\nCode\n# set the order\nrainfall$Intensity &lt;- factor(rainfall$Intensity, levels = c(\"Not rain\", \n                                                            \"Light rainfall\",\n                                                            \"Moderate rainfall\", \n                                                            \"Heavy rainfall\", \n                                                            \"Storm\",\n                                                            \"Severe storm\", \n                                                            \"Extreme storm\"))\n\n# get summary data\nrainfall_summary &lt;- rainfall %&gt;%\n  group_by(Year, season, Intensity) %&gt;%\n  summarise(days_count = n(), .groups = 'drop')\n\n# set color scale\ncolor_scale &lt;- scale_fill_manual(values = c(\"Not rain\" = \"grey\",\n                                            \"Light rainfall\" = \"#ADD8E6\",\n                                            \"Moderate rainfall\" = \"#87CEEB\",\n                                            \"Heavy rainfall\" = \"#00BFFF\",\n                                            \"Storm\" = \"#1E90FF\",\n                                            \"Severe storm\" = \"#0000CD\",\n                                            \"Extreme storm\" = \"dark blue\"))\n\n# Plot as a stacked bar chart\nplt3 &lt;- ggplot(rainfall_summary, aes(x = factor(Year), y = days_count, fill = Intensity)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(aes(label = days_count), position = position_stack(vjust = 0.5), color = \"white\", size = 2.5) +  # Add white labels\n  facet_wrap(~season) +\n  labs(\n    title = \"Frequency of Rainfall Intensity in Selected Month\",\n    x = \"\",\n    y = \"Number of Days\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title = element_text(face = \"bold\"),\n    axis.text.y = element_text(size = 7),\n    axis.text.x = element_text(size = 7),\n    plot.title = element_text(size = 12, face = \"bold\")\n  ) +\n  color_scale\n\n\nplt3"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#intensity-of-rainfall",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#intensity-of-rainfall",
    "title": "Take-home Exercise 3",
    "section": "Intensity of Rainfall",
    "text": "Intensity of Rainfall\nDelving deeper, I analyze the distribution and intensity of daily rainfall. This granular view provides insight into the daily variations and characteristics of rainfall, offering a detailed perspective on its intensity and distribution patterns.\n\n\nCode\n# set the color scale for daily rainfall based on intensity\ncolor_scale &lt;- scale_color_manual(values = c(\"Not rain\" = \"white\",\n                                             \"Light rainfall\" = \"#ADD8E6\",\n                                             \"Moderate rainfall\" = \"#87CEEB\",\n                                             \"Heavy rainfall\" = \"#00BFFF\",\n                                             \"Storm\" = \"#1E90FF\",\n                                             \"Severe storm\" = \"#0000CD\",\n                                             \"Extreme storm\" = \"dark blue\"),\n                                  breaks = c(\"Not rain\", \"Light rainfall\", \"Moderate rainfall\", \n                                            \"Heavy rainfall\", \"Storm\", \"Severe storm\", \"Extreme storm\"))\n\n# filter only rainy days\nfiltered_rainfall &lt;- rainfall %&gt;% \n  filter(if_rain == TRUE)\n\n# hover text\nfiltered_rainfall$hover_text &lt;- paste(\n  \"Day:\", filtered_rainfall$date,\n  \"\\nDaily Rainfall:\", filtered_rainfall$daily_rainfall,\n  \"\\nSeason:\", filtered_rainfall$season,\n  \"\\nIntensity:\", filtered_rainfall$Intensity\n)\n\n\n# plot a scatter diagram\nplt4 &lt;- ggplot(filtered_rainfall, aes(x=factor(Year), y=factor(Day), size = daily_rainfall, color = Intensity, text = hover_text)) +\n  geom_point(alpha=0.7) +\n  facet_wrap(~season) +\n  color_scale +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 5),\n    axis.text.x = element_text(size = 5),\n    plot.title = element_text(size = 12, face = \"bold\"),\n    strip.text = element_text(size = 8)\n  ) +\n  labs(\n    title = \"Intensity of Daily Rainfall in Selected Month\",\n    y = \"\",\n    x = \"\" \n  ) +\n  scale_size_continuous(range = c(0.1, 10), name = \"\") +\n  guides(color = guide_legend(title = \"\", order = 1))\n\nplt4_stat &lt;- plt4 +\n  scale_size_continuous(range = c(0.1, 10), name = \"Daily Rainfall\") + \n  guides(color = guide_legend(order = 1))\n\nplt4_stat"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#static-dashboard",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#static-dashboard",
    "title": "Take-home Exercise 3",
    "section": "Static Dashboard",
    "text": "Static Dashboard\n\n\nCode\npatchwork &lt;- (plt1_stat | plt3) / (plt2 | plt4_stat)\npatchwork + \n  plot_annotation(title = \"Volumn, Frequency, Intensity of Rainfall Comparasion in Selected Months\") \n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nVolume\nRegarding the total volume of monthly rainfall, both dry and wet seasons appear to have decreased from 1983 to 2023. There is no clear pattern in the difference between the two seasons in the selected data.\nInterestingly, when examining the average daily rainfall for rainy days, the wet seasonâ€™s average gradually declines over time, while the dry seasonâ€™s average actually increases. Specifically, in 2023, the mean daily rainfall for the dry season is higher than that of the wet season. The standard error for the wet season diminishes over time, indicating more consistent rainfall amounts, whereas it increases for the dry season, suggesting greater variability. This inconsistency in the dry season, with larger mean values, might be attributed to anomalies, as can also be inferred from the chart at the bottom right.\nWhen analyzing the two charts in parallel, the wet season exhibits higher total monthly rainfall but lower average rainfall on rainy days as we approach 2023. This suggests a more frequent, yet lighter, rainfall pattern in the wet season, supported by the higher proportion of blue in the bottom left chart and smaller blue bubbles in the wet season on the right chart. In contrast, the dry season demonstrates a more irregular rainfall pattern. Despite more non-rainy days and lower total monthly rainfall, the average rainfall actually increases.\nFrequency\nThe number of rainy days from 1983 to 2023 increases in the wet season and decreases in the dry season. Considering the frequency of rainy days, there is a more pronounced contrast between the wet and dry seasons over the years in the selected data. However, the occurrence of heavier rains (indicated by darker colors) varies over time.\nIntensity\nIn terms of rainfall intensity, earlier years seem to have experienced more extreme rainfall. The bottom chart shows more dots (rainy days) for the wet season in 2023 compared to earlier years, and fewer dots for the dry season. However, the intensity of rainfall, especially heavy rainfall, has decreased.\n\n\nAs the static charts only shows an overall trends, it would be more interesting to interactively filter and select based on the rainy patterns, to understand and dive into the overall rainy behaviors between dry and wet season across the years."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-dashboard",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#interactive-dashboard",
    "title": "Take-home Exercise 3",
    "section": "Interactive Dashboard",
    "text": "Interactive Dashboard\nplotly for individual plot\n\n\nCode\nplt1_int &lt;- ggplotly(plt1, tooltip = \"text\") \n# plt1_int\n\n\n\n\nCode\nplt2_int &lt;- ggplotly(plt2, tooltip = \"text\")  %&gt;% \n  layout(showlegend = FALSE)\n# plt2_int\n\n\n\n\nCode\nplt5 &lt;- subplot(plt1_int, plt2_int, nrows = 2, shareX = TRUE, shareY = FALSE, titleX = FALSE, titleY = FALSE)\n# plt5\n\n\n\n\nCode\n# get pivot table\nrainfall_pivot &lt;- rainfall_summary %&gt;%\n  pivot_wider(names_from = Intensity, values_from = days_count, values_fill = list(days_count = 0))\n\n# Convert Year to factor\nrainfall_pivot$Year &lt;- as.factor(rainfall_pivot$Year)\n\n# Define the color scale\ncolors &lt;- c(\"Not rain\" = \"grey\",\n            \"Light rainfall\" = \"#ADD8E6\",\n            \"Moderate rainfall\" = \"#87CEEB\",\n            \"Heavy rainfall\" = \"#00BFFF\",\n            \"Storm\" = \"#1E90FF\",\n            \"Severe storm\" = \"#0000CD\",\n            \"Extreme storm\" = \"dark blue\")\n\n# Create individual plots for wet season with trace\nplot_wet &lt;- plot_ly(data = filter(rainfall_pivot, season == \"Wet Season\"), \n                    x = ~Year, type = 'bar',\n                    y = ~`Severe storm`, name = 'Severe storm', marker = list(color = colors[\"Severe storm\"]),\n                    legendgroup = \"Severe storm\",showlegend = FALSE\n                    ) %&gt;%\n  add_trace(y = ~Storm, name = 'Storm', marker = list(color = colors[\"Storm\"]),\n            legendgroup = \"Storm\",showlegend = FALSE) %&gt;%\n  add_trace(y = ~`Heavy rainfall`, name = 'Heavy rainfall', marker = list(color = colors[\"Heavy rainfall\"]),\n            legendgroup = \"Heavy rainfall\",showlegend = FALSE) %&gt;%\n  add_trace(y = ~`Moderate rainfall`, name = 'Moderate rainfall', marker = list(color = colors[\"Moderate rainfall\"]),\n            legendgroup = \"Moderate rainfall\",showlegend = FALSE) %&gt;%\n  add_trace(y = ~`Light rainfall`,name = 'Light rainfall',marker = list(color = colors[\"Light rainfall\"]),\n            legendgroup = \"Light rainfall\") %&gt;%\n  add_trace(y = ~`Not rain`, name = 'Not rain', marker = list(color = colors[\"Not rain\"]),\n            legendgroup = \"Not rain\",showlegend = FALSE) %&gt;%\n  layout(yaxis = list(title = 'Number of Days'), \n         xaxis = list(showticklabels = FALSE), # Turn off x-axis labels\n         barmode = 'stack',showlegend = FALSE)\n\n\n# Create individual plots for dry season with trace\nplot_dry &lt;- plot_ly(data = filter(rainfall_pivot, season == \"Dry Season\"), \n                    x = ~Year, type = 'bar',\n                    y = ~`Severe storm`, name = 'Severe storm', marker = list(color = colors[\"Severe storm\"]),\n                    legendgroup = \"Severe storm\", showlegend = FALSE\n                    ) %&gt;%\n  add_trace(y = ~Storm, name = 'Storm', marker = list(color = colors[\"Storm\"]),\n            legendgroup = \"Storm\", showlegend = FALSE) %&gt;%\n  add_trace(y = ~`Heavy rainfall`, name = 'Heavy rainfall', marker = list(color = colors[\"Heavy rainfall\"]),\n            legendgroup = \"Heavy rainfall\", showlegend = FALSE) %&gt;%\n  add_trace(y = ~`Moderate rainfall`, name = 'Moderate rainfall', marker = list(color = colors[\"Moderate rainfall\"]),\n            legendgroup = \"Moderate rainfall\") %&gt;%\n  add_trace(y = ~`Light rainfall`,name = 'Light rainfall',marker = list(color = colors[\"Light rainfall\"]),\n            legendgroup = \"Light rainfall\", showlegend = FALSE) %&gt;%\n  add_trace(y = ~`Not rain`, name = 'Not rain', marker = list(color = colors[\"Not rain\"]),\n            legendgroup = \"Not rain\", showlegend = FALSE) %&gt;%\n  layout(yaxis = list(title = 'Number of Days'), \n         xaxis = list(showticklabels = FALSE), # Turn off x-axis labels\n         barmode = 'stack',\n         showlegend = FALSE)\n\n# Combine the plots using subplot\nplt_bar &lt;- subplot(plot_dry, plot_wet, nrows = 1, shareX = TRUE, shareY = TRUE, titleX = FALSE, titleY = TRUE) %&gt;%\n  layout(\n    annotations = list(\n      list(\n        text = \"Dry Season\", \n        x = 0.07, xref = \"paper\", y = 0.98, yref = \"paper\",\n        showarrow = FALSE, xanchor = \"center\", yanchor = \"bottom\"\n      ),\n      list(\n        text = \"Wet Season\", #\n        x = 0.59, xref = \"paper\", y = 0.98, yref = \"paper\",\n        showarrow = FALSE, xanchor = \"center\", yanchor = \"bottom\"\n      )\n    )\n  )\n\nplt3_int &lt;- plt_bar %&gt;%\n  layout(showlegend = FALSE)\n# plt3_int\n\n\n\n\nCode\nplt4_int &lt;- ggplotly(plt4, tooltip = \"text\") \n# plt4_int\n\n\ncombining right plots\n\n\nCode\nplt6 &lt;- subplot(plt3_int, plt4_int, nrows = 2, shareX = FALSE, shareY = FALSE, titleX = FALSE, titleY = FALSE)\n# plt6\n\n\ncombining call plots\n\n\nCode\nplt &lt;- subplot(plt5, plt6, nrows = 1, shareX = FALSE, shareY = FALSE, titleX = FALSE, titleY = FALSE)\n\n\n# Adjusting layout\nplt &lt;- plt %&gt;% layout(\n  title = \"Volume, Frequency, Intensity of Rainfall Comparison in Selected Months\",\n  showlegend = TRUE,  \n  font = list(size = 8),  \n  margin = list(l = 50, r = 50, b = 50, t = 100, pad = 4),  \n  legend = list(font = list(size = 8)),  \n  uniformtext = list(minsize = 8, mode = 'hide')\n)\n\nplt"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "Superstore Sales and Profit Report\nSuperstore Sales and Profit Story\nExam Data"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#total-volume-of-rainfall",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#total-volume-of-rainfall",
    "title": "Take-home Exercise 3",
    "section": "Total Volume of Rainfall",
    "text": "Total Volume of Rainfall\nOn the bigger picture, it is necessary to understand the total rainfall within the selected month between the dry and wet season across the years.\nthis drawing is inspired by the economist and this plot\n\n\nCode\n# Define the colors scheme\nRED &lt;- \"#FF6666\"\nBLUE &lt;- \"#00BFFF\"\n\ntotal_rainfall &lt;- rainfall %&gt;%\n  group_by(Year, season) %&gt;%\n  summarise(\n    monthly_rainfall = sum(daily_rainfall), \n    rainy_days = sum(if_rain, na.rm = TRUE), \n    .groups = 'drop'\n  ) \n\n# Pivot table including number of rainy days\npivot_season &lt;- total_rainfall %&gt;%\n  pivot_wider(\n    names_from = season,      \n    values_from = c(monthly_rainfall, rainy_days), \n    values_fill = list(monthly_rainfall = 0, rainy_days = 0) \n  )\n\npivot_season &lt;- pivot_season %&gt;% \n  rename(`Dry Season` = `monthly_rainfall_Dry Season`,\n         `Wet Season` = `monthly_rainfall_Wet Season`,\n         `day_dry` = `rainy_days_Dry Season`,\n         `day_wet` = `rainy_days_Wet Season`\n         )\n\n# Create custom hover text for each season\npivot_season$hover_text &lt;- paste(\"Year:\", pivot_season$Year, \n                                 \"\\nWet Season:\", pivot_season$`Wet Season`, \n                                 \"\\nDifference:\", round(pivot_season$`Wet Season` - pivot_season$`Dry Season`), \n                                 \"\\nDry Season:\", pivot_season$`Dry Season`)\n\n# draw 2 line plot with area shaded in between\nplt1 &lt;- ggplot(pivot_season, aes(x = Year)) +\n  geom_ribbon(aes(ymin = `Dry Season`, ymax = `Wet Season`), fill = \"gray70\", alpha = 0.3) +\n  geom_line(aes(y = `Dry Season`, color = \"Dry Season\"), size = 1) +\n  geom_line(aes(y = `Wet Season`, color = \"Wet Season\"), size = 1) +\n  labs(color = \"Season\") +  # Changing legend title\n  geom_point(aes(y = `Dry Season`, fill = \"Dry Season\", text = hover_text, size = 3),  pch = 21, color = \"white\", stroke = 0.5) +\n  geom_point(aes(y = `Wet Season`, fill = \"Wet Season\", text = hover_text, size = 3), pch = 21, color = \"white\", stroke = 0.5) +\n  # scale_size_continuous(range = c(1, 10)) +\n  scale_color_manual(values = c(\"Dry Season\" = RED, \"Wet Season\" = BLUE),\n                     name = \"Season\",\n                     labels = c(\"Dry Season\", \"Wet Season\")) +\n  scale_fill_manual(values = c(\"Dry Season\" = RED, \"Wet Season\" = BLUE),\n                    name = \"Season\",\n                    labels = c(\"Dry Season\", \"Wet Season\")) +\n  geom_segment(data = pivot_season, aes(x = Year,xend = Year, \n                                        y = `Dry Season`, yend = `Wet Season`, text = hover_text),\n               linetype = \"dashed\", color = \"grey50\") \n\n# adjust the layout\nplt1 &lt;- plt1 +\n  guides(color = guide_legend(title = \"Season\"), fill = guide_legend(title = \"Season\")) +\n  ggtitle(\"Difference in Monthly Rainfall by Season\") +\n  scale_x_continuous(\n    limits = c(1980, 2026),\n    expand = c(0, 0), \n    breaks = c(1983, 1993, 2003, 2013, 2023),  \n    labels = c(\"1983\", \"1993\", \"2003\", \"2013\", '2023') \n  ) +\n  scale_y_continuous(\n    limits = c(0, 410),\n    breaks = seq(0, 400, by = 50), \n    expand = c(0, 0)\n  ) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid = element_blank(),\n    panel.grid.major.x = element_line(color = \"#D3D3D3\", linetype = \"solid\", size = 0.2),\n    panel.grid.major.y = element_line(color = \"#D3D3D3\", size = 0.1),\n    axis.ticks.length.y = unit(0, \"mm\"), \n    axis.ticks.length.x = unit(2, \"mm\"),\n    axis.title = element_blank(),\n    axis.line.x.bottom = element_line(color = \"black\"),\n    axis.text.y = element_text(size = 7),\n    axis.text.x = element_text(size = 7),\n    plot.title = element_text(size = 12, face = \"bold\")\n  )\n\nplt1_stat &lt;- plt1 + geom_text(\n  data = pivot_season, aes(x = Year + 1 , y = (`Dry Season` + `Wet Season`) / 2, label = round(`Wet Season` - `Dry Season`)), \n            vjust = -0.5, size = 3)\n\nplt1_stat"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#average-volume-of-rainfall",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#average-volume-of-rainfall",
    "title": "Take-home Exercise 3",
    "section": "Average Volume of Rainfall",
    "text": "Average Volume of Rainfall\nFocusing on the days with precipitation, what are the mean and standard deviation of daily rainfall? To illustrate this, an error bar graph is utilized to represent these statistical measures.\n\n\nCode\n# filter only rainy days\nfiltered_rainfall &lt;- rainfall %&gt;% \n  filter(if_rain == TRUE)\n\n# Assuming 'filtered_rainfall' is your dataframe\nrainfall_summary &lt;- filtered_rainfall %&gt;%\n  group_by(Year, season) %&gt;%\n  summarise(\n    mean_rainfall = mean(daily_rainfall),\n    se_rainfall = sd(daily_rainfall) / sqrt(n()),\n    .groups = 'drop'\n  )\n\n# Create custom hover text for error bars\nrainfall_summary$hover_text &lt;- paste(\n  \"Year:\", rainfall_summary$Year,\n  \"\\nSeason:\", rainfall_summary$season,\n  \"\\nMean Daily Rainfall:\", format(rainfall_summary$mean_rainfall, digits = 2),\n  \"mm\\nStandard Error:\", format(rainfall_summary$se_rainfall, digits = 2)\n)\n\n# Create the plot\nplt2 &lt;- ggplot(rainfall_summary, aes(x = Year, y = mean_rainfall, color = season)) +\n  geom_line(size = 0.5, alpha = 0.5, linetype = \"dashed\") + \n  geom_errorbar(\n    aes(ymin = mean_rainfall - se_rainfall, ymax = mean_rainfall + se_rainfall, text = hover_text),\n    width = 3,\n    alpha = 0.6,  \n    size = 0.8\n  ) +\n  geom_point(\n    aes(text = hover_text),\n    size = 2, \n    alpha = 0.9 \n  ) +\n  scale_color_manual(values = c(\"Dry Season\" = \"#FF6666\", \"Wet Season\" = \"#00BFFF\")) +\n  ggtitle(\"Standard Error of Average Daily Rainfall by Season\") +\n  xlab(\"Year\") +\n  ylab(\"Average Daily Rainfall\")\n\n# adjust the layout\nplt2 &lt;- plt2 +\n  guides(color = guide_legend(title = \"Season\"), fill = guide_legend(title = \"Season\")) +\n  scale_x_continuous(\n    limits = c(1980, 2026),\n    expand = c(0, 0), \n    breaks = c(1983, 1993, 2003, 2013, 2023),  \n    labels = c(\"1983\", \"1993\", \"2003\", \"2013\", '2023') \n  ) +\n  scale_y_continuous(\n    limits = c(0, 35),\n    breaks = seq(0, 35, by = 5), \n    expand = c(0, 0)\n  ) + \n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid = element_blank(),\n    panel.grid.major.x = element_line(color = \"#D3D3D3\", linetype = \"solid\", size = 0.2),\n    panel.grid.major.y = element_line(color = \"#D3D3D3\", size = 0.1),\n    axis.ticks.length.y = unit(0, \"mm\"), \n    axis.ticks.length.x = unit(2, \"mm\"),\n    axis.title = element_blank(),\n    axis.line.x.bottom = element_line(color = \"black\"),\n    axis.text.y = element_text(size = 7),\n    axis.text.x = element_text(size = 7),\n    plot.title = element_text(size = 12, face = \"bold\")\n  )\n\nplt2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#conclusion",
    "title": "Take-home Exercise 3",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\nCLAIM\n\n\n\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\n\nUpon the analysis based on the selected data\n\nTotal Volume of Monthly Rainfall: There is no strong evidence of significant differences in the total volume of monthly rainfall between the wet and dry seasons across the years.\nFrequency of Rainy Days: The frequency of rainy days shows a more pronounced contrast between the wet and dry seasons over the years in the selected data. This indicates a distinct pattern in the occurrence of rainfall throughout the year.\nIntensity of Rainfall: While the occurrence of rainfall events, indicated by blue colors, shows improvement over time, the overall intensity of rainfall, especially heavy rainfall, has displayed a decreasing trend. This trend hints at a potential shift in precipitation patterns or intensity over the years.\n\nOverall, while the total volume of monthly rainfall may not demonstrate significant differences between wet and dry seasons, the frequency of rainfall events appears to be more pronounced. However, the contrast in intensity seems to be less significant. Itâ€™s important to note that the analysis is based on data from a single station over a 5-year interval, limited to specific months. Therefore, to draw more accurate conclusions, additional data from a broader range of locations and time periods would be necessary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "theory: a summary of R for Visual Analytics - Chap 17\n\ncalender heatmap\ncycle plot\nslopegraph\n\npractice: some exploration about the dataset\n\nQucik access to Some Plotting Exercise below"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#package-and-data-import",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#package-and-data-import",
    "title": "Hands-on Exercise 6",
    "section": "Package and Data Import",
    "text": "Package and Data Import\nThe code chunk below import the necessary packages\n\n\nCode\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse, plotly)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Hands-on Exercise 6",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nData structureFeature Engineering\n\n\nkable() can be used to review the structure of the imported data frame.\n\n\nCode\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are 3 columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\ntwo new fields namely wkday and hour need to be derived to plot a calender heatmap\nthe code chunk below create a function to take into 3 varaibles ts, sc, tz, to create wkday and hour\n\n\nCode\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\nThe code chunk below called the function make_hr_wkday created above and create the 2 new columns for the dataframe.\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so theyâ€™ll be ordered when plotting\n\n\nCode\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\nTable below shows the tidy tibble table after processing.\n\n\nCode\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmaps",
    "title": "Hands-on Exercise 6",
    "section": "Building the Calendar Heatmaps",
    "text": "Building the Calendar Heatmaps\n\n\nCode\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "title": "Hands-on Exercise 6",
    "section": "Plotting Multiple Calendar Heatmaps",
    "text": "Plotting Multiple Calendar Heatmaps\nBuilding multiple heatmaps for the top four countries with the highest number of attacks.\n\nGroup by CountriesExtract RecordsPlotting\n\n\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\n\nCode\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nthe code chunk below display the top6 rows of the dataframe.\n\n\nCode\nhead(attacks_by_country)\n\n\n# A tibble: 6 Ã— 3\n  source_country     n percent  \n  &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;    \n1 CN             85243 42.62171%\n2 US             48684 24.34212%\n3 KR             12648 6.32403% \n4 NL              8572 4.28602% \n5 VN              6340 3.17002% \n6 TW              3469 1.73451% \n\n\n\n\nextract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e.Â top4_attacks).\n\n\nCode\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\n\n\nCode\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\nNote: theme_tufte() of ggthemes package is used to remove unnecessary chart junk.\n\n\nCode\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  # theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "title": "Hands-on Exercise 6",
    "section": "Plotting Slopegraph",
    "text": "Plotting Slopegraph\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\nStep 1: Data Import\nImport the rice data set into R environment by using the code chunk below.\n\n\nCode\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nStep 2: Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\n\nCode\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-import",
    "title": "Hands-on Exercise 6",
    "section": "Data Import",
    "text": "Data Import\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\n\nCode\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "title": "Hands-on Exercise 6",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nMonth ColumnComputing Monthly Average\n\n\n\n\nCode\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\nExtract Target Country\n\n\nCode\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\n\nCode\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "title": "Hands-on Exercise 6",
    "section": "Plotting the cycle plot",
    "text": "Plotting the cycle plot\n\n\nCode\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#heatmap",
    "title": "Hands-on Exercise 6",
    "section": "Heatmap",
    "text": "Heatmap\nThe code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\n\nCode\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\nData Preparation\n\nData structureFeature Engineering\n\n\nkable() can be used to review the structure of the imported data frame.\n\n\nCode\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are 3 columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\ntwo new fields namely wkday and hour need to be derived to plot a calender heatmap\nthe code chunk below create a function to take into 3 varaibles ts, sc, tz, to create wkday and hour\n\n\nCode\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\nThe code chunk below called the function make_hr_wkday created above and create the 2 new columns for the dataframe.\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so theyâ€™ll be ordered when plotting\n\n\nCode\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\nTable below shows the tidy tibble table after processing.\n\n\nCode\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\nBuilding the Calendar Heatmaps\n\n\nCode\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n\nPlotting Multiple Calendar Heatmaps\nBuilding multiple heatmaps for the top four countries with the highest number of attacks.\n\nGroup by CountriesExtract RecordsPlotting\n\n\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\n\nCode\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nthe code chunk below display the top6 rows of the dataframe.\n\n\nCode\nhead(attacks_by_country)\n\n\n# A tibble: 6 Ã— 3\n  source_country     n percent  \n  &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;    \n1 CN             85243 42.62171%\n2 US             48684 24.34212%\n3 KR             12648 6.32403% \n4 NL              8572 4.28602% \n5 VN              6340 3.17002% \n6 TW              3469 1.73451% \n\n\n\n\nextract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e.Â top4_attacks).\n\n\nCode\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\n\n\nCode\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\nNote: theme_tufte() of ggthemes package is used to remove unnecessary chart junk.\n\n\nCode\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  # theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#cycle-plot",
    "title": "Hands-on Exercise 6",
    "section": "Cycle Plot",
    "text": "Cycle Plot\ncycle plot shows the time-series patterns and trend of visitor arrivals from Vietnam\n\nData Import\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\n\nCode\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nData Preparation\n\nMonth ColumnComputing Monthly Average\n\n\n\n\nCode\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\nExtract Target Country\n\n\nCode\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\n\nCode\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n\n\n\nPlotting the cycle plot\n\n\nCode\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") \n\n\n\n\n\nCode\n  # theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#slopegraph",
    "title": "Hands-on Exercise 6",
    "section": "Slopegraph",
    "text": "Slopegraph\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\nData Import\nImport the rice data set into R environment by using the code chunk below.\n\n\nCode\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nPlotting\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\n\nCode\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = NULL)\n\n\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to my ISSS608 Visual Analytics and Applications homepage!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 7",
    "section": "",
    "text": "theory: a summary of R for Visual Analytics\n\nChoropleth Mapping\nGeospatial Point Data\nAnalytical Mapping"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started",
    "title": "Hands-on Exercise 7",
    "section": "Getting Started",
    "text": "Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\nCode\npacman::p_load(sf, tmap, tidyverse)\n\n\nNotice that, we only need to install tidyverse instead of readr, tidyr and dplyr individually.\n\nImporting Data into R\n2 data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e.Â MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.Â respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but itâ€™s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\nImport Geospatial DataImporting Attribute Data\n\n\nuse st_read() of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as mpsz\n\n\nCode\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/ljyjiayi/Library/CloudStorage/OneDrive-SingaporeManagementUniversity/term 4/608 vaa/llljyjy/VAA_ljyjiayi/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nto examine the content of mpsz, first 10 records will be displayed\n\n\nCode\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nusing read_csv() of readr and name it as popagsex.\n\n\nCode\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n\n\n\nData Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\nData wranglingCapitalizationLeft Join\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nCode\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nCode\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\nleft_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g.Â SUBZONE_N and SZ as the common identifier.\n\n\nCode\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nCode\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data-into-r",
    "title": "Hands-on Exercise 7",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n2 data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e.Â MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.Â respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but itâ€™s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\nImport Geospatial DataImporting Attribute Data\n\n\nuse st_read() of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as mpsz\n\n\nCode\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/ljyjiayi/Library/CloudStorage/OneDrive-SingaporeManagementUniversity/term 4/608 vaa/llljyjy/VAA_ljyjiayi/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nto examine the content of mpsz, first 10 records will be displayed\n\n\nCode\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nusing read_csv() of readr and name it as popagsex.\n\n\nCode\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n\n\nData Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\nData wranglingCapitalizationLeft Join\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nCode\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nCode\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\nleft_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g.Â SUBZONE_N and SZ as the common identifier.\n\n\nCode\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nCode\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 7",
    "section": "Choropleth Mapping Geospatial Data Using tmap",
    "text": "Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\nqtm()tmapâ€™s elements\n\n\n\ntmap_mode() with â€œplotâ€ option is used to produce a static map. For interactive mode, â€œviewâ€ option should be used.\nfill argument is used to map the attribute (i.e.Â DEPENDENCY)\n\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\n\nCode\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\nthe code chunk below use tmapâ€™s drawing elements to customize Choropleth Mapping\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nMore About tmap\n\nBase MapFill & BorderClassificationCustom BreakColorLegendStyleCartographics\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape(): define input data (i.e mpsz_pop2020)\ntm_polygons(): draw the planning subzone polygons\n\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\nassign target variable Dependency to tm_polygons(), showing geographical distribution of selected variable by planning subzone\n\ndefault interval binning: â€œprettyâ€ -&gt; refer to classification\ndefault colour scheme:YlOrRd of ColorBrewer. -&gt; refer to color scheme\nMissing value will be shaded in grey\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\ntm_polygons() = tm_fill() + tm_border()\n\ntm_fill(): shades polygons by using the default colour scheme\ntm_borders(): adds borders\n\nThe code chunk below draws a choropleth map using tm_fill()\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nThe code chunk below draws the borders using tm_borders()\n\nalpha = transparency, 0 (totally transparent) and 1 (not transparent), default = 1\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is â€œsolidâ€.\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  # tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\nclassification: take large number of observations and group into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nbuilt-in styles: category breaks are computed internally.\nbreakpoints: set explicitly using breaks\n\nbreaks include a minimum and maximum. n categories -&gt; n+1 elements\nthe values must be in increasing order\n\nCode chunk below compute and display the descriptive statistics of DEPENDENCY field.\n\n\nCode\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nsetting break point at 0.60, 0.70, 0.80, and 0.90. minimum at 0 and maximum at 100.\ncode chunk below plots Choropleth with breaks\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          # style = 'equal',\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 100)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntmap supports colour ramps either\n\ndefined by the user\nfrom the RColorBrewer\n\nset palette of tm_fill() as blue as shown in the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nTo reverse the colour shading, add a â€œ-â€ prefix.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nMap layout: all map elements include among title, scale bar, compass, margins and aspects ratios.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed with tmap_style()\ncode chunk below shows the classic style is used\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\ntmap provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\n\nCode\ntmap_style(\"white\")\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMultiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in 3 ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nmultiple valuestm_facets()tmap_arrange()\n\n\nsmall multiple choropleth maps are created by defining ncols in tm_fill()\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\nsmall multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\n\nCode\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\nmultiple small choropleth maps are created by using tm_facets().\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nmultiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange()\n\n\nCode\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nCode\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#reference",
    "title": "Hands-on Exercise 7",
    "section": "Reference",
    "text": "Reference\n\nAll about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\nGeospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\nData wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with â€˜spread()â€™ and â€˜gather()â€™ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#learning-outcome",
    "title": "Hands-on Exercise 7",
    "section": "Learning outcome",
    "text": "Learning outcome\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#the-data",
    "title": "Hands-on Exercise 7",
    "section": "The data",
    "text": "The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-import-and-preparation",
    "title": "Hands-on Exercise 7",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\n\nCode\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\n\nCode\nlist(sgpools) \n\n\n[[1]]\n# A tibble: 306 Ã— 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marâ€¦ 2 Bayfâ€¦    18972 30842. 29599. Branch                        5\n 2 Livewire (Resâ€¦ 26 Senâ€¦    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (Kâ€¦ Lotus â€¦   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (Pâ€¦ 1 Seleâ€¦   188306 29777. 31382. Branch                       44\n 5 Prime Serangoâ€¦ Blk 54â€¦   552542 32239. 39519. Branch                        0\n 6 Singapore Pooâ€¦ 1A Wooâ€¦   731001 21012. 46987. Branch                        3\n 7 Singapore Pooâ€¦ Blk 64â€¦   370064 33990. 34356. Branch                       17\n 8 Singapore Pooâ€¦ Blk 88â€¦   370088 33847. 33976. Branch                       16\n 9 Singapore Pooâ€¦ Blk 30â€¦   540308 33910. 41275. Branch                       21\n10 Singapore Pooâ€¦ Blk 20â€¦   560202 29246. 38943. Branch                       25\n# â„¹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 7",
    "section": "Creating a sf data frame from an aspatial data frame",
    "text": "Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other countryâ€™s epsg code by refering to epsg.io.\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\n\nCode\nlist(sgpools_sf)\n\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 Ã— 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayfâ€¦    18972 Branch                        5\n 2 Livewire (Resorts World Senâ€¦ 26 Senâ€¦    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus â€¦   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Seleâ€¦   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54â€¦   552542 Branch                        0\n 6 Singapore Pools Woodlands Câ€¦ 1A Wooâ€¦   731001 Branch                        3\n 7 Singapore Pools 64 Circuit â€¦ Blk 64â€¦   370064 Branch                       17\n 8 Singapore Pools 88 Circuit â€¦ Blk 88â€¦   370088 Branch                       16\n 9 Singapore Pools Anchorvale â€¦ Blk 30â€¦   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio â€¦ Blk 20â€¦   560202 Branch                       25\n# â„¹ 296 more rows\n# â„¹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. Itâ€™s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Hands-on Exercise 7",
    "section": "It all started with an interactive point symbol map",
    "text": "It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\n\nCode\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#lets-make-it-proportional",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#lets-make-it-proportional",
    "title": "Hands-on Exercise 7",
    "section": "Lets make it proportional",
    "text": "Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\n\nCode\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#lets-give-it-a-different-colour",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#lets-give-it-a-different-colour",
    "title": "Hands-on Exercise 7",
    "section": "Lets give it a different colour",
    "text": "Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\n\nCode\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#i-have-a-twin-brothers",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#i-have-a-twin-brothers",
    "title": "Hands-on Exercise 7",
    "section": "I have a twin brothers :)",
    "text": "I have a twin brothers :)\nAn impressive and little-know feature of tmapâ€™s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\n\nCode\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmapâ€™s Viewer back to plot mode by using the code chunk below.\n\n\nCode\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#all-about-tmap-package-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#all-about-tmap-package-1",
    "title": "Hands-on Exercise 7",
    "section": "All about tmap package",
    "text": "All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#geospatial-data-wrangling-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#geospatial-data-wrangling-2",
    "title": "Hands-on Exercise 7",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-wrangling-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-wrangling-2",
    "title": "Hands-on Exercise 7",
    "section": "Data wrangling",
    "text": "Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with â€˜spread()â€™ and â€˜gather()â€™ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview-1",
    "title": "Hands-on Exercise 7",
    "section": "Overview",
    "text": "Overview\n\nObjectives\nIn this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\nLearning outcome\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-2",
    "title": "Hands-on Exercise 7",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and loading packages\nUsing the steps you learned in previous lesson, install and load sf, tmap and tidyverse packages into R environment.\n\n\nCode\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n\nImporting data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\nUsing appropriate sf function import NGA_wp.rds into R environment.\n\n\nCode\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 7",
    "section": "Basic Choropleth Mapping",
    "text": "Basic Choropleth Mapping\n\nVisualising distribution of non-functional water point\nPlot a choropleth map showing the distribution of non-function water point by LGA\n\n\nCode\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\nCode\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\nCode\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 7",
    "section": "Choropleth Map for Rates",
    "text": "Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\nDeriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\n\nCode\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nPlotting map of rate\nPlot a choropleth map showing the distribution of percentage functional water point by LGA\n\n\nCode\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#extreme-value-maps",
    "title": "Hands-on Exercise 7",
    "section": "Extreme Value Maps",
    "text": "Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\nPercentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\nData Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nCode\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\n\nStep 2: Creating customised classification and extracting values\n\n\nCode\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\nWhy writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e.Â updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\nCreating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e.Â wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nCode\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nA percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\n\nCode\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nTest drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\n\nCode\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\nBox map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nCode\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\nCreating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\nCode\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n\nCreating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nCode\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nTest drive the newly created function\nLetâ€™s test the newly created function\n\n\nCode\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\nBoxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\n\nCode\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nCode\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 3a",
    "section": "",
    "text": "theory: a summary of R for Visual Analytics - Chap 3\npractice: some exploration about the dataset\n\nQucik access to Some Plotting Exercise below"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 3a",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nIn this exercise, ggiraph, plotly, DT, tidyverse, patchwork packages are used. A summary of the new packages introduced can be found at the section below.\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#importing-data",
    "title": "Hands-on Exercise 3a",
    "section": "Importing data",
    "text": "Importing data\nThe code chunk below is to import the data\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#ggigraph",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#ggigraph",
    "title": "Hands-on Exercise 3a",
    "section": "ggigraph",
    "text": "ggigraph\nggiraph is an htmlwidget and a ggplot2 extension which allows interaction made with ggplot geometries\ncontinuing 3 arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\nTooltip\n\nBasicDisplay Multiple InformationCustomising Tooltip styleDisplay Statistics on Tooltip\n\n\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\n\nShow the code\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\nNotice that two steps are involved.\n\nan interactive version of ggplot2 geom (i.e.Â geom_dotplot_interactive()) will be used to create the basic graph.\ngirafe() will be used to generate an svg object to be displayed on an html page.\n\nBy hovering the mouse pointer on an data point of interest, the studentâ€™s ID will be displayed.\n\n\nThe content of the tooltip can be customized by including a list object as shown in the code chunk below.\n\n\nShow the code\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\n\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\n\nShow the code\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\n\nBackground colour of the tooltip is black and the font colour is white and bold.\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\n\n\nShow the code\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)                                   \n\n\n\n\n\n\n\n\n\n\n\nHover\nElements associated with a data_id (i.e CLASS) will be highlighted upon mouse over (hovering effect).\n\nBasicwith tooltipStyling\n\n\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\nNote that the default value of the hover css is hover_css = â€œfill:orange;â€.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                                                          \n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS, tooltip = ID),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)        \n\n\n\n\n\n\n\nIn the code chunk below, css codes are used to change the highlighting effect.\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                                                                   \n\n\n\n\n\n\n\n\n\n\nCombining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                                                                                                \n\n\n\n\n\n\n\nClick\nonclick argument of ggiraph provides hotlink interactivity on the web.\n\nBasicCoordinated Multiple Views\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                                                                               \n\n\n\n\n\n\n\nCoordinated multiple views methods has been implemented in the data visualisation below.\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#plotly",
    "title": "Hands-on Exercise 3a",
    "section": "Plotly",
    "text": "Plotly\nPlotlyâ€™s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\nplot_ly()plot_ly() with colorggplotly()Plotly with Multiple Views\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)                                   \n\n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e.Â RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)                                                                                                               \n\n\n\n\n\n\n\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)                                                                                                              \n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\nThing to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk,\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#dt",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#dt",
    "title": "Hands-on Exercise 3a",
    "section": "DT",
    "text": "DT\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\nInteractive Data Table: DT packageLinked brushing: crosstalk method\n\n\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library â€˜DataTablesâ€™ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")                                                                                                        \n\n\n\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!.\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#stacked-bar-chart-of-race-distribution-by-gender",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#stacked-bar-chart-of-race-distribution-by-gender",
    "title": "Hands-on Exercise 3a",
    "section": "Stacked Bar Chart of Race Distribution by Gender",
    "text": "Stacked Bar Chart of Race Distribution by Gender\n\n\nShow the code\np &lt;- ggplot(data = exam_data, \n       aes(x = reorder(RACE, -table(RACE)[RACE]), fill = GENDER)) +\n  geom_bar(position = \"stack\",\n           alpha = 0.9) +\n  geom_text(\n    aes(label = after_stat(count)),\n    stat = \"count\",\n    position = position_stack(vjust = 0.5),\n    size = 3,\n    color = \"white\"\n  ) +\n  labs(title = \"Race Distribution by Gender\", x = \"Race\", y = \"Number of Students\") +\n  theme_minimal() \n\nggplotly(p)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#boxplot-of-english-scores-by-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#boxplot-of-english-scores-by-class",
    "title": "Hands-on Exercise 3a",
    "section": "Boxplot of English Scores by Class",
    "text": "Boxplot of English Scores by Class\n\n\nShow the code\np &lt;- ggplot(data = exam_data, \n       aes(x = CLASS, y = ENGLISH)) +\n  geom_boxplot(fill = \"#D1EEEE\", color = \"#7A8B8B\") +\n  geom_hline(yintercept = mean(exam_data$ENGLISH), linetype = \"dashed\", color = \"#CD2626\") +\n  stat_summary(\n    fun = mean, \n    geom = \"point\", \n    color = \"#CD2626\"\n  ) +\n  annotate(\n    \"text\", \n    x = 1,  y = mean(exam_data$ENGLISH) + 2,\n    label = paste(\"Avg:\", round(mean(exam_data$ENGLISH), 2)),\n    color = \"#CD2626\"\n  ) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(\n    title = \"English Scores by Class\",\n    x = \"Class\",\n    y = \"English Score\"\n  ) +\n  theme_minimal()\n\nggplotly(p)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#scatterplot-of-math-and-science-scores",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#scatterplot-of-math-and-science-scores",
    "title": "Hands-on Exercise 3a",
    "section": "Scatterplot of Math and Science Scores",
    "text": "Scatterplot of Math and Science Scores\n\n\nShow the code\np &lt;- ggplot(data = exam_data,\n       aes(x = MATHS, y = SCIENCE)) +\n  geom_point(aes(color = GENDER), size = 1.5, alpha = 0.7) +\n  geom_hline(yintercept = 50, linetype = \"dashed\", color = \"gray\") +  \n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"gray\") +  \n  geom_smooth(method = \"lm\", size = 0.5) +      \n  labs(\n    title = \"Correlation between Math and Science Scores\",\n    x = \"Math Score\",\n    y = \"Science Score\"\n  ) +\n  coord_cartesian(xlim = c(0, 100), ylim = c(0, 100)) +\n  theme_minimal()\nggplotly(p)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#density-plot-of-english-scores-by-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03a.html#density-plot-of-english-scores-by-class",
    "title": "Hands-on Exercise 3a",
    "section": "Density Plot of English Scores by Class",
    "text": "Density Plot of English Scores by Class\n\n\nShow the code\n# Density plot of ENGLISH scores combined for both genders faceted by class\np &lt;- ggplot(data = exam_data, \n       aes(x = ENGLISH, fill = GENDER)) +\n  geom_density(alpha = 0.5, color = \"black\", linewidth = 0.3) + \n  labs(title = \"Distribution of English Scores by Class\", x = \"English Score\") +\n  theme_minimal() +\n  facet_grid(CLASS ~ .) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.text = element_text(size = 8),  \n        legend.title = element_text(size = 8))\n\nggplotly(p)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class Exercise 6 - Horizon Plot",
    "section": "",
    "text": "Load the ggHoriPlot package\n\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#load-package",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#load-package",
    "title": "In-class Exercise 6 - Horizon Plot",
    "section": "",
    "text": "Load the ggHoriPlot package\n\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#import-data",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#import-data",
    "title": "In-class Exercise 6 - Horizon Plot",
    "section": "Import Data",
    "text": "Import Data\nthe csv has already been changed to long format using excel. To convert in R, can use pivot_longer function instead\n\naverp &lt;- read_csv(\"data/AVERP.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#data-preperatin",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#data-preperatin",
    "title": "In-class Exercise 6 - Horizon Plot",
    "section": "Data Preperatin",
    "text": "Data Preperatin\nDate column is imported as a character field, and it should be converted into a date format instead.\n\n averp &lt;- averp %&gt;% mutate(`Date` = dmy(`Date`))\n\nanother way is to use lubridate"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#plotting",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#plotting",
    "title": "In-class Exercise 6 - Horizon Plot",
    "section": "Plotting",
    "text": "Plotting\nThe code chunk below use geom_horizon to create a Horizon Plot, facet_grid here creates the plot for different Consumer Items\ntheme_few() : use as less number of features possible\nscale_fill_hcl(palette = 'RdBu' : here creates a diverging color using color palette, only if there is decrease/increase\n\n\nShow the code\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() + \n  scale_fill_hcl(palette = 'RdBu') + \n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "theory:\n\na summary of Visualising Distribution - Qucik access to Distribution\na summary of Visual Statistical Analysis - Qucik access to Statistical Analysis\na summary of Visualising Uncertainty - Qucik access to Visualising Uncertainty\na summary of Funnel Plots - Qucik access to Funnel Plots"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-required-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-required-libraries",
    "title": "Hands-on Exercise 4",
    "section": "Installing and loading the required libraries",
    "text": "Installing and loading the required libraries\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\n\nCode\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data",
    "title": "Hands-on Exercise 4",
    "section": "Importing data",
    "text": "Importing data\nThe code chunk below is to import the data\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ridgeline-plot",
    "title": "Hands-on Exercise 4",
    "section": "Ridgeline Plot",
    "text": "Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nWhen to use Ridgeline plot?\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\nThere are several ways to plot ridgeline plot with R. In this section, ggridges package will be used.\nggridges package provides two main geom to plot gridgeline plots:\n\ngeom_ridgeline() : takes height values directly to draw the ridgelines,\ngeom_density_ridges(): first estimates data densities and then draws those using ridgelines\n\n\nBasicGradientProbabilitiesQuantile95% CI\n\n\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nSometimes we would like to have the area under a ridgeline to fill with olors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nIt allows mapping of probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#raincloud-plot",
    "title": "Hands-on Exercise 4",
    "section": "Raincloud Plot",
    "text": "Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a â€œraincloudâ€. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, a raincloud plot to visualise the distribution of English score by race will be created by using functions provided by ggdist and ggplot2 packages step by step.\n\nalf Eye GraphBoxplotDot PlotsFinal\n\n\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = â€œleftâ€ to indicate we want it on the left-hand side.\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggstatsplot",
    "title": "Hands-on Exercise 4",
    "section": "ggstatsplot",
    "text": "ggstatsplot\n\nInstalling and loading the required libraries\n\n\nCode\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\nImporting data\nThe code chunk below is to import the data\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nSignificant Test\n\nOne-sampleTwo-sample MeanOneway ANOVACorrelationAssociation (Depedence)\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\n\nCode\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\n\nCode\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\nâ€œnsâ€ â†’ only non-significant\nâ€œsâ€ â†’ only significant\nâ€œallâ€ â†’ everything\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\n\nCode\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut(). And ggbarstats() is used to build a visual for Significant Test of Association\n\n\nCode\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\n\nUnpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThatâ€™s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this oneâ€”first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#models",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#models",
    "title": "Hands-on Exercise 4",
    "section": "Models",
    "text": "Models\nThis section aims to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\nInstalling and loading the required libraries\n\n\nCode\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n\nImporting data\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\n\nCode\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 Ã— 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA â€¦ 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA â€¦ 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA â€¦ 13750        23        10     2002  72937           210   1165\n 4     3 Â TOYOTAâ€¦ 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA â€¦ 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA â€¦ 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA â€¦ 12950        32         1     2002  61000           210   1170\n 8     7 Â TOYOTAâ€¦ 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA â€¦ 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA â€¦ 16950        27         6     2002 110404           234   1255\n# â„¹ 1,426 more rows\n# â„¹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, â€¦\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\nMultiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\n\nCode\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\nModel Diagnostic\n\nmulticolinearitynormality assumptionhomogeneity of variancesComplete check\n\n\nIn the code chunk, check_collinearity() of performance package.\n\n\nCode\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\nCode\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\nIn the code chunk, check_normality() of performance package.\n\n\nCode\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\n\nCode\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model().\n\n\nCode\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\nVisualising Regression Parameters\n\nsee methodsggcoefstats() methods\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\n\nCode\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\n\nCode\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4",
    "section": "Installing and loading the packages",
    "text": "Installing and loading the packages\nfollowing R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\n\nCode\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#data-import",
    "title": "Hands-on Exercise 4",
    "section": "Data import",
    "text": "Data import\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggplot2-methods",
    "title": "Hands-on Exercise 4",
    "section": "ggplot2 methods",
    "text": "ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nNote\n\n\n\nDonâ€™t confuse the uncertainty of a point estimate with the variation in the sample\n\n\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\n\nCode\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\n\nCode\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\nStandard Error BarsConfidence IntervalInteractive Error Bars\n\n\nplot the standard error bars of mean maths score by race as shown below.\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=â€œidentityâ€.\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\ninteractive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below\n\n\nCode\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggdist-package",
    "title": "Hands-on Exercise 4",
    "section": "ggdist package",
    "text": "ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(â€œfreq-uncertainty-visâ€));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\nDefaultMedian95 & 99 CIdisplaying distribution\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nIn the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nThe code below use .width = c(0.95, 0.99) to show 95% and 99% confidence intervals.\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    .width = c(0.95, 0.99), \n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4",
    "section": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nStep 1: Installing ungeviz package\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\nStep 2: Launch the application in R\n\n\nCode\nlibrary(ungeviz)\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 4",
    "section": "Installing and Launching R Packages",
    "text": "Installing and Launching R Packages\n4 R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\n\nCode\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data-3",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data-3",
    "title": "Hands-on Exercise 4",
    "section": "Importing Data",
    "text": "Importing Data\nCOVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e.Â kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\n\nCode\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr",
    "title": "Hands-on Exercise 4",
    "section": "FunnelPlotR",
    "text": "FunnelPlotR\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\nBasic plotChange data_type and rangeRemove Label and Update Title\n\n\nThe code chunk below plots a funnel plot.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e.Â Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is â€œSRâ€.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\nThe code chunk below plots a funnel plot.\n\ndata_type argument is used to change from default â€œSRâ€ to â€œPRâ€ (i.e.Â proportions).\nxrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\nThe code chunk below plots a funnel plot.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.\n\n\n\nCode\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggplot2-methods-1",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#ggplot2-methods-1",
    "title": "Hands-on Exercise 4",
    "section": "ggplot2 methods",
    "text": "ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\nComputing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\n\nCode\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\n\nNext, the fit.mean is computed by using the code chunk below.\n\n\nCode\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nCalculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\n\nCode\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nPlotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\n\nCode\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\nInteractive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\n\nCode\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#fill-border-color",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#fill-border-color",
    "title": "Hands-on Exercise 7",
    "section": "Fill & Border Color",
    "text": "Fill & Border Color\ntm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  # tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is â€œsolidâ€.\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth",
    "title": "Hands-on Exercise 7",
    "section": "choropleth",
    "text": "choropleth\n\n\n\n\n\n\n\n\n\nqtm()\ntmap\n\n\n\n\nadvantages\neasiest and quickest\nhigh quality cartographic choropleth map\n\n\ndisadvantages\naesthetics of individual layers harder to control\n\n\n\narguments\nqtm(data, fill)\ntm_shape(input)\ntm_polygons(variable) = tm_fill() + tm_border()\nstyle = classification (build in category style) v.s. breaks = min, break, max\npalette = color schema, reverse colour with â€œ-â€ prefix\ntm_layout(title, legend)\ntmap_style(â€œclassicâ€)\ntm_compass(type=â€œ8starâ€)\ntm_scale_bar()\ntm_grid()\n\n\nfacets\n\ntm_fill(c(â€œYOUNGâ€, â€œAGEDâ€)\ntm_facets()\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\npoint\n\ntm_bubbles()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "title": "Hands-on Exercise 7",
    "section": "Getting Started",
    "text": "Getting Started\nCode below ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\n\nCode\npacman::p_load(sf, tmap, tidyverse)\n\n\n\nThe data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\n\nData Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\n\nCode\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\n\nCode\nlist(sgpools) \n\n\n[[1]]\n# A tibble: 306 Ã— 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marâ€¦ 2 Bayfâ€¦    18972 30842. 29599. Branch                        5\n 2 Livewire (Resâ€¦ 26 Senâ€¦    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (Kâ€¦ Lotus â€¦   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (Pâ€¦ 1 Seleâ€¦   188306 29777. 31382. Branch                       44\n 5 Prime Serangoâ€¦ Blk 54â€¦   552542 32239. 39519. Branch                        0\n 6 Singapore Pooâ€¦ 1A Wooâ€¦   731001 21012. 46987. Branch                        3\n 7 Singapore Pooâ€¦ Blk 64â€¦   370064 33990. 34356. Branch                       17\n 8 Singapore Pooâ€¦ Blk 88â€¦   370088 33847. 33976. Branch                       16\n 9 Singapore Pooâ€¦ Blk 30â€¦   540308 33910. 41275. Branch                       21\n10 Singapore Pooâ€¦ Blk 20â€¦   560202 29246. 38943. Branch                       25\n# â„¹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame.\n\n\nCreating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other countryâ€™s epsg code by refering to epsg.io.\n\n\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\n\nCode\nlist(sgpools_sf)\n\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 Ã— 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayfâ€¦    18972 Branch                        5\n 2 Livewire (Resorts World Senâ€¦ 26 Senâ€¦    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus â€¦   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Seleâ€¦   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54â€¦   552542 Branch                        0\n 6 Singapore Pools Woodlands Câ€¦ 1A Wooâ€¦   731001 Branch                        3\n 7 Singapore Pools 64 Circuit â€¦ Blk 64â€¦   370064 Branch                       17\n 8 Singapore Pools 88 Circuit â€¦ Blk 88â€¦   370088 Branch                       16\n 9 Singapore Pools Anchorvale â€¦ Blk 30â€¦   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio â€¦ Blk 20â€¦   560202 Branch                       25\n# â„¹ 296 more rows\n# â„¹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. Itâ€™s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 7",
    "section": "Drawing Proportional Symbol Map",
    "text": "Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\n\nCode\ntmap_mode(\"view\")\n\n\n\nPoint MapProportionalColor\n\n\nThe code chunks below are used to create an interactive point symbol map.\n\n\nCode\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\n\nCode\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\nproportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable\n\n\nCode\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#multiple-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#multiple-maps",
    "title": "Hands-on Exercise 7",
    "section": "Multiple Maps",
    "text": "Multiple Maps\nAn impressive and little-know feature of tmapâ€™s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\n\nCode\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmapâ€™s Viewer back to plot mode by using the code chunk below.\n\n\nCode\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#reference-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#reference-1",
    "title": "Hands-on Exercise 7",
    "section": "Reference",
    "text": "Reference\n\nAll about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\nGeospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\nData wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with â€˜spread()â€™ and â€˜gather()â€™ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "title": "Hands-on Exercise 7",
    "section": "Overview",
    "text": "Overview\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "R shiny prototype\nhttps://ljyjiayili.shinyapps.io/prototype1/"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-1",
    "title": "Hands-on Exercise 7",
    "section": "choropleth",
    "text": "choropleth"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#set-mode",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#set-mode",
    "title": "Hands-on Exercise 7",
    "section": "Set Mode",
    "text": "Set Mode\n\n\n\ntmap_mode()\nplot\nview\n\n\n\n\ntype\nstatic\ninteractive"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#point-plot",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#point-plot",
    "title": "Hands-on Exercise 7",
    "section": "Point Plot",
    "text": "Point Plot\ntm_bubbles(col = â€œredâ€, size = 1, border.col = â€œblackâ€, border.lwd = 1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "This project have the following the component:\n\nUni-Vraiant & Time series Analysis\nMulti-Variant Analysis\nGeospatial Analysis\nModeling\n\nPanel Regression\nNetwork Analysis\nTime Series Clustering\n\n\nThis take home exercise will cover on Multi-Variant Analysis, and Time Series Clustering."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#loading-r-packages",
    "title": "Take-home Exercise 4",
    "section": "Loading R packages",
    "text": "Loading R packages\n\npacman::p_load(ggiraph, plotly, tidyverse,\n               corrplot, ggstatsplot,ggraph, igraph, \n               heatmaply, treemap, dplyr,GGally)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#importing-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#importing-data",
    "title": "Take-home Exercise 4",
    "section": "Importing Data",
    "text": "Importing Data\nThe code below uses the read_csv() function from the readr package to import bmi_data.csv and country_data.csv into the environment.\n\nbmi &lt;- read_csv(\"data/bmi_data.csv\")\ncountry &lt;- read_csv(\"data/country_data.csv\")\n\nTwo datasets are used: bmi_data contains the target variable, Big Mac Index, and a list of economic indicators from 28 countries and regions for the years 2002 to 2022. This data has already been cleaned and manipulated for analysis. country_data contains geographical and economic category groups for the 28 countries and regions involved, such as EU, continent, and G20 etc.\n\nBMIRegionCombined\n\n\n\nhead(bmi)\n\n# A tibble: 6 Ã— 15\n  country  year currency_code bmi_localprice bmi_usd_price bmi_change export_usd\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Argentâ€¦  2002 ARS                     2.5          0.799       0      9838651.\n2 Argentâ€¦  2003 ARS                     4.1          1.42       64     16066847.\n3 Argentâ€¦  2004 ARS                     4.36         1.48        6.34  25007443.\n4 Argentâ€¦  2005 ARS                     4.75         1.64        8.94  30870438.\n5 Argentâ€¦  2006 ARS                     4.75         1.55        0     37603723.\n6 Argentâ€¦  2007 ARS                     8.25         2.67       73.7   49020947.\n# â„¹ 8 more variables: import_usd &lt;dbl&gt;, net_export &lt;dbl&gt;, GDP &lt;dbl&gt;,\n#   gdp_per_capita &lt;dbl&gt;, inflation &lt;dbl&gt;, unemployment &lt;dbl&gt;, hdi &lt;dbl&gt;,\n#   population &lt;dbl&gt;\n\n\n\n\n\nhead(country)\n\n# A tibble: 6 Ã— 6\n  country   continent     g7    g20   eu    brics\n  &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 Argentina South America FALSE TRUE  FALSE FALSE\n2 Australia Oceania       FALSE TRUE  FALSE FALSE\n3 Brazil    South America FALSE TRUE  FALSE TRUE \n4 Canada    North America TRUE  TRUE  FALSE FALSE\n5 Chile     South America FALSE FALSE FALSE FALSE\n6 China     Asia          FALSE TRUE  FALSE TRUE \n\n\n\n\nLeft join country with bmi data by country name, and take a preview of the data\n\nbmi_all &lt;- left_join(bmi, country, by = \"country\")\nhead(bmi_all)\n\n# A tibble: 6 Ã— 20\n  country  year currency_code bmi_localprice bmi_usd_price bmi_change export_usd\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Argentâ€¦  2002 ARS                     2.5          0.799       0      9838651.\n2 Argentâ€¦  2003 ARS                     4.1          1.42       64     16066847.\n3 Argentâ€¦  2004 ARS                     4.36         1.48        6.34  25007443.\n4 Argentâ€¦  2005 ARS                     4.75         1.64        8.94  30870438.\n5 Argentâ€¦  2006 ARS                     4.75         1.55        0     37603723.\n6 Argentâ€¦  2007 ARS                     8.25         2.67       73.7   49020947.\n# â„¹ 13 more variables: import_usd &lt;dbl&gt;, net_export &lt;dbl&gt;, GDP &lt;dbl&gt;,\n#   gdp_per_capita &lt;dbl&gt;, inflation &lt;dbl&gt;, unemployment &lt;dbl&gt;, hdi &lt;dbl&gt;,\n#   population &lt;dbl&gt;, continent &lt;chr&gt;, g7 &lt;lgl&gt;, g20 &lt;lgl&gt;, eu &lt;lgl&gt;,\n#   brics &lt;lgl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "theory: a summary of R for Visual Analytics\n\nModelling, Visualising and Analysing Network Data with R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview",
    "title": "Hands-on Exercise 8",
    "section": "Overview",
    "text": "Overview\nBy the end of this hands-on exercise, you will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "title": "Hands-on Exercise 8",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and launching R packages\nFour network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\n\nCode\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "title": "Hands-on Exercise 8",
    "section": "The Data",
    "text": "The Data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\nThe edges dataThe nodes data\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n\nImporting network data from files\nThe code below use read_csv() of readr package to import GAStech_email_node.csv and GAStech_email_edges-v2.csv into the environment\n\n\nCode\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\nData Wrangling\n\nglimpse()Aggregation\n\n\nexamine the structure of the data frame using glimpse() of dplyr.\n\n\nCode\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26â€¦\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29â€¦\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"â€¦\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0â€¦\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorPâ€¦\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work relaâ€¦\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herrâ€¦\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Cocâ€¦\n\n\n\n\n\n\n\n\nNote\n\n\n\nSentDate in GAStech_edges is â€œCharacterâ€ data type instead of date data type, and should be changed to â€œDateâ€â€ data type\n\n\nThe code chunk below convert GAStech_edges to date datatype\n\n\nCode\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e.Â Monday. The function will create a new column in the data.frame i.e.Â Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation. In view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\n\nCode\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 8",
    "section": "Creating network objects using tidygraph",
    "text": "Creating network objects using tidygraph\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\nThe tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\nThe dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\nUsing tbl_graph() to build tidygraph data model.\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraphâ€™s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\n\nCode\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\nReviewing the output tidygraphâ€™s graph object\n\n\nCode\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 Ã— 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manaâ€¦\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# â„¹ 44 more rows\n#\n# Edge Data: 1,372 Ã— 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# â„¹ 1,369 more rows\n\n\n\n\nReviewing the output tidygraphâ€™s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of â€œNode Dataâ€ and the first three of â€œEdge Dataâ€.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\nChanging the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest â€œweightâ€ first, we could use activate() and then arrange().\nFor example,\n\n\nCode\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 Ã— 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# â„¹ 1,362 more rows\n#\n# Node Data: 54 Ã— 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# â„¹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 8",
    "section": "Plotting Static Network Graphs with ggraph package",
    "text": "Plotting Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraphâ€™s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\nPlotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\n\nCode\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\nChanging the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read itâ€™s reference guide at least once.\n\n\nCode\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above:\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\nChanging the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\n\nCode\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nWorking with ggraphâ€™s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\nFruchterman and Reingold layout\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nCode\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\nModifying network nodes\nIn this section, you will colour each node by referring to their respective departments.\n\n\nCode\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\nModifying edges\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\n\nCode\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "title": "Hands-on Exercise 8",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nWorking with facet_edges()\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read itâ€™s reference guide at least once.\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nWorking with facet_edges()\nThe code chunk below uses theme() to change the position of the legend.\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nA framed facet graph\nThe code chunk below adds frame to each graph.\n\n\nCode\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nWorking with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read itâ€™s reference guide at least once.\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "title": "Hands-on Exercise 8",
    "section": "Network Metrics Analysis",
    "text": "Network Metrics Analysis\n\nComputing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A Userâ€™s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\n\nCode\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\nVisualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nCode\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\nVisualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\n\nCode\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 8",
    "section": "Building Interactive Network Graph with visNetwork",
    "text": "Building Interactive Network Graph with visNetwork\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an â€œidâ€ column, and the edge list must have â€œfromâ€ and â€œtoâ€ columns.\nThe function also plots the labels for the nodes, using the names of the actors from the â€œlabelâ€ column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nData preparation\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nCode\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nPlotting the first interactive network graph\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nCode\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nWorking with layout\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayoutâ€™s argument.\n\n\nWorking with visual attributes - Nodes\nvisNetwork() looks for a field called â€œgroupâ€ in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nCode\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nWorking with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisit Option to find out more about visEdgesâ€™s argument.\n\n\nInteractivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nCode\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisit Option to find out more about visOptionâ€™s argument."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#model-calibration",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#model-calibration",
    "title": "Take-home Exercise 4",
    "section": "Model Calibration",
    "text": "Model Calibration\nKey considerations for calibrating the time series clustering model include:\n\nInput Selection: Identifying which columns or metrics to incorporate into the clustering analysis.\nClustering Algorithm: Selecting the appropriate clustering method. Common options include â€œpartitionalâ€ (e.g., k-means) and â€œhierarchicalâ€ clustering, each suitable for different analysis scenarios.\nSeed Value: Setting an initial seed (defaulted to 2024) to ensure reproducibility of results. Users can modify this seed based on their needs.\nNumber of Clusters (k): Determining the optimal number of clusters to best categorize the data, which can significantly impact the analysis outcome.\n\n\nData Preparation\nBased on the documentation, the data input should be: a matrix or data frame where each row is a time series, or a list where each element is a time series. Multivariate series should be provided as a list of matrices where time spans the rows and the variables span the columns of each matrix. Since we have multiple variables, we should firstly convert the dataframe.\nThe code below firstly drops all missing variables, and then convert it into a multi variant time series matrics.\n\n# drop missing values\nbmi_clean &lt;- bmi %&gt;%\n  filter(complete.cases(.))\n\n# 1. Select relevant columns\nbmi_filtered &lt;- select(bmi_clean, -currency_code)\n\n# 2. Convert to a wide format, prepare for conversion to list of matrices\nbmi_wide &lt;- bmi_filtered %&gt;%\n  pivot_longer(cols = -c(country, year), names_to = \"variable\", values_to = \"value\") %&gt;%\n  pivot_wider(names_from = variable, values_from = value, names_sort = TRUE) %&gt;%\n  arrange(country, year)\n\n\n# Group by country and convert each group to a matrix\nlist_matrices_per_country &lt;- bmi_wide %&gt;%\n  group_by(country) %&gt;%\n  group_split() %&gt;%\n  lapply(function(df) {\n    # Ensure year is not included in the matrix\n    df &lt;- select(df, -country, -year)\n    as.matrix(df)\n  })\n\n\n\nPerform Time Series Clustering\n\npartitional\n\nModel BuildingModel Evaluation with Scatter PlotModel Evulation with silhouette score\n\n\nthe code below build a cluster model with k means clustering method\n\nclustering_result &lt;- tsclust(list_matrices_per_country, type = \"partitional\", k = 4, distance = \"dtw\")\n\n\n\nassign the country with the predicted cluster\n\n\nCode\ncluster_assignments &lt;- clustering_result@cluster\n\ncountry_names &lt;- bmi_wide$country %&gt;% unique()\n\nif(length(country_names) == length(cluster_assignments)) {\n  country_cluster_df &lt;- data.frame(country = country_names, cluster = cluster_assignments)\n} else {\n  stop(\"Mismatch between the number of countries and the number of cluster assignments.\")\n}\n\n\nconduct dimensional reduction\n\n\nCode\n# Assuming list_matrices_per_country is your list of matrices\nfeatures &lt;- lapply(list_matrices_per_country, function(matrix) {\n  c(\n    mean = mean(matrix, na.rm = TRUE),\n    sd = sd(matrix, na.rm = TRUE),\n    min = min(matrix, na.rm = TRUE),\n    max = max(matrix, na.rm = TRUE)\n    # Add other features as needed\n  )\n})\n\n# Convert the list of features to a data frame\nfeature_df &lt;- do.call(rbind, features)\n\n# Ensure no missing values\nfeature_df &lt;- na.omit(feature_df)\n\n# Perform PCA\npca_result &lt;- prcomp(feature_df, center = TRUE, scale. = TRUE)\n\n# Extracting the first two principal components\npc1 &lt;- pca_result$x[, \"PC1\"]\npc2 &lt;- pca_result$x[, \"PC2\"]\n\n# Assuming country_names is your vector of country names\npca_df &lt;- data.frame(country = country_names, PC1 = pc1, PC2 = pc2)\n\n\nvisualize the model performance with a scatter plot\n\n\nCode\nvisualization_df &lt;- merge(pca_df, country_cluster_df, by = \"country\")\nplot_ly(data = visualization_df, \n             x = ~PC1, y = ~PC2, \n             type = 'scatter', mode = 'markers',\n             text = ~country, # Show country name on hover\n             hoverinfo = 'text',\n             color = ~factor(cluster), colors = RColorBrewer::brewer.pal(8, \"Set1\"),\n             marker = list(size = 10))\n\n\n\n\n\n\n\n\ncompute mean silhouette score to evaluate the model performance\n\n\nCode\ndist_matrix &lt;- dist(list_matrices_per_country, method = \"DTW\")\n\ncluster_assignments &lt;- clustering_result@cluster\n\nsil_scores &lt;- silhouette(cluster_assignments, dist_matrix)\n\nmean_sil_width &lt;- mean(sil_scores[, \"sil_width\"])\n\nprint(mean_sil_width)\n\n\n[1] 0.4993436\n\n\n\n\n\n\n\nhierarchical\n\nModel BuilidingVisulization with DendrogramModel Evulation with silhouette score\n\n\nthe code below build a cluster model with k means clustering method\n\n\nCode\nclustering_result &lt;- tsclust(list_matrices_per_country, type = \"hierarchical\", k = 4, distance = \"dtw\")\n\n\n\n\n\n\nCode\nplot(clustering_result)\n\n\n\n\n\n\n\nassign the country with the predicted cluster\n\n\nCode\ncluster_assignments &lt;- clustering_result@cluster\n\ncountry_names &lt;- bmi_wide$country %&gt;% unique()\n\nif(length(country_names) == length(cluster_assignments)) {\n  country_cluster_df &lt;- data.frame(country = country_names, cluster = cluster_assignments)\n} else {\n  stop(\"Mismatch between the number of countries and the number of cluster assignments.\")\n}\n\n\ncompute mean silhouette score to evaluate the model performance\n\n\nCode\ndist_matrix &lt;- dist(list_matrices_per_country, method = \"DTW\")\n\ncluster_assignments &lt;- clustering_result@cluster\n\nsil_scores &lt;- silhouette(cluster_assignments, dist_matrix)\n\nmean_sil_width &lt;- mean(sil_scores[, \"sil_width\"])\n\nprint(mean_sil_width)\n\n\n[1] 0.7815754\n\n\n\n\n\n\n\nSummary\nDuring the model training, there are some adjustable parameters:\n\ninput:\n\nvariables used for clustering\ntimeframe for analysis\n\nmodel calibration:\n\ntype of clustering method to use: \"partitional\", \"hierarchical\"\nk: Number of desired clusters\n\n\n\n\n\nClstering Results Visulization\ntaking the results of k means with cluster 4\n\n\nCode\nclustering_result &lt;- tsclust(list_matrices_per_country, type = \"partitional\", k = 4, distance = \"dtw\")\ncluster_assignments &lt;- clustering_result@cluster\n\ncountry_names &lt;- bmi_wide$country %&gt;% unique()\n\nif(length(country_names) == length(cluster_assignments)) {\n  country_cluster_df &lt;- data.frame(country = country_names, cluster = cluster_assignments)\n} else {\n  stop(\"Mismatch between the number of countries and the number of cluster assignments.\")\n}\n\n\nThe code below display the country and cluster in a dataframe. But it may not be intuitive enough.\n\n\nCode\ncountry_cluster_df\n\n\n          country cluster\n1       Argentina       1\n2       Australia       3\n3          Brazil       3\n4          Canada       3\n5           Chile       1\n6           China       4\n7      Czech Rep.       1\n8         Denmark       1\n9       Hong Kong       1\n10        Hungary       1\n11      Indonesia       3\n12          Japan       2\n13          Korea       3\n14       Malaysia       1\n15         Mexico       3\n16    New Zealand       1\n17           Peru       1\n18    Philippines       1\n19         Poland       1\n20         Russia       3\n21      Singapore       1\n22   South Africa       1\n23         Sweden       1\n24    Switzerland       1\n25       Thailand       1\n26         Turkey       1\n27 United Kingdom       3\n28  United States       4\n\n\nPLotting the relationship with a Dendrogram allows a more intuitive visualization in the cluster assignment.\n\n\nCode\nroot_node &lt;- data.frame(cluster = unique(country_cluster_df$cluster))\n\nedges_cluster_country &lt;- country_cluster_df %&gt;%\n  select(cluster, country) %&gt;%\n  rename(from = cluster, to = country)\n\nedges_root_cluster &lt;- data.frame(from = \"\", to = root_node$cluster)\n\nedge_list &lt;- rbind(edges_root_cluster, edges_cluster_country)\n\nmygraph &lt;- graph_from_data_frame(edge_list)\n\n# Plot\nggraph(mygraph, layout = 'dendrogram', circular = FALSE) + \n  geom_edge_diagonal() +\n  geom_node_point(color=\"#ffcc00\", size=3) +\n  geom_node_text(aes(label=name), hjust=\"inward\", nudge_y=0.5) +\n  theme_void() +\n  coord_flip() +\n  scale_y_reverse()\n\n\n\n\n\nAdditionally, it is important to understand the number of countries in each cluster.\n\n\nCode\ncluster_summary &lt;- country_cluster_df %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(Count = n())\n\nggplot(cluster_summary, aes(x = cluster, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"#ffcc00\", color = \"#ffcc00\") +\n  geom_text(aes(label = Count), vjust = -0.5, color = \"black\") + \n  theme_minimal() +\n  labs(title = \"Number of Countries per Cluster\",\n       x = \"Cluster\",\n       y = \"Number of Countries\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-of-model",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-of-model",
    "title": "Take-home Exercise 4",
    "section": "Visualization of Model",
    "text": "Visualization of Model\nTo better understand and visualize the clustering results, consider plotting the Big Mac Index prices in USD for each country, colored by their cluster assignment. This step can provide insights into the similarities within each cluster.\n\nClusters Back to Countries\nAfter obtaining the cluster assignments, map these back to the countries to identify which countries have been grouped together.\nadjustable parameters:\n\ninput:\n\ntarget variables for clustering\ntimeframe for analysis\n\nmodel calibration:\n\ntype of clustering method to use: \"partitional\", \"hierarchical\"\nk: Number of desired clusters. It can be a numeric vector with different values\n\n\nhttps://rdrr.io/cran/dtwclust/man/tsclust.html\nhttps://www.rdocumentation.org/packages/dtwclust/versions/5.5.12\nhttps://cran.r-project.org/web/packages/dtwclust/dtwclust.pdf\ngeo spatial analysis to visualize the location of such countries by cluster on the world map directly, to understand its geospatial location, if there is any pattern\nto find out more about geo-spatial portion, please refer to our team mateâ€™s work xxxx\ncan we predict the behaviour of countries without big mac data ?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects",
    "title": "Hands-on Exercise 8",
    "section": "Creating network objects",
    "text": "Creating network objects\nWhile network data itself is not tidy, it can be envisioned as two tidy tables,\n\none for node data and\none for edge data.\n\ntidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\n\nThe tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\nThe dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\nUsing tbl_graph() to build tidygraph data model\nThe code bleow use tbl_graph() of tinygraph package to build an tidygraphâ€™s network graph data.frame.\n\n\nCode\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\nReviewing the output tidygraphâ€™s graph object\n\n\nCode\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 Ã— 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manaâ€¦\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# â„¹ 44 more rows\n#\n# Edge Data: 1,372 Ã— 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# â„¹ 1,369 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of â€œNode Dataâ€ and the first three of â€œEdge Dataâ€.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\n\nChanging the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest â€œweightâ€ first, we could use activate() and then arrange().\nFor example,\n\n\nCode\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 Ã— 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# â„¹ 1,362 more rows\n#\n# Node Data: 54 Ã— 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# â„¹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#facet-graphs",
    "title": "Hands-on Exercise 8",
    "section": "Facet graphs",
    "text": "Facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nWorking with facet_edges()\n\nBasicChange Theme\n\n\nIn the code chunk below, facet_edges() is used\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\nFramed Facet Graph\nThe code chunk below adds frame to each graph.\n\n\nCode\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\nWorking with facet_nodes()\n\nBasicFramed\n\n\nIn the code chunkc below, facet_nodes() is used.\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)\n\n\n\n\n\n\n\n\n\nCode\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#framed-facet-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#framed-facet-graph",
    "title": "Hands-on Exercise 8",
    "section": "Framed Facet Graph",
    "text": "Framed Facet Graph\nThe code chunk below adds frame to each graph.\n\n\nCode\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-preperation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-preperation",
    "title": "Hands-on Exercise 8",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nData Required: edge data + node data\nAggregation by node\nnetwork object\n\n\ncreate network object: tbl_graph(nodes, edges, directed = TRUE)\nchange activation : activate(edges)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#static-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#static-graph",
    "title": "Hands-on Exercise 8",
    "section": "Static Graph",
    "text": "Static Graph\n\n\n\npackage\nOperation\nCode\narguments\n\n\n\n\nggraph\nBasic\nggraph() + geom_edge_link() + geom_node_point()\n\n\n\n\nChange Theme\n+ theme_graph()\n\n\n\n\nChange color\n\ngeom_edge_link(aes(colour = â€˜grey50â€™))\ntheme_graph(background = â€˜grey10â€™, text_colour = â€˜whiteâ€™)\n\n\ngeom_edge_link & geom_node_point : aes(color = )\nthem_graph: background, text_color\n\n\n\n\nChange Layout\nggraph(GAStech_graph, layout = â€œfrâ€)\n\nggraph: layout\nstar, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl\n\n\n\n\nModify Node\ngeom_node_point(aes(colour = Department, size = 3))\n\ngeom_node_point()\nalpha, colour, fill, shape, size, stroke, filter\n\n\n\n\nModify Edge\ngeom_edge_link(aes(width=Weight), alpha=0.2) + scale_edge_width(range = c(0.1, 5))\n\ngeom_edge_link()\nedge_colour, edge_width, edge_linetype, edge_alpha, filter\n\n\n\n\nFacet graphs\ng &lt;- Basic plot\ng + facet_edges() (~Weekday)\nfacet_edges()\n\n\n\nFacet graphs Change Theme\ng &lt;- Basic plot + theme(legend.position = â€˜bottomâ€™)\ng + facet_edges() (~Weekday)\ntheme()\n\n\n\nFramed Facet Graphs\ng + facet_nodes(~Department)+ th_foreground(foreground = â€œgrey80â€, border = TRUE)\nth_foreground ()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#interactive-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#interactive-network-graph",
    "title": "Hands-on Exercise 8",
    "section": "Interactive Network Graph",
    "text": "Interactive Network Graph\n\n\n\nPackage\nOperation\nCode\nArgument\n\n\n\n\nvisNetwork()\nBasic\nvisNetwork(node, edge)\n\n\n\n\nChange layout\nvisNetwork() %&gt;% visIgraphLayout(layout = â€œlayout_with_frâ€)\n\nvisIgraphLayout(layout)\nlayout: â€œlayout_with_frâ€\n\n\n\n\nColor by Node\nnodes %&gt;% rename(group)\nvisNetwork(node, edge)\n\n\n\n\nEdge\nvisNetwork(node, edge) + visEdges(arrows = â€œtoâ€, smooth = list(enabled = TRUE, type = â€œcurvedCWâ€))\n\nvisEdges()\narrows, smooth, type"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC â€˜http://mrcc.com/qgis.dtdâ€™ â€˜SYSTEMâ€™&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Exercise 7 - Geospatial",
    "section": "",
    "text": "In Class Exercise - Tableau: Geo-spatial Analysis on SG property market\niso plot in R -WIP"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#load-package",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#load-package",
    "title": "In-class Exercise 7 - Geospatial",
    "section": "Load Package",
    "text": "Load Package\n\npacman::p_load(sf, terra, gstat, tmap,\n               viridis, tidyverse)\n\nterra: handle raster data\ngstat: geo statistics method, for sampling data, spatial interpolation"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#import-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#import-data",
    "title": "In-class Exercise 7 - Geospatial",
    "section": "Import Data",
    "text": "Import Data\nthe code chunk below import the rainfall station data\n\nrfstations &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\nthe code chunk below import the rainfall data and get the aggregated by month data. It is important to look at monthly rainfall here\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;% \n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarize(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\nselect: to only filter rainfall data\ngroup_by: group by to get aggregated data by stations\nsummarize: aggregation\nungroup(): need to ungroup\n\n\n\nthe code chunk below join the 2 table togther, adding into longitude and latitude into the rainfall table\n\nrfdata &lt;- rfdata %&gt;%\n  left_join(rfstations)\n\n\n\n\n\n\n\nNote\n\n\n\n\nuse left_join here as many stations donâ€™t have data\nby default need to use by = join_by(Station), it is not used here as both tables have the same column\ncheck if there is any null values after the joining\n\n\n\nthe code chunk below use st_as_sf to convert it into spatial data\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\n\ncoords = c(â€œLongitudeâ€, â€œLatitudeâ€), need to go Longitude first then Latitude\ncrs here is used to do the projection\noutput contains a geometry column, POINT(Longitude, Latitude) after st_as_sf()\nst_transform convert all data from decimal to meter, the number would be much larger\n\n\n\nthe code chunk below import the geo-spatial shape data\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/ljyjiayi/Library/CloudStorage/OneDrive-SingaporeManagementUniversity/term 4/608 vaa/llljyjy/VAA_ljyjiayi/In-class_Ex/In-class_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nNote:\nuse st_read to do the import as it is a shape file\nmultipolygon instead of a POINT\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\nNote\n\n\n\n\ncheck.and.fix = TRUE is used to fix the geometric error, it wont change the data, didnt fix the error\ntmap_mode(â€œviewâ€) used to make the map interactive version\nInstead of using tm_fill or tm_polygons, only tm_borders() is used, so that only borders will be shown"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-import-data-preparation",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#data-import-data-preparation",
    "title": "In-class Exercise 7 - Geospatial",
    "section": "Data Import & Data Preparation",
    "text": "Data Import & Data Preparation\nthe code chunk below import the rainfall station data\n\nrfstations &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\nthe code chunk below import the rainfall data and get the aggregated by month data. It is important to look at monthly rainfall here\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;% \n  select(c(1,5)) %&gt;%\n  group_by(Station) %&gt;%\n  summarize(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\nselect: to only filter rainfall data\ngroup_by: group by to get aggregated data by stations\nsummarize: aggregation\nungroup(): need to ungroup\n\n\n\nthe code chunk below join the 2 table togther, adding into longitude and latitude into the rainfall table\n\nrfdata &lt;- rfdata %&gt;%\n  left_join(rfstations)\n\n\n\n\n\n\n\nNote\n\n\n\n\nuse left_join here as many stations donâ€™t have data\nby default need to use by = join_by(Station), it is not used here as both tables have the same column\ncheck if there is any null values after the joining\n\n\n\nthe code chunk below use st_as_sf to convert it into spatial data\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\nNote\n\n\n\n\ncoords = c(â€œLongitudeâ€, â€œLatitudeâ€), need to go Longitude first then Latitude\ncrs here is used to do the projection\noutput contains a geometry column, POINT(Longitude, Latitude) after st_as_sf()\nst_transform convert all data from decimal to meter, the number would be much larger\n\n\n\nthe code chunk below import the geo-spatial shape data\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/ljyjiayi/Library/CloudStorage/OneDrive-SingaporeManagementUniversity/term 4/608 vaa/llljyjy/VAA_ljyjiayi/In-class_Ex/In-class_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nuse st_read to do the import as it is a shape file\nmultipolygon instead of a POINT"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#ploting-point-plot",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#ploting-point-plot",
    "title": "In-class Exercise 7 - Geospatial",
    "section": "Ploting Point Plot",
    "text": "Ploting Point Plot\nThe code chunk plot a geo-spatial plot\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\ntm_shape(mpsz2019) +\n  tm_borders() +\ntm_shape(rfdata_sf) +\n  tm_dots(col = 'MONTHSUM')\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\nNote\n\n\n\n\ncheck.and.fix = TRUE is used to fix the geometric error, it wont change the data, didnt fix the error\ntmap_mode(â€œviewâ€) used to make the map interactive version\nInstead of using tm_fill or tm_polygons, only tm_borders() is used, so that only borders will be shown\ntm_shape(rfdata_sf) is a simple feature layer, and plotted as dot, the color is based on MONTHSUM - monthly total rainfall, but default, only 4 class\n\n\n\nThe plot only indicate the rainfall based on the rainfall station location, instead, showing the rainfall based on the region itself would make more sense."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#plot-iso-graph",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#plot-iso-graph",
    "title": "In-class Exercise 7 - Geospatial",
    "section": "Plot iso Graph",
    "text": "Plot iso Graph\nThe code below create a grid\n\ngrid &lt;- terra::rast(mpsz2019,\n                    nrow = 690,\n                    ncols = 1075)\n\nxy &lt;- terra::xyFromCell(grid,\n                        1:ncell(grid))\n\n\n\n\n\n\n\nNote\n\n\n\nonly resolution of grid should be decided by the users, the calculation and conversion should be done offline\n\n\nthe code below\n\n### \n\n\n###\n\n\n\n\n\n\n\nNote\n\n\n\n\nnmax, is the max neighbors, then able to generate the raster layer\nthe plot is controlled by the nmax\n\n\n\nanother way is to do variogram"
  },
  {
    "objectID": "In-class_Ex/Hands-on_Ex08.html",
    "href": "In-class_Ex/Hands-on_Ex08.html",
    "title": "testing",
    "section": "",
    "text": "this is a testing"
  },
  {
    "objectID": "In-class_Ex/Hands-on_Ex08.html#section",
    "href": "In-class_Ex/Hands-on_Ex08.html#section",
    "title": "testing",
    "section": "",
    "text": "please note this is a testing"
  },
  {
    "objectID": "In-class_Ex/Hands-on_Ex08.html#level-2",
    "href": "In-class_Ex/Hands-on_Ex08.html#level-2",
    "title": "testing",
    "section": "",
    "text": "this is a testing"
  },
  {
    "objectID": "In-class_Ex/Hands-on_Ex08.html#level-2-1",
    "href": "In-class_Ex/Hands-on_Ex08.html#level-2-1",
    "title": "testing",
    "section": "level 2",
    "text": "level 2\nplease note this is a testing"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "title": "In-class Exercise 8 - Network",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#load-the-package",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#load-the-package",
    "title": "In-class Exercise 8 - Network",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#import-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#import-data",
    "title": "In-class Exercise 8 - Network",
    "section": "Import Data",
    "text": "Import Data\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nGAStech_nodes in the node form, 54 roles, and 4 columns\nGAStech_edge have the sourcelabel in additional to label"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-preparation",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-preparation",
    "title": "In-class Exercise 8 - Network",
    "section": "Data preparation",
    "text": "Data preparation\nthe code below create 2 new column into the dataframe\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nNote\n\n\n\n\ndmy from lubridate convert character field SentDate into a date column with the format of day, month, year\nabbr is one of the function in wday, when TRUE, it is MON\n\n\n\nthe code chunk below aggregate the data\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nInsight\n\n\n\nfilter(MainSubject == \"Work related\") used to select only work related, take note == is used instead of =\ngroup_by(source, target, Weekday) allows for aggregation by source, target, Weekday\nsummarise(Weight = n()) gives the aggregated number\nfilter(source!=target) to filter out user sent to send himself\nfilter(Weight &gt; 1) dont want to include send 1 time only, only keep 2 time and above\n\n\nGetting the graph\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 Ã— 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manaâ€¦\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# â„¹ 44 more rows\n#\n# Edge Data: 1,372 Ã— 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# â„¹ 1,369 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ndirected refers to direction of the relationship, directed network v.s. non-directed network. In the use case here, email is always with a direction.\nit is necessary to make sure that the number of nodes and edges matches between the network graph and the original data\ninstead of using the name of the person, the number is used only\n\n\n\nPlotting\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\ngeom_edge_arc() provides a curvature while geom_edge_link() gives a straight line"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#heatmap-association-test",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#heatmap-association-test",
    "title": "Take-home Exercise 4",
    "section": "HEATMAP + ASSOCIATION TEST",
    "text": "HEATMAP + ASSOCIATION TEST\n\nEDA - HEATMAP\nThere are 2 methods taught in this class to plot a heatmap, using heatmap() of R stats package to plot a static heatmap, or using heatmaply package to plot an interactive heatmap. Given the datasetâ€™s complexity, with numerous countries and variables, interactivity enhances data exploration significantly. Interactive heatmaps allow users to hover over cells to reveal detailed information, offering a richer analysis experience compared to static heatmaps. For more insights and capabilities of heatmaply, refer to its documentation.\nWhen crafting a heatmap, some parameters can be varied are:\n\ndata transformation method\nchange of data aggregation level\nchange of grouped level\nchange of display\n\n\nData Preparation\nTo accommodate diverse inputs such as aggregation method and group-by criteria, we design a function that dynamically processes the dataset based on user-defined parameters. This approach allows for flexibility in analyzing the data without resorting to hard coding.\nThe code chunk below takes in the dataframe, aggregation method, and group by method, the later 2 are dynamic input computed by the users. The code first prepares a list to accommodate five distinct aggregation methods. Subsequently, it groups the data based on the user-defined categorization, computing aggregated values for various economic indicators while excluding any null entries. The process yields an aggregated dataframe, which is then ready to serve as the basis for heatmap visualization.\n\naggregate_data &lt;- function(data, agg_method, group_level) {\n  \n  agg_funcs &lt;- list(\n    mean = mean,\n    max = max,\n    min = min,\n    median = median,\n    var = var\n  )\n  \n  aggregated_data &lt;- data %&gt;%\n    group_by(.data[[group_level]]) %&gt;%\n    summarise(\n      bmi_localprice = agg_funcs[[agg_method]](bmi_localprice, na.rm = TRUE),\n      bmi_usd_price = agg_funcs[[agg_method]](bmi_usd_price, na.rm = TRUE),\n      bmi_change = agg_funcs[[agg_method]](bmi_change, na.rm = TRUE),\n      export_usd = agg_funcs[[agg_method]](export_usd, na.rm = TRUE),\n      import_usd = agg_funcs[[agg_method]](import_usd, na.rm = TRUE),\n      net_export = agg_funcs[[agg_method]](net_export, na.rm = TRUE),\n      GDP = agg_funcs[[agg_method]](GDP, na.rm = TRUE),\n      gdp_per_capita = agg_funcs[[agg_method]](gdp_per_capita, na.rm = TRUE),\n      inflation = agg_funcs[[agg_method]](inflation, na.rm = TRUE),\n      unemployment = agg_funcs[[agg_method]](unemployment, na.rm = TRUE),\n      hdi = agg_funcs[[agg_method]](hdi, na.rm = TRUE),\n      population = agg_funcs[[agg_method]](population, na.rm = TRUE)\n    )\n  \n  return(aggregated_data)\n}\n\n\n\nData Transformation Method\nThe package provides three distinct data transformation methods: normalize, scale, and percentile. These transformations are crucial, particularly in the context of our dataset, which spans a wide range of values across various economic indicators. Such disparity in data ranges can obfuscate direct comparisons. Employing these transformation methods enables a more coherent analysis, facilitating clearer comparisons by adjusting the data to a common scale. This is particularly beneficial when analyzing economic indicators, where differences in magnitude can significantly impact the interpretability of the data.\nThe code below created an aggregated dataframe with mean, grouped by different country and convert it into a matrix for ploting a heatmap with country as the unique indicator per row.\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"mean\", \"country\")\n\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n\n\nScale - ColumnScale - RowNormalizePercentile\n\n\n\nheatmaply(bmi_heatmap_matrix,\n          scale = \"col\")\n\n\n\n\n\n\n\n\nheatmaply(bmi_heatmap_matrix,\n          scale = \"row\")\n\n\n\n\n\n\n\n\nheatmaply(normalize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\n\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nThe experiments indicate that since the dataset ranges vary significantly (e.g., HDI is small a score while population figures are large), the percentile method emerges as the default choice for its ability to bin countries effectively.\nThe normalization and column scaling methods also provide insights by highlighting outliers and patterns, making them valuable options.\nHowever, scaling by rows (countries) is less insightful due to its categorical nature and does not facilitate meaningful comparison across different indicators. Thus, all aggregation types should be made available for user exploration.\nThis inclusivity ensures a broader analytical scope, allowing users to discover outliers with scaling, uncover specific patterns through normalization, and categorize data effectively using the percentile approach.\n\n\n\n\n\n\n\n\nAnalysis Insights\n\n\n\nBased on the percentile graph analysis, the local Big Mac price, especially in relation to changes grouped with inflation and population, suggests a potential association among these factors. This observation leads us to infer that the local price of a Big Mac could be closely linked with such factors. For the Big Mac price in USD, it is observed to group with the Human Development Index (HDI) and GDP per capita, indicating that, globally, it may be associated with such economic indicators.\nAn intriguing aspect of this analysis is the potential connection between the Big Mac price in USD with unemployment, and the change in Big Mac price with population.\nTo fully comprehend these associations, confirmatory data analysis (CDA) should be conducted. This deeper investigation will help in understanding the underlying patterns and relationships between the price of Big Macs and various economic indicators across different regions.\n\n\n\n\nChange of Aggregation Level\nEmploying different aggregation methods enriches the comparative analysis by highlighting various aspects of the economic indicators across countries. Specifically:\n\nMean and Median: These measures provide insights into the central tendency of the economic indicators, offering a snapshot of the typical or average values.\nMin and Max: By showcasing the extremes, these aggregations reveal the full range of the data, from the lowest to the highest values.\nVariance: This measure elucidates the variability or stability of each indicator within a country, indicating how dispersed the values are around the mean.\n\nEach of these aggregation levels serves a distinct purpose in the analysis, allowing for a multifaceted understanding of the data. To explore these different perspectives, user should be able to simply adjust the aggregation method parameter in the function that prepares the data for heatmap visualization.\nThe code below compare with different aggregation method by changing the input into the function creating the heatmap matrix: bmi_heatmap &lt;- aggregate_data(bmi, \"mean\", \"country\")\n\nMeanMedianMaxMinVariance\n\n\nthe code chunk below plot a heatmap aggregated at mean level\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"mean\", \"country\")\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\nthe code chunk below plot a heatmap aggregated at median level\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"median\", \"country\")\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\nthe code chunk below plot a heatmap aggregated at max level\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"max\", \"country\")\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\nthe code chunk below plot a heatmap aggregated at min level\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"min\", \"country\")\n\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\nthe code chunk below plot a heatmap aggregated at variance level\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"var\", \"country\")\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nExposing all aggregation methods offers valuable insights, particularly at the country level. It enables the discovery of specific patterns across different countries. For example, analyzing variance across indicators can illuminate the stability or volatility of each factor within nations, providing a deeper understanding of economic dynamics. Thus making all aggregation typesâ€”mean, median, max, min, and varianceâ€”available for exploration, users are empowered to conduct a thorough examination of the dataset from various angles. This flexibility facilitates a more nuanced analysis, allowing users to identify unique trends, outliers, and correlations that might not be apparent under a single aggregation approach.\nSetting the mean as the default aggregation method, given its widespread acceptance as a fundamental measure of central tendency. It offers a solid starting point for initial explorations, from which users can then delve deeper into more specific or sophisticated analyses.\n\n\n\n\nChange of Grouped Level\nThe data supports two primary grouping levels: by country and by year, enabling cross-national or temporal comparisons. This versatility allows for nuanced comparisons across different nations or over time.\nSimilarly like aggregation methods, changing the group_level parameter in the function alters the heatmap visualization instead of hard coding it.\nThe code below compare with different group by method by changing the input into the function creating the heatmap matrix: bmi_heatmap &lt;- aggregate_data(bmi, \"mean\", \"country\"). By experimenting with various scaling methods and grouping by year, weâ€™ve assessed both mean and variance to elucidate the overarching trends of the variables over time and across countries. Only the best results are shown below.\n\nyear - meanyear - variancecountry\n\n\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"mean\", 'year')\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$year\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"var\", 'year')\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$year\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(normalize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\n\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"mean\", 'country')\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nThe analysis framework enables exploration through two pivotal lenses: country and year. Grouping data by country provides a comprehensive, nation-wide analysis, while grouping by year illuminates outliers and temporal dynamics across nations. This dual perspective enriches our understanding of the interplay among various variables, facilitating exploratory data analysis (EDA) that empowers users to uncover and interpret patterns related to these variables. Given its broader applicability, grouping by country will be the default setting, acknowledging its prevalence in analytical explorations.\n\n\n\n\n\n\n\n\nAnalysis Insights\n\n\n\nA particularly intriguing finding emerges from the year-by-year analysis, where we observe a mostly chronological arrangement of data points, reflecting national development and expected improvements in various indices over time. However, specific years stand out as outliers, notably 2009, 2010, and 2020. These years correspond to significant global eventsâ€”the economic crisis of 2009 and the COVID-19 pandemic in 2020â€”highlighting their impact on the data.\nVariance analysis across years shows which factors are most unstable across countries. For example, BMI variability was highest in 2011, inflation in 2019, and other factors varied greatly in 2021, demonstrating the value of this analysis for understanding fluctuations.\nAdditionally, BMI, net export and import are grouped based on dendrogram, leading to further investigation. This finding prompts us to explore how, beyond typical economic indicators, import and export values might specifically influence Big Mac prices through network analysis.\n\n\n\n\nChange of Display\nAdditional layout method allows the user to on or off the dendrogram by setting dendrogram augement between TRUE and FALSE\n\n\nCode\n# create an aggregated dataframe\nbmi_heatmap &lt;- aggregate_data(bmi, \"mean\", 'country')\n\n# convert to a matrix\nrow.names(bmi_heatmap) &lt;- bmi_heatmap$country\nbmi_heatmap_matrix &lt;- data.matrix(bmi_heatmap)\n\n\n\nwith dendrogramwithout dendrogram\n\n\n\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix), \n          dendrogram = TRUE)\n\n\n\n\n\n\n\n\n# plot the heatmap\nheatmaply(percentize(bmi_heatmap_matrix), \n          dendrogram = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nGiven that the dendrogram assists in the initial clustering of countries and variables, offering a hierarchical view of the data, it is recommended to be enabled by default for enhanced analytical depth. This feature enriches the userâ€™s ability to discern patterns and relationships within the data, facilitating a more intuitive understanding of complex datasets.\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical exploratory data analysis (EDA) to have a general idea on overall value range and discern potential relationships among variables. With this preliminary EDA, users can subsequently delve into comparative data analysis (CDA) targeting specific variables to test hypotheses.\nKey Features for Comprehensive Analysis:\n\nGrouping Method: Options include by year or country, facilitating targeted analysis.\nAggregation Level: Supports mean, median, max, min, and variance to accommodate different analytical needs.\nTransformation Setting: Allows scaling, normalization, or percentile-based adjustments for data preparation.\nAesthetic Setting: Offers the ability to toggle the dendrogram on or off, enhancing visual clarity.\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe: aggregate_data(data, aggregation_by, group_by)\n\ngroup_by: year(default), country\naggregation_by: mean(default), median, max, min, var\n\nModifying heatmaply() for transfomation:\n\nscale: heatmaply(data, scale = â€œcolâ€)\npercentile(default): heatmaply(percentize(data))\nnormalize: heatmaply(normalize(data))\n\nModifying augement in heatmaply():\n\ndendrogram: TRUE(default), FALSE\n\n\n\n\n\n\nCDA - ASSOCIATION\nAfter analyzing the heatmap, association tests can be employed to validate hypotheses generated from the heatmap observations, particularly to explore if thereâ€™s a relationship between any variable and the target Big Mac index, whether in local currency or USD.\nThe choice of association tests stems from our exploratory data analysis (EDA), where many variables are presented in percentiles or bins due to the significant variance in their ranges. Binning variables can offer additional insights, making the association test particularly suited for our analysis.\nThe ggstatsplot package is selected for plotting and conducting association tests because of its versatility, offering a broad array of statistical test options, and its capacity for fine-tuning the analysis. This package supports a wide variety of statistical methods and aesthetic adjustments, making it an excellent tool for our purposes. Documentation is available at ggstatsplot documentation.\nKey parameters to consider when conducting comparative data analysis (CDA) include:\n\nInput-related parameters: Variables for comparison, binning methods, and bin size.\nTesting-related parameters: Type of statistical test and confidence level.\nAesthetic choices: Representation in percentages or counts.\n\nThese considerations ensure a comprehensive and nuanced analysis, allowing for the validation of initial hypotheses and the exploration of deeper associations between variables and the Big Mac index.\n\nData Preparation\nTo accommodate diverse inputs of binning method and binning size, we design a function that dynamically processes the dataset based on user-defined parameters. This approach allows for flexibility in analyzing the data without resorting to hard coding.\nThe code chunk below creates 2 functions accepts a dataframe, two column names for binning, the number of bins. It applies either percentile-based binning or equal-sized binning.\n\nprepare_binned_data &lt;- function(data, col1, col2, n_bins, method = \"percentile\") {\n  if (method == \"percentile\") {\n    # Binning based on percentiles\n    col1_bins &lt;- ntile(data[[col1]], n_bins)\n    col2_bins &lt;- ntile(data[[col2]], n_bins)\n  } else if (method == \"equal\") {\n    # Binning into equal-sized bins\n    col1_bins &lt;- cut(data[[col1]], breaks = n_bins, labels = FALSE)\n    col2_bins &lt;- cut(data[[col2]], breaks = n_bins, labels = FALSE)\n  } else {\n    stop(\"Invalid binning method specified. Use 'percentile' or 'equal'.\")\n  }\n  \n  # Dynamically generate labels based on the actual number of unique bins\n  unique_bins_col1 &lt;- length(unique(col1_bins))\n  unique_bins_col2 &lt;- length(unique(col2_bins))\n  \n  data[[col1]] &lt;- factor(col1_bins, levels = 1:unique_bins_col1, labels = paste(\"Bin\", 1:unique_bins_col1))\n  data[[col2]] &lt;- factor(col2_bins, levels = 1:unique_bins_col2, labels = paste(\"Bin\", 1:unique_bins_col2))\n  \n  return(data)\n}\n\n\n\nChoice of Variables\n\nUsing bmi_usd_priceUsing bmi_change\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_usd_price\", \"gdp_per_capita\", 5, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  x = \"gdp_per_capita\",\n  y = \"bmi_usd_price\",\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 5, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nThe experiment delves into the dynamics between the Big Mac Index and various factors, positioning Big Mac-related indices prominently for comparative analysis. Among the three Big Mac indicesâ€”bmi_usd_dollars, bmi_change, and bmi_localpriceâ€”the choice of index for the x-axis is crucial for clarity and insight.\n\nbmi_usd_dollars serves as a pivotal metric for cross-country comparisons based on pricing, offering a standard scale to assess value discrepancies globally.\nbmi_change provides insights into the temporal price evolution of the Big Mac, revealing trends and fluctuations over time.\nbmi_localprice, while indicative of the Big Macâ€™s price in local currencies, introduces a layer of complexity due to varying currency strengths across countries. This variance may not directly contribute to the analytical objectives, potentially introducing bias rather than clarity.\n\nTherefore, for x-axis variables, bmi_usd_dollars and bmi_change should be exposed to comapre against all other indicators to uncover significant relationships without the distortion of currency variability.\n\n\n\n\nChoice of Binning\n\nPercentileEqual\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 5, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 5, method = \"equal\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nPercentile binning is preferred for its ability to evenly distribute observations across bins, especially when variables exhibit large value ranges. Equal binning, focusing more on uniform value intervals, might unduly highlight outliers or specific cases, thereby skewing the analysis. Both options should be provided for analysis.\n\n\n\n\nNumber of Bins\n\nBin = 2Bin = 4Bin = 7\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 2, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 7, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nChoosing the right number of bins is crucial: too few (e.g., 2) obscure patterns, while too many (e.g., 7) clutter the plot, hindering analysis. An optimal range of 3-6 bins, with a default of 4, strikes a balance between simplicity and detail, enabling clear and insightful data visualizations.\n\n\n\n\nType of Statistical Test\n\nparametricnonparametricrobustbayes\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"parametric\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"robust\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"bayes\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nParametric, Nonparametric, and robust shows no difference in the display results. Thus only 1 of them and Bayes should be exposed for simplification. Nonparametric tests set as default as it is a reliable alternative when data does not adhere to normal distribution assumptions, ensuring robust analysis across various data types.\n\n\n\n\nChoice of Confidence Level\n\n95 CI99 CI\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n   conf.level = 0.99\n)\n\n\n\n\n\n\n\n\n\n\nRepresentation\n\npercentagecount\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n  conf.level = 0.95,\n  label = \"percentage\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n   conf.level = 0.95,\n  label = \"count\"\n)\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical confirmatory data analysis (EDA) to understand the association between our target variables and other variables.\n\nKey Features for Comprehensive Analysis:\nChoice of Variables: x - BMI related, y - others\nBinning method: Options include by percentile, or equal\nNumber of Bins: Supports a range of numbers in a slider.\nStatistical Test: Allows from nonparametric, bayes\nConfidence Level: 95%, or 99%\nAesthetic Setting: Offers the ability to choose label by count of percentage\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe: prepare_binned_data(data, col1, col2, bins, method)\n\ncol1: bmi-related\ncol2: others\nbins: integers from 3 to 6\nmethod: percentile, equal\n\nModifying augement in ggbarstats():\n\ntype: â€œnonparametricâ€, â€œbayesâ€\nconf.level: 0.95, 0.99\nlabel: â€œcountâ€, â€œpercentageâ€\n\n\n\n\n\n\nUI Design\n\nThe overall layout consists of the top navigation bar, which allows navigation among big analysis topics, side navigation filter to navigate between sub analysis. There are 2 plot augment filtering panales, one located at the side, which allows the user to adjust the input related variables, such as x, y, analysis details, group by methods. The one in the main panel allows the user to adjust the individual plot, such as coloring coding, on/off certain features. Below the plotting area, is the text selction, whereby analysis would be displaying at the bottom.\nThere are 3 special features of this dashboard\n\nthe note taking panel allows the user to quickly drop any insights during EDA, and when he change over to the CDA page, he can still refer to the note takings to conduct any test\nthe insights panel summarized the insights for the analysis as a reference\nquick access to the homepage by clicking the icon.\n\n\nHeatmap Page\n\nfor each individual exposed feature, please refer to the summary above. Instead of using the original argument name, the augment has been renamed and grouped accordingly to unsure better understanding with people who are not familiar with coding.\n\n\nAssociation Page\nfor each individual exposed feature, please refer to the summary above. Instead of using the original argument name, the augment has been renamed and grouped accordingly to unsure better understanding with people who are not familiar with coding."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#graph-layout",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#graph-layout",
    "title": "In-class Exercise 8 - Network",
    "section": "graph layout",
    "text": "graph layout\nusing igraph layout instead\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"kk\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#add-in-color-to-node",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#add-in-color-to-node",
    "title": "In-class Exercise 8 - Network",
    "section": "add in color to node",
    "text": "add in color to node\nthe code below add in color to node based on department, and increase the size to 3\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#modify-edge",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#modify-edge",
    "title": "In-class Exercise 8 - Network",
    "section": "modify Edge",
    "text": "modify Edge\nthe code below use width to control the thickness of the edge, and adding scale_edge_width() to ensure it is propotinoal\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width = Weight), alpha = 0.2) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nsome edges are darker, with higher overlapping"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#facet",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#facet",
    "title": "In-class Exercise 8 - Network",
    "section": "Facet",
    "text": "Facet\nsimilar to ggplot, it can use facet, there are 3 types: graph, edge, and node\n\nset_graph_style()\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width = Weight), alpha = 0.2) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + facet_edges(~Weekday)\n\n\n\n\nall using he same layout, the location is the same, some appear at certain day, but some are not"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#interactive-visualization",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#interactive-visualization",
    "title": "In-class Exercise 8 - Network",
    "section": "Interactive Visualization",
    "text": "Interactive Visualization\nanother library viznetwork is used, the data preparation is different.\n\nsource, target -&gt; from, to, rename the id, using from, to"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#tree-map-anova-test",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#tree-map-anova-test",
    "title": "Take-home Exercise 4",
    "section": "TREE MAP + ANOVA TEST",
    "text": "TREE MAP + ANOVA TEST\n\nTREE MAP\nThere are 3 methods taught in this class to plot a treemap, using treemap() of treemap package to plot a static treemap, using treemapify package to plot, or d3treeR for an interactive plot. since the data will be already grouped based on its regional groups, there is not much information on the display, an interactive map is not necessary. among the 2 static package, treemap is chosen due to its flexibility and simplicity. the complete documentation could be found here.\nWhen crafting a treemap, some parameters can be varied are:\n\nchange of data aggregation level\nchange of grouped level\nchange of variables for display\nchange of algorithm(allow sorting), and color coding\n\n\nData Preperation\nSimilarly like the heatmap, an aggregated dataframe should be used and prepared before the plotting\n\naggregate_data &lt;- function(data, agg_method, group_levels) {\n  agg_funcs &lt;- list(\n    mean = mean,\n    max = max,\n    min = min,\n    median = median,\n    var = var\n  )\n  \n  # Ensuring group_levels is a character vector\n  if(is.character(group_levels)) {\n    group_levels &lt;- syms(group_levels)\n  } else {\n    stop(\"group_levels should be a vector of column names.\")\n  }\n  \n  aggregated_data &lt;- data %&gt;%\n    group_by(!!!group_levels) %&gt;%\n    summarise(\n      bmi_localprice = agg_funcs[[agg_method]](bmi_localprice, na.rm = TRUE),\n      bmi_usd_price = agg_funcs[[agg_method]](bmi_usd_price, na.rm = TRUE),\n      bmi_change = agg_funcs[[agg_method]](bmi_change, na.rm = TRUE),\n      export_usd = agg_funcs[[agg_method]](export_usd, na.rm = TRUE),\n      import_usd = agg_funcs[[agg_method]](import_usd, na.rm = TRUE),\n      net_export = agg_funcs[[agg_method]](net_export, na.rm = TRUE),\n      GDP = agg_funcs[[agg_method]](GDP, na.rm = TRUE),\n      gdp_per_capita = agg_funcs[[agg_method]](gdp_per_capita, na.rm = TRUE),\n      inflation = agg_funcs[[agg_method]](inflation, na.rm = TRUE),\n      unemployment = agg_funcs[[agg_method]](unemployment, na.rm = TRUE),\n      hdi = agg_funcs[[agg_method]](hdi, na.rm = TRUE),\n      population = agg_funcs[[agg_method]](population, na.rm = TRUE),\n      .groups = 'drop'\n    )\n  \n  return(aggregated_data)\n}\n\n\n\nchange of data aggregation level\n\nmeanvarianceMax\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"var\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"max\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nMean offers a central tendency measure, providing a snapshot of the average outcome across a dataset. Itâ€™s invaluable for understanding the general behavior of a variable. Variance sheds light on the variability within the dataset, indicating how spread out the data points are from the mean.\nWhile minimum, maximum, and median provide basic data insights, they may not always uncover deeper patterns effectively. Therefore, only mean and variance will be exposed.\n\n\n\n\ngroup by different region group\n\ncontinentg20eu\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"g20\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"g20\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"eu\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"eu\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\nto visualize with different variables\n\n\nVariables for display\n\ngdp_per_capita & bmi_usdimport_usd & bmi_changeunemployment & bmi_change\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"export_usd\",\n        vColor = \"bmi_change\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"unemployment\",\n        vColor = \"bmi_change\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis Insights\n\n\n\nThe treemap visualization uniquely showcases BMI-related variables alongside others, utilizing spatial grouping or approximation. This method uncovers patterns based on location or organizational insights effectively. For instance, when examining variables like â€œexport_usdâ€ and â€œbmi_changeâ€ by continent, an intriguing pattern emerges: countries in South America tend to display significant changes in their local Big Mac prices. These countries are represented by smaller shapes in the treemap, suggesting lower export trade values. This observation prompts further investigation into two key areas:\n\nthe veracity of the apparent higher BMI change in South American countries, and\nthe potential underlying relationship between export values and BMI changes.\n\n\n\n\n\nChange of Color Coding\n\nvaluemanual\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'value'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'manual'\n        )\n\n\n\n\n\n\n\n\n\n\nSorting\n\nwithout sortingwith sorting\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'manual'\n        )\n\n\n\n\n\n\n\n\n\nCode\nbmi_heatmap &lt;- aggregate_data(bmi_all, \"mean\", c(\"continent\", \"country\"))\n\ntreemap(bmi_heatmap,\n        index = c(\"continent\", \"country\"),\n        vSize = \"gdp_per_capita\",\n        vColor = \"bmi_usd_price\",\n        algorithm = \"squarified\",\n        sortID = \"bmi_usd_price\",\n        palette = \"RdYlBu\",\n        type = 'manual'\n        )\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical exploratory data analysis (EDA) to have a general idea on patterns related to its organizational, or locational characters. With this preliminary EDA, users can subsequently delve into comparative data analysis (CDA) targeting specific variables to test hypotheses.\nKey Features for Comprehensive Analysis:\n\nAggregation Level: Supports mean, and variance to accommodate different analytical needs.\nGrouping Method: allows at continent(locational), or g20, g7, eu (organizational)\nInput Variables: Allows all variables to be selected as size, and color\nAesthetic Setting: Offers the ability to select from color coing method, and if sort by its colored variable.\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe: aggregate_data(data, aggregation_by, group_by)\n\ngroup_by: continent(default), g20, g7, eu\naggregation_by: mean(default), var\n\nModifying augement in treemap():\n\nvSize\nvColor\ntype: manual, value\nalgorithm = â€œsquarifiedâ€, sortID = â€œcolâ€\n\n\n\n\n\n\nANOVA TEST\nWth this, we can conduct anova test to test on our hypothesis from the EDA above, if any countries, or regions have higher bmi, or other indicators than others. The package used to conduct anova test is the same as the association test, The ggstatsplot package is selected because of its versatility, offering a broad array of statistical test options, and its capacity for fine-tuning the analysis.\nKey parameters to consider when conducting comparative data analysis (CDA) include:\n\nInput-related parameters: compare at the region level or at the individual country level\nTesting-related parameters: Type of statistical test and confidence level.\nAesthetic choices:Plot type, display type\n\nThese considerations ensure a comprehensive and nuanced analysis, allowing for the validation of initial hypotheses and the exploration of deeper associations between variables and the Big Mac index.\n\nLevel of Details\n\nby coutryby grouped region\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nCode\nggbetweenstats(\n  data = bmi_all,\n  x = g7, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nffering insights across different granularity levels, itâ€™s beneficial to provide users with the option to select their desired level of detail. When displaying data at the country level, considerations about data filtering become crucial to avoid overwhelming the visualization and to facilitate meaningful comparisons. Therefore, employing two separate dataframes for plottingâ€”each tailored to a specific granularity level (e.g., country versus region or continent)â€”ensures that users can navigate through the data effectively, making comparisons and drawing insights in a manner that best suits their analytical needs. This approach enhances the user experience by providing clarity and focus, allowing for a more nuanced exploration of the data.\n\n\n\n\nChoice of Statistical test\n\nparametricNon parametricrobustbayes\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"r\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"b\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\n\nChoice of Confidence Level\n\n95 CI99 CI\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\n\nChoice of Display Label\n\nallnoneonly significantonly non-significant\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"all\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"none\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"ns\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nFor optimal clarity in data visualization, itâ€™s important to focus on displaying only those relationships that are statistically significant, eliminating the need to differentiate between â€˜allâ€™, and â€˜non-significantâ€™, in the analysis. Highlighting significant relationships simplifies the interpretation process, steering attention toward meaningful insights. The inclusion of non-significant data points often adds unnecessary complexity without contributing valuable insights, potentially obscuring the truly impactful findings.\n\n\n\n\nDisplay Plot Type\n\nboth voilin plot and boxpolotonly voilin plotonly boxplot\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"all\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"none\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  conf.level = 0.95,\n  # to remove boxplot\n  boxplot.args = list(width = 0)\n)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_filter &lt;- filter(bmi_all, g7  == TRUE)\nggbetweenstats(\n  data = bmi_all_filter,\n  x = country, \n  y = bmi_usd_price,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE,\n  plot.type = \"box\",\n  conf.level = 0.95,\n  # to remove violin plot\n  violin.args = list(width = 0, linewidth = 0)\n)\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical confirmatory data analysis (CDA) via ANOVA test\n\nKey Features for Comprehensive Analysis:\nLevel of Details: region, country\nStatistical Test: Allows from parametric, nonparametric, bayes, robust\nConfidence Level: 95%, or 99%\nAesthetic Setting: Offers the ability to choose display label, and plot type\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe:using bmi data, and filter()\nModifying augement in ggbarstats():\n\ntype: â€œparametricâ€, â€œnonparametricâ€, â€œbayesâ€, â€œrobustâ€\nconf.level: 0.95, 0.99\npairwise.display: â€œallâ€, â€œnoneâ€, â€œnoneâ€, â€œsâ€, â€œnsâ€\noff boxplot/violin chart:\n\nboxplot.args = list(width = 0)\nviolin.args = list(width = 0, linewidth = 0)\n\n\n\n\n\n\nTree Map Page\n\nfor each individual exposed feature, please refer to the summary above. Instead of using the original argument name, the augment has been renamed and grouped accordingly to unsure better understanding with people who are not familiar with coding.\n\n\nANOVA Page\n\nfor each individual exposed feature, please refer to the summary above. Instead of using the original argument name, the augment has been renamed and grouped accordingly to unsure better understanding with people who are not familiar with coding."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#using-bmi_change",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#using-bmi_change",
    "title": "Take-home Exercise 4",
    "section": "Using bmi_change",
    "text": "Using bmi_change\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 5, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nThe experiment delves into the dynamics between the Big Mac Index and various factors, positioning Big Mac-related indices prominently for comparative analysis. Among the three Big Mac indicesâ€”bmi_usd_dollars, bmi_change, and bmi_localpriceâ€”the choice of index for the x-axis is crucial for clarity and insight.\n\nbmi_usd_dollars serves as a pivotal metric for cross-country comparisons based on pricing, offering a standard scale to assess value discrepancies globally.\nbmi_change provides insights into the temporal price evolution of the Big Mac, revealing trends and fluctuations over time.\nbmi_localprice, while indicative of the Big Macâ€™s price in local currencies, introduces a layer of complexity due to varying currency strengths across countries. This variance may not directly contribute to the analytical objectives, potentially introducing bias rather than clarity.\n\nTherefore, for x-axis variables, bmi_usd_dollars and bmi_change should be exposed to comapre against all other indicators to uncover significant relationships without the distortion of currency variability.\n\n\n\nChoice of Binning\n\nPercentileEqual\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 5, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 5, method = \"equal\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nPercentile binning is preferred for its ability to evenly distribute observations across bins, especially when variables exhibit large value ranges. Equal binning, focusing more on uniform value intervals, might unduly highlight outliers or specific cases, thereby skewing the analysis. Both options should be provided for analysis.\n\n\n\n\nNumber of Bins\n\nBin = 2Bin = 4Bin = 7\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 2, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 7, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nChoosing the right number of bins is crucial: too few (e.g., 2) obscure patterns, while too many (e.g., 7) clutter the plot, hindering analysis. An optimal range of 3-6 bins, with a default of 4, strikes a balance between simplicity and detail, enabling clear and insightful data visualizations.\n\n\n\n\nType of Statistical Test\n\nparametricnonparametricrobustbayes\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"parametric\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"robust\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"bayes\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nParametric, Nonparametric, and robust shows no difference in the display results. Thus only 1 of them and Bayes should be exposed for simplification. Nonparametric tests set as default as it is a reliable alternative when data does not adhere to normal distribution assumptions, ensuring robust analysis across various data types.\n\n\n\n\nChoice of Confidence Level\n\n95 CI99 CI\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n  conf.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n   conf.level = 0.99\n)\n\n\n\n\n\n\n\n\n\n\nRepresentation\n\npercentagecount\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n  conf.level = 0.95,\n  label = \"percentage\"\n)\n\n\n\n\n\n\n\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_change\", \"inflation\", 4, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  y = \"bmi_change\",\n  x = \"inflation\",\n  type = \"nonparametric\",\n   conf.level = 0.95,\n  label = \"count\"\n)\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical confirmatory data analysis (EDA) to understand the association between our target variables and other variables.\nKey Features for Comprehensive Analysis:\nChoice of Variables: x - BMI related, y - others\nBinning method: Options include by percentile, or equal\nNumber of Bins: Supports a range of numbers in a slider.\nStatistical Test: Allows from nonparametric, bayes\nConfidence Level: 95%, or 99%\nAesthetic Setting: Offers the ability to choose label by count of percentage\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe: prepare_binned_data(data, col1, col2, bins, method)\n\ncol1: bmi-related\ncol2: others\nbins: integers from 3 to 6\nmethod: percentile, equal\n\nModifying augement in ggbarstats():\n\ntype: â€œnonparametricâ€, â€œbayesâ€\nconf.level: 0.95, 0.99\nlabel: â€œcountâ€, â€œpercentageâ€\n\n\n\n\n\nUI Design"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#using-bmi_usd_price",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#using-bmi_usd_price",
    "title": "Take-home Exercise 4",
    "section": "Using bmi_usd_price",
    "text": "Using bmi_usd_price\n\n\nCode\nprepared_data &lt;- prepare_binned_data(bmi, \"bmi_usd_price\", \"gdp_per_capita\", 5, method = \"percentile\")\n\nggbarstats(\n  data = prepared_data,\n  x = \"gdp_per_capita\",\n  y = \"bmi_usd_price\",\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#paraplot",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#paraplot",
    "title": "Take-home Exercise 4",
    "section": "PARAPLOT",
    "text": "PARAPLOT\nParaplot offers a way to visualize relationships among multiple variables. Its advantage lies in the comprehensive representation of multidimensional data. The ggparcoord() function from the GGally package can be used for creating parallel coordinate plots.\nWhen ploting paraplot, some considerations are:\n\nSelect a grouping level such as countries, years, or regions.\nAggregate or facet the data visualization.\nChoose a scaling method.\nAdjust line opacity.\nToggle boxplots on or off\n\n\nControl Group by Level\n\ncountryyearcontinent\n\n\n\n\nCode\nggparcoord(data = bmi_all, \n           columns = c(4:15), \n           groupColumn = 1,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nbmi_all_new &lt;- bmi_all\nbmi_all_new$year &lt;- factor(bmi_all_new$year, levels = sort(unique(bmi_all_new$year)))\n\nggparcoord(data = bmi_all_new, \n           columns = c(4:15), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = bmi_all, \n           columns = c(4:15), \n           groupColumn = 16,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nParaplot becomes challenging with too many categories, as displaying all countries or years simultaneously can lead to clutter. Itâ€™s generally more effective for grouped regions, yet even then, the visualization can become overwhelmed by numerous lines.\n\n\n\n\nVisualization Level\n\nFacetAggregated\n\n\n\n\nCode\nggparcoord(data = bmi_all, \n           groupColumn = 16,\n           columns = c(4:15), \n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE) +\n  facet_wrap(~ continent) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = bmi_all, \n           columns = c(4:15), \n           groupColumn = 16,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\nSummary\nParaplot, while useful for certain analyses, may not be the most effective method in contexts where clarity and insightfulness are paramount. Alternatives like treemaps provide intuitive visualization for comparing variables across groups, and statistical methods like ANOVA tests offer rigorous analysis. Additionally, correlation plots can elucidate the relationships among factors. In this case, the paraplotâ€™s potential for clutter and redundancy suggests exploring other analytical tools might yield more actionable insights."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#correlation",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#correlation",
    "title": "Take-home Exercise 4",
    "section": "CORRELATION",
    "text": "CORRELATION\nFor the correlation analysis, there are 2 packages covered in this course\n\ncorrplot package: Tailored for crafting visually appealing correlation matrices. It shines with its vast array of customization options, including varied display shapes, matrix reordering, and distinct customizations for the matrixâ€™s upper and lower parts. Importantly, it facilitates using different statistical methods for correlation computation, enhancing its utility for exploratory analysis.\nggcorrmat() from ggstatsplot: Integrates well within the ggplot2 ecosystem, offering a structured approach to generating correlation matrices embellished with ggplot2â€™s styling and theming capabilities.\n\nCorrplot package is chosen due to its more aesthetic appealing and offers more custimization options where plotting\nWhen crafting a correlation plot, some parameters can be varied are:\n\nChoice of Correlation Coefficient\nData Filtering Capabilities\nDisplay Customization\n\n\nData Preperation\nAll variables used in the correlation analysis must be numerical. The code below select numerical and country column.\n\n\nCode\nbmi_numeric &lt;- bmi %&gt;%\n  select(country, where(is.numeric))\n\n\n\n\nChoice of Correlation Coefficient\nUsers can select the type of statistical correlation coefficient for their analysis, such as Pearson, Spearman, or Kendall. This selection allows for the adaptation of the analysis to the nature of the data and the specific relationships of interest.\nThree types of correlation coefficients can be specified using the method argument in the cor() function:\n\nPearson: The default method, suitable for linear relationships, calculating the linear dependence between variables.\nKendall: A non-parametric test that measures the ordinal association between variables.\nSpearman: A non-parametric test that assesses how well the relationship between two variables can be described using a monotonic function.\n\n\nPearsonKendallSpearman\n\n\nThe code chunk below demonstrates the use of the Pearson method.\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"pearson\",use = \"complete.obs\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\nThe code chunk below demonstrates the use of the Kendall method.\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"kendall\", use = \"complete.obs\")\na &lt;- corrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\ntThe code chunk below demonstrates the use of the Spearman method.\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nAmong these three, Pearson is the most commonly used method for calculating correlation efficiency. Itâ€™s set as the default input variable, allowing users to switch between different methods.\n\n\n\n\nData Filtering Capabilities\nTo hone in on relevant insights, users can filter the input data based on specific criteria, such as particular countries, ranges of years, or predefined groups. This targeted analysis helps in isolating the effects and relationships that are most pertinent to the userâ€™s research questions.\nAdditionally, zooming into specific years or countries can provide more focused analysis. To facilitate this:\n\nfiltering specific years\nfiltering specific countries\n\nSetting selected columns as variables makes adjustments easier. Below, the column and input are set as variables:\n\n\nCode\nx_col &lt;- \"year\"  \nx_input &lt;- \"2020\"  \n\nfiltered_data &lt;- bmi_numeric %&gt;%\n  filter(.data[[x_col]] == x_input) %&gt;% \n  select(-.data[[x_col]]) \n\n\n\nFilter by YearFilter By Country\n\n\n\n\nCode\nx_col &lt;- \"year\"  \nx_input &lt;- \"2020\"  \n\nfiltered_data &lt;- bmi_numeric %&gt;%\n  filter(.data[[x_col]] == x_input) %&gt;% \n  select(-.data[[x_col]]) \n\nbmi.cor &lt;- cor(filtered_data[-1],method = \"pearson\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\nCode\nx_col &lt;- \"country\"  \nx_input &lt;- \"China\"  \n\nfiltered_data &lt;- bmi_numeric %&gt;%\n  filter(.data[[x_col]] == x_input) %&gt;% \n  select(-.data[[x_col]]) \n\nbmi.cor &lt;- cor(filtered_data[-1],method = \"pearson\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\nAggregated and Faceted Analysis\nOffering users the capability to conduct comparative analysis across specific years or countries enriches the analytical depth. However, when direct faceting support is lacking within the chosen package, a practical workaround involves generating two distinct plots for the respective comparisons. These plots can then be arranged side by side on the same webpage through frontend design.\n\n\nCustomization Represnetation\n\nCircleSquareEllipseNumberShadeColorPie\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'circle'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'ellipse'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'number'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'shade'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'color'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'pie'\n         )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\n\nâ€˜Circleâ€™ and â€˜Squareâ€™ provide similar visual cues, differing mainly in shape. Since their informational value is essentially the same, opting for oneâ€”such as â€˜Squareâ€™â€”is sufficient for simplicity.\nâ€˜Ellipseâ€™ enriches the analysis by indicating the directionality of relationships, adding a layer of insight beyond mere correlation strength.\nâ€˜Numberâ€™ directly presents the correlation coefficients, offering precise quantitative insights at a glance.\nâ€˜Shadeâ€™ and â€˜Colorâ€™ resemble a heatmap, effectively visualizing the strength of relationships through color gradients. This method is intuitive and visually engaging, making it easy to identify patterns of interest.\nâ€˜Pieâ€™, while creative, tends to clutter the visualization, making it challenging to extract meaningful insights.\n\nGiven these considerations, the recommended options to make available for user selection are â€˜Squareâ€™, â€˜Ellipseâ€™, and â€˜Numberâ€™. These choices balance clarity, informational depth, and ease of interpretation, facilitating a comprehensive and accessible analytical experience.\n\n\n\n\nCustomized Layout for Upper, Lower\n::: panel-tabset"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#circle",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#circle",
    "title": "Take-home Exercise 4",
    "section": "Circle",
    "text": "Circle\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'circle'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#circle-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#circle-1",
    "title": "Take-home Exercise 4",
    "section": "Circle",
    "text": "Circle\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'circle'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#square",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#square",
    "title": "Take-home Exercise 4",
    "section": "Square",
    "text": "Square\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ellipse",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ellipse",
    "title": "Take-home Exercise 4",
    "section": "Ellipse",
    "text": "Ellipse\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'ellipse'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#number",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#number",
    "title": "Take-home Exercise 4",
    "section": "Number",
    "text": "Number\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'number'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#shade",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#shade",
    "title": "Take-home Exercise 4",
    "section": "Shade",
    "text": "Shade\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'shade'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#color",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#color",
    "title": "Take-home Exercise 4",
    "section": "Color",
    "text": "Color\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'color'\n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#pie",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#pie",
    "title": "Take-home Exercise 4",
    "section": "Pie",
    "text": "Pie\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'pie'\n         )\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\n\nâ€˜Circleâ€™ and â€˜Squareâ€™ provide similar visual cues, differing mainly in shape. Since their informational value is essentially the same, opting for oneâ€”such as â€˜Squareâ€™â€”is sufficient for simplicity.\nâ€˜Ellipseâ€™ enriches the analysis by indicating the directionality of relationships, adding a layer of insight beyond mere correlation strength.\nâ€˜Numberâ€™ directly presents the correlation coefficients, offering precise quantitative insights at a glance.\nâ€˜Shadeâ€™ and â€˜Colorâ€™ resemble a heatmap, effectively visualizing the strength of relationships through color gradients. This method is intuitive and visually engaging, making it easy to identify patterns of interest.\nâ€˜Pieâ€™, while creative, tends to clutter the visualization, making it challenging to extract meaningful insights.\n\nGiven these considerations, the recommended options to make available for user selection are â€˜Squareâ€™, â€˜Ellipseâ€™, and â€˜Numberâ€™. These choices balance clarity, informational depth, and ease of interpretation, facilitating a comprehensive and accessible analytical experience.\n\n\n\nCustomized Layout for Upper, Lower\n::: panel-tabset"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#upper---number-lower---ellipse",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#upper---number-lower---ellipse",
    "title": "Take-home Exercise 4",
    "section": "Upper - number, Lower - ellipse",
    "text": "Upper - number, Lower - ellipse\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot.mixed(bmi.cor,\n         tl.col = \"black\",\n         lower = 'ellipse',\n         upper = 'number',\n         \n         )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#all-square",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#all-square",
    "title": "Take-home Exercise 4",
    "section": "All Square",
    "text": "All Square\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nUtilizing different styles for representing correlation matrices, such as through corrplot.mixed() instead of the standard corrplot(), introduces a notable change in function usage. While these mixed representations can add visual diversity, they also complicate the plotting process and may lengthen processing time. This added complexity, particularly in a dynamic Shiny interface where users can switch representation methods on-the-fly, may not significantly enhance the analytical value. Given this consideration, opting for a uniform plotting approach without mixed representations is advised to maintain simplicity and efficiency. This decision aligns with the goal of providing a streamlined and effective user experience, prioritizing clarity and speed over the marginal benefits of mixed visual styles.\n\n\n\nCustomized Layout\n\nOnly LowerOnly UpperAll\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square',\n         type = 'lower'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square',\n         type = 'upper'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )\n\n\n\n\n\n\n\n\n\n\nCustomized Layout - diag\n\nOFFON\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = TRUE\n         )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nIn correlation matrices, the diagonal elements, which represent the correlation of variables with themselves, invariably equal 1. This uniformity means these elements do not contribute additional insights into the interrelationships among the variables. Recognizing this, displaying the diagonal is not essential for enhancing the analytical depth of the plots. Consequently, the option to toggle the visibility of the diagonal will not be offered to users; it will be defaulted to â€˜offâ€™ for all plots. This approach simplifies the visualization, directing focus to the more informative aspects of the correlation matrix and streamlining the user experience by eliminating redundant information.\n\n\n\n\nCustomized Layout - order\n\nhclustalphabetAOEFPC\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'hclust'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'alphabet'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'AOE'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'FPC',\n         \n         )\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct EDA\n\nChoice of Correlation Coefficient\nData Filtering Capabilities\nComparative analysis: At the interface\nAesthetic Setting: Represnetation type, layoput type, orders\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input cor.matrix: cor(data,method)\n\nmethod = â€œkendallâ€, â€œpearsonâ€, â€œspearmanâ€\n\nFiltering Input daraframe: x_col, x_input\nModifying augement in corrplot():\n\nmethod: â€˜squareâ€™, â€˜ellipseâ€™, â€˜numberâ€™\ntype = â€˜lowerâ€™, â€˜upperâ€™\norder: â€˜hclustâ€™, â€˜alphabetâ€™, â€˜AOEâ€™, â€˜FPCâ€™\ndiag: TRUE, FALSE\n\n\n\n\n\nScatter Plot\nFor pairs of variables with higher correlation, the user then can conduct CDA to perform statistical tests to evaluate the significance of these correlations.\nggstatsplot package is used to perform significance testing due to its flexibility in changing the augements while conducting the analysis.\nKey parameters to consider when conducting comparative data analysis (CDA) include:\nInput-related parameters: Variables for comparison, binning methods, and bin size.\nTesting-related parameters: Type of statistical test and confidence level.\nAesthetic choices: Representation in percentages or counts.\n\nData Preperation\n\n\nData Preperation\nto visualize the relationship between gdp_per_capita and bmi_usd_price, including a significance test for their correlation.\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = gdp_per_capita,\n  y = bmi_usd_price,\n  marginal = FALSE,\n  )\n\n\n\n\n\nSummary\n\n\n\n\n\n\narguments\n\n\n\n\ntest method\nsignificant test\nx & y"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#correlation-scatter-plot",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#correlation-scatter-plot",
    "title": "Take-home Exercise 4",
    "section": "CORRELATION + SCATTER PLOT",
    "text": "CORRELATION + SCATTER PLOT\n\nCORRELATION\nFor the correlation analysis, there are 2 packages covered in this course\n\ncorrplot package: Tailored for crafting visually appealing correlation matrices. It shines with its vast array of customization options, including varied display shapes, matrix reordering, and distinct customizations for the matrixâ€™s upper and lower parts. Importantly, it facilitates using different statistical methods for correlation computation, enhancing its utility for exploratory analysis.\nggcorrmat() from ggstatsplot: Integrates well within the ggplot2 ecosystem, offering a structured approach to generating correlation matrices embellished with ggplot2â€™s styling and theming capabilities.\n\nCorrplot package is chosen due to its more aesthetic appealing and offers more custimization options where plotting\nWhen crafting a correlation plot, some parameters can be varied are:\n\nChoice of Correlation Coefficient\nData Filtering Capabilities\nDisplay Customization\n\n\nData Preperation\nAll variables used in the correlation analysis must be numerical. The code below select numerical and country column.\n\n\nCode\nbmi_numeric &lt;- bmi %&gt;%\n  select(country, where(is.numeric))\n\n\n\n\nChoice of Correlation Coefficient\nUsers can select the type of statistical correlation coefficient for their analysis, such as Pearson, Spearman, or Kendall. This selection allows for the adaptation of the analysis to the nature of the data and the specific relationships of interest.\nThree types of correlation coefficients can be specified using the method argument in the cor() function:\n\nPearson: The default method, suitable for linear relationships, calculating the linear dependence between variables.\nKendall: A non-parametric test that measures the ordinal association between variables.\nSpearman: A non-parametric test that assesses how well the relationship between two variables can be described using a monotonic function.\n\n\nPearsonKendallSpearman\n\n\nThe code chunk below demonstrates the use of the Pearson method.\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"pearson\",use = \"complete.obs\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\nThe code chunk below demonstrates the use of the Kendall method.\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"kendall\", use = \"complete.obs\")\na &lt;- corrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\ntThe code chunk below demonstrates the use of the Spearman method.\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nAmong these three, Pearson is the most commonly used method for calculating correlation efficiency. Itâ€™s set as the default input variable, allowing users to switch between different methods.\n\n\n\n\nData Filtering Capabilities\nTo hone in on relevant insights, users can filter the input data based on specific criteria, such as particular countries, ranges of years, or predefined groups. This targeted analysis helps in isolating the effects and relationships that are most pertinent to the userâ€™s research questions.\nAdditionally, zooming into specific years or countries can provide more focused analysis. To facilitate this:\n\nfiltering specific years\nfiltering specific countries\n\nSetting selected columns as variables makes adjustments easier. Below, the column and input are set as variables:\n\n\nCode\nx_col &lt;- \"year\"  \nx_input &lt;- \"2020\"  \n\nfiltered_data &lt;- bmi_numeric %&gt;%\n  filter(.data[[x_col]] == x_input) %&gt;% \n  select(-.data[[x_col]]) \n\n\n\nFilter by YearFilter By Country\n\n\n\n\nCode\nx_col &lt;- \"year\"  \nx_input &lt;- \"2020\"  \n\nfiltered_data &lt;- bmi_numeric %&gt;%\n  filter(.data[[x_col]] == x_input) %&gt;% \n  select(-.data[[x_col]]) \n\nbmi.cor &lt;- cor(filtered_data[-1],method = \"pearson\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\nCode\nx_col &lt;- \"country\"  \nx_input &lt;- \"China\"  \n\nfiltered_data &lt;- bmi_numeric %&gt;%\n  filter(.data[[x_col]] == x_input) %&gt;% \n  select(-.data[[x_col]]) \n\nbmi.cor &lt;- cor(filtered_data[-1],method = \"pearson\")\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\nAggregated and Faceted Analysis\nOffering users the capability to conduct comparative analysis across specific years or countries enriches the analytical depth. However, when direct faceting support is lacking within the chosen package, a practical workaround involves generating two distinct plots for the respective comparisons. These plots can then be arranged side by side on the same webpage through frontend design.\n\n\nCustomization Represnetation\n\nCircleSquareEllipseNumberShadeColorPie\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'circle'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'ellipse'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'number'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'shade'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'color'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'pie'\n         )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\n\nâ€˜Circleâ€™ and â€˜Squareâ€™ provide similar visual cues, differing mainly in shape. Since their informational value is essentially the same, opting for oneâ€”such as â€˜Squareâ€™â€”is sufficient for simplicity.\nâ€˜Ellipseâ€™ enriches the analysis by indicating the directionality of relationships, adding a layer of insight beyond mere correlation strength.\nâ€˜Numberâ€™ directly presents the correlation coefficients, offering precise quantitative insights at a glance.\nâ€˜Shadeâ€™ and â€˜Colorâ€™ resemble a heatmap, effectively visualizing the strength of relationships through color gradients. This method is intuitive and visually engaging, making it easy to identify patterns of interest.\nâ€˜Pieâ€™, while creative, tends to clutter the visualization, making it challenging to extract meaningful insights.\n\nGiven these considerations, the recommended options to make available for user selection are â€˜Squareâ€™, â€˜Ellipseâ€™, and â€˜Numberâ€™. These choices balance clarity, informational depth, and ease of interpretation, facilitating a comprehensive and accessible analytical experience.\n\n\n\n\nCustomized Layout for Upper, Lower\n\nUpper - number, Lower - ellipseAll Square\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot.mixed(bmi.cor,\n         tl.col = \"black\",\n         lower = 'ellipse',\n         upper = 'number',\n         \n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nUtilizing different styles for representing correlation matrices, such as through corrplot.mixed() instead of the standard corrplot(), introduces a notable change in function usage. While these mixed representations can add visual diversity, they also complicate the plotting process and may lengthen processing time. This added complexity, particularly in a dynamic Shiny interface where users can switch representation methods on-the-fly, may not significantly enhance the analytical value. Given this consideration, opting for a uniform plotting approach without mixed representations is advised to maintain simplicity and efficiency. This decision aligns with the goal of providing a streamlined and effective user experience, prioritizing clarity and speed over the marginal benefits of mixed visual styles.\n\n\n\n\nCustomized Layout\n\nOnly LowerOnly UpperAll\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square',\n         type = 'lower'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square',\n         type = 'upper'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         diag = FALSE,\n         tl.col = \"black\",\n         method = 'square'\n         )\n\n\n\n\n\n\n\n\n\n\nCustomized Layout - diag\n\nOFFON\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = TRUE\n         )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nIn correlation matrices, the diagonal elements, which represent the correlation of variables with themselves, invariably equal 1. This uniformity means these elements do not contribute additional insights into the interrelationships among the variables. Recognizing this, displaying the diagonal is not essential for enhancing the analytical depth of the plots. Consequently, the option to toggle the visibility of the diagonal will not be offered to users; it will be defaulted to â€˜offâ€™ for all plots. This approach simplifies the visualization, directing focus to the more informative aspects of the correlation matrix and streamlining the user experience by eliminating redundant information.\n\n\n\n\nCustomized Layout - order\n\nhclustalphabetAOEFPC\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'hclust'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'alphabet'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'AOE'\n         )\n\n\n\n\n\n\n\n\n\nCode\nbmi.cor &lt;- cor(bmi_numeric[-1],method = \"spearman\", use = \"complete.obs\")\n\ncorrplot(bmi.cor,\n         tl.col = \"black\",\n         method = 'square',\n         diag = FALSE,\n         order = 'FPC',\n         \n         )\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct EDA\n\nChoice of Correlation Coefficient\nData Filtering Capabilities\nComparative analysis: At the interface\nAesthetic Setting: Represnetation type, layoput type, orders\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input cor.matrix: cor(data,method)\n\nmethod = â€œkendallâ€, â€œpearsonâ€, â€œspearmanâ€\n\nFiltering Input daraframe: x_col, x_input\nModifying augement in corrplot():\n\nmethod: â€˜squareâ€™, â€˜ellipseâ€™, â€˜numberâ€™\ntype = â€˜lowerâ€™, â€˜upperâ€™\norder: â€˜hclustâ€™, â€˜alphabetâ€™, â€˜AOEâ€™, â€˜FPCâ€™\ndiag: TRUE, FALSE\n\n\n\n\n\n\nSCATTER PLOT\nWhen identifying pairs of variables that exhibit higher correlation, users can delve deeper into Comparative Data Analysis (CDA) to conduct statistical tests. These tests assess the significance of the observed correlations, offering insights into the strength and reliability of the relationships between variables.\nThe ggstatsplot package is chosen due to its capabilities of advanced statistical analysis features, and flexibility.\nKey parameters to consider when conducting comparative data analysis (CDA) include:\n\nInput-related parameters: Variables for comparison\nTesting-related parameters: Type of statistical test and confidence level.\nAesthetic choices: smoothing method\n\n\nVariables Selection\n\nbmi_usd_price & gdp_per_capitabmi_change & inflation\n\n\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = gdp_per_capita,\n  y = bmi_usd_price,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = inflation,\n  y = bmi_change,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights on Argument Exposure\n\n\n\nThe choice of the variables can just simply be an input into the plotting function, y should be bmi-related variables, and x are the non bmi-related\n\n\n\n\nType of statistical test & Confidence level\nSimilarly, type of statistical test & Confidence level can be varied with type, and conf.level argument\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = inflation,\n  y = bmi_change,\n  marginal = FALSE,\n  type = \"parametric\",\n  conf.level = 0.95\n  )\n\n\n\n\n\n\n\nline smoothing\n\nloesslm\n\n\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = inflation,\n  y = bmi_change,\n  marginal = FALSE,\n  type = \"parametric\",\n  conf.level = 0.95,\n  smooth.line.args = list(linewidth = 1.5, color = \"blue\", method = \"loess\", formula = y ~x)\n  )\n\n\n\n\n\n\n\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = inflation,\n  y = bmi_change,\n  marginal = FALSE,\n  type = \"parametric\",\n  conf.level = 0.95,\n  smooth.line.args = list(linewidth = 1.5, color = \"blue\", method = \"lm\", formula = y ~x)\n  )\n\n\n\n\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical confirmatory data analysis (CDA)\nKey Features for Comprehensive Analysis:\n\nLevel of Details: region, country\nStatistical Test: Allows from parametric, nonparametric, bayes, robust\nConfidence Level: 95%, or 99%\nAesthetic Setting: Offers the ability to choose display label, and plot type\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe: select input varibles as x, y columns\nModifying augement in ggscatterstats():\n\ntype: â€œparametricâ€, â€œnonparametricâ€, â€œbayesâ€, â€œrobustâ€\nconf.level: 0.95, 0.99\nsmooth.line.args\n\nmethod: â€œloessâ€, â€œlmâ€\n\n\n\n\n\n\n\nUI Design\n\nCorroplot Page\n\nfor each individual exposed feature, please refer to the summary above. Instead of using the original argument name, the augment has been renamed and grouped accordingly to unsure better understanding with people who are not familiar with coding.\n\n\nCompare Page\n\nfor this page, it is similar to the correlation map, just allows for 2 additional input to define the left and right plots\n\n\nScatter Plot\n\nfor each individual exposed feature, please refer to the summary above. Instead of using the original argument name, the augment has been renamed and grouped accordingly to unsure better understanding with people who are not familiar with coding."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#lm",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#lm",
    "title": "Take-home Exercise 4",
    "section": "lm",
    "text": "lm\n\n\nCode\nggscatterstats(\n  data = bmi_all,\n  x = inflation,\n  y = bmi_change,\n  marginal = FALSE,\n  type = \"parametric\",\n  conf.level = 0.95,\n  smooth.line.args = list(linewidth = 1.5, color = \"blue\", method = \"lm\", formula = y ~x)\n  )\n\n\n\n\n\n\nSummary\nBased on the above experiment, the plot should allow for the functions below to provide enough flexibility for user to conduct statistical confirmatory data analysis (CDA)\nKey Features for Comprehensive Analysis:\n\nLevel of Details: region, country\nStatistical Test: Allows from parametric, nonparametric, bayes, robust\nConfidence Level: 95%, or 99%\nAesthetic Setting: Offers the ability to choose display label, and plot type\n\nThese features ensure that users have the necessary tools to perform robust statistical analysis, laying the groundwork for more complex investigations.\nFor seamless backend integration, the API setup includes functionalities that cater to diverse analytical preferences below\n\n\n\n\n\n\nSummary - API set up\n\n\n\nModifying Input Dataframe:using bmi data, and filter()\nModifying augement in ggbarstats():\n\ntype: â€œparametricâ€, â€œnonparametricâ€, â€œbayesâ€, â€œrobustâ€\nconf.level: 0.95, 0.99\npairwise.display: â€œallâ€, â€œnoneâ€, â€œnoneâ€, â€œsâ€, â€œnsâ€\noff boxplot/violin chart:\n\nboxplot.args = list(width = 0)\nviolin.args = list(width = 0, linewidth = 0)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-of-clustered-results",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#visualization-of-clustered-results",
    "title": "Take-home Exercise 4",
    "section": "Visualization of Clustered Results",
    "text": "Visualization of Clustered Results\nTo better understand and visualize the clustering results, consider plotting the results based on its clusters. Tataking the results of k means with cluster 4\n\n\nCode\nclustering_result &lt;- tsclust(list_matrices_per_country, type = \"partitional\", k = 4, distance = \"dtw\")\ncluster_assignments &lt;- clustering_result@cluster\n\ncountry_names &lt;- bmi_wide$country %&gt;% unique()\n\nif(length(country_names) == length(cluster_assignments)) {\n  country_cluster_df &lt;- data.frame(country = country_names, cluster = cluster_assignments)\n} else {\n  stop(\"Mismatch between the number of countries and the number of cluster assignments.\")\n}\n\nmerged_df &lt;- merge(bmi_all, country_cluster_df, by = \"country\", all.x = TRUE)\n\np &lt;- ggplot(merged_df, aes(x = year, y = bmi_usd_price, color = country, group = country)) + \n  geom_line() + \n  facet_wrap(~ cluster, scales = \"free_y\") + \n  theme_bw() + \n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n# Convert to an interactive plotly object\np_interactive &lt;- ggplotly(p, tooltip = c(\"country\", \"y\", \"x\"))\n\n# Show the plot\np_interactive"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui",
    "title": "Take-home Exercise 4",
    "section": "UI",
    "text": "UI"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui-design-2",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui-design-2",
    "title": "Take-home Exercise 4",
    "section": "UI Design",
    "text": "UI Design\n\nDesign Consideration\nthe interface offers mainly 2 function, model calibrations, and cluster visualizations. For calibration, after the model built, the user can see the key evaluation results from the main panel, and visualize the clustering with the appropriate graphs. On the side, there is also 2 charts to summarize the cluster allocations, and number of countries in each cluster. For results display, it is necessary to allow the user to refer back to the model parameters and results, thus the main panel is kept, and the key results are remained in the page. Additionally, user is able to select on the variables to display and check across the clustering results more interactively.\n\n\n\n\nSample UI"
  }
]